<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>pydata: Huiming's learning notes</title><link href="/" rel="alternate"></link><link href="/feeds/all-en.atom.xml" rel="self"></link><id>/</id><updated>2025-02-23T00:00:00-06:00</updated><subtitle>Keep Looking, Don't Settle</subtitle><entry><title>DeepSeek V3 learning notes</title><link href="/pages/2025/02/23/deepseek-v3-learning-notes/" rel="alternate"></link><published>2025-02-23T00:00:00-06:00</published><updated>2025-02-23T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2025-02-23:/pages/2025/02/23/deepseek-v3-learning-notes/</id><summary type="html">&lt;h2&gt;1. What the problem to solve?&lt;/h2&gt;
&lt;p&gt;When I went back for the Spring Festival, DeepSeek released a new model. For a while, all kinds of media discussed it a lot, almost rising to the height of national destiny. The most important points discussed should be two: the first is the …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. What the problem to solve?&lt;/h2&gt;
&lt;p&gt;When I went back for the Spring Festival, DeepSeek released a new model. For a while, all kinds of media discussed it a lot, almost rising to the height of national destiny. The most important points discussed should be two: the first is the benchmark score comparable to other mainstream models; the second is that the training speed is much faster and uses much less GPU time. However, most of the discussions are prosperous discussions and exciting news, but there is no specific reason why DS is good and how it is good. The first weekend after I came back, I spent the weekend reading the DS V3 paper. The reason why I chose V3 is that R1 is RL and SFT to enhance the reasoning ability of the model. The basic model is still V3, and the DS V3 paper is more detailed. The second part of the article describes what improvements DS has made in the model architecture (science), and the third part describes what improvements have been made in the model training (engineering). Then the two parts talk about pretraining, posttraining and evaluation related things.&lt;/p&gt;
&lt;h2&gt;2. How to solve?&lt;/h2&gt;
&lt;h3&gt;2.1. Basic Architecture&lt;/h3&gt;
&lt;p&gt;There are two main points in the basic architecture of the model: 1. The original Attention is changed to Multi-head Latent Attention; 2. The original FFN is changed to DeepSeekMoE. The purpose of doing this is to reduce the amount of calculation and reduce the cache data, thereby saving GPU memory. The details are as follows:&lt;/p&gt;
&lt;p&gt;DeekSeek Model Architecture
&lt;img alt="20250216_01_deepseek_v3_architecture.png" src="/figures/20250216_01_deepseek_v3_architecture.png"&gt;&lt;/p&gt;
&lt;h4&gt;2.1.1. Multi-head latent attention (MLA)&lt;/h4&gt;
&lt;p&gt;Compared with the attention mechanism introduced in the original &lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention&lt;/a&gt; model, DS uses the Multi-head latent attention (MLA) architecture. The difference from the original Multi-head attention is that when calculating &lt;span class="math"&gt;\(k, q, v\)&lt;/span&gt; from &lt;span class="math"&gt;\(h_t\)&lt;/span&gt;, it first reduces the dimension and projects &lt;span class="math"&gt;\(h_t \in \mathbb{R}^d\)&lt;/span&gt; to a low-dimensional space &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV} = W^{DKV} h_t}\)&lt;/span&gt;, and then upgrades the dimension to &lt;span class="math"&gt;\(\textcolor{blue}{k_t^C=W^{UK}c_t^{KV}}\)&lt;/span&gt;; and projects it to a low-dimensional &lt;span class="math"&gt;\(\textcolor{blue}{c_t^Q=W^{DQ}h_t}\)&lt;/span&gt;, and then upgrades the dimension to &lt;span class="math"&gt;\(\textcolor{blue}{q_t^C = W^{UQ} c_t^Q}\)&lt;/span&gt;. In this case, when performing inference, we only need to save low-dimensional &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textcolor{blue}{c_t^Q}\)&lt;/span&gt;. For example, the original &lt;span class="math"&gt;\(\text{dim}(h_t) = 7168\)&lt;/span&gt; is reduced to 4096 dimensions through a dimensionality reduction matrix &lt;span class="math"&gt;\(\text{dim}(W^{DKV})=4096 \times 7186\)&lt;/span&gt;, so the matrix &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt; that needs to be saved is much smaller. Specifically,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp;  \textcolor{blue}{c_t^{KV}} = W^{DKV} h_t, \\
&amp;amp;  [k_{t,1}^{C}, k_{t,2}^{C}, ..., k_{t,n_h}^{C}] = k_t^C = W^{UK} c_t^{KV}, \\
&amp;amp;  \textcolor{blue}{k_t^R} = \text{RoPE}(W^{KR} h_t), \\
&amp;amp;  k_{t,i} = \left[k_{t,i}^{C}; k_t^R \right], \\
&amp;amp;  [v_{t,1}^{C}, v_{t,2}^{C}, ..., v_{t,n_h}^{C}] = v_t^C = W^{UV} c_t^{KV},
\end{aligned}
 $$&lt;/div&gt;
&lt;p&gt;Here &lt;span class="math"&gt;\(W^{DKV} \in \mathbb{R}^{d_c \times d}\)&lt;/span&gt; is a dimension reduction projection matrix; &lt;span class="math"&gt;\(W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}\)&lt;/span&gt; are the corresponding dimension increase matrices of keys and values. When doing inference, only &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\textcolor{blue}{k_t^R}\)&lt;/span&gt; are needed, which saves both computation and memory.&lt;/p&gt;
&lt;p&gt;For query, the same process is used to reduce the dimension&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; \textcolor{blue}{c_t^Q} = W^{DQ} h_t, \\
&amp;amp; [q_{t,1}^{C}, q_{t,2}^{C}, ..., q_{t,n_h}^{C}] = q_t^C = W^{UQ} c_t^Q, \\
&amp;amp; [q_{t,1}^{R}, q_{t,2}^{R}, ..., q_{t,n_h}^{R}] = q_t^R = \text{RoPE}(W^{QR} c_t^Q), \\
&amp;amp; q_{t,i} = [q_{t,i}^{C}, q_{t,i}^{R}],
\end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing&lt;/h4&gt;
&lt;h5&gt;Basic Architecture of DeepSeekMoE&lt;/h5&gt;
&lt;p&gt;In the 2017 Attention paper, the decoder first calculates Attention and then calculates FFN to predict the final token. In DS, FFN is further changed to MoE. MoE means that the model is composed of a series of Experts. Each Expert can be an MLP, a CNN or other neural network model, or each Expert can be a tansformer encoder/decoder. Each Expert is responsible for learning different data patterns. MoE selects the appropriate Expert through the Gating Network so that different data is sent to the best Expert for processing. When predicting the next token, MoE does not use a certain Expert or all Experts, but selects the Top &lt;span class="math"&gt;\(K\)&lt;/span&gt; Experts to make predictions. Which &lt;span class="math"&gt;\(K\)&lt;/span&gt; Experts are selected is determined by a Gating network. Usually &lt;span class="math"&gt;\(K \ll N\)&lt;/span&gt;, so MoE is a sparse network. The first advantage of using MoE is that through more refined divisions, the model has greater flexibility, and different Experts can learn different data. For example, if &lt;span class="math"&gt;\(N=16\)&lt;/span&gt;, &lt;span class="math"&gt;\(K=2\)&lt;/span&gt;, then there are &lt;span class="math"&gt;\(C(16, 2)=160\)&lt;/span&gt; combinations. If each Expert is split into 4 small Experts, the total number of combinations is &lt;span class="math"&gt;\(C(64, 8)=4,426,165,368\)&lt;/span&gt;. This greatly increases the flexibility of the model. The second benefit is saving computing resources, because only a few Experts need to be activated each time to participate in the calculation, unlike the transformer where all parameters participate in the calculation. In other words, MoE can train a larger model with more parameters, thereby remembering more knowledge, but only a small number of parameters actually participate in the calculation each time. And because each Expert is an independent model with no shared parameters, they can perform calculations in parallel.&lt;/p&gt;
&lt;p&gt;MoE Architecture
&lt;img alt="20250216_03_deepseek_v2_moe.png" src="/figures/20250216_03_deepseek_v2_moe.png"&gt;&lt;/p&gt;
&lt;p&gt;In DS's FFN, DeepSeekMoE goes a step further on the basis of MOE: 1) They further refine the Expert, and then some Experts are used for routing. Specifically, for the routingd Expert, the prediction is only routed to some of these Experts. At the same time, they also have some Experts that are shared Experts, that is, they are used every time when predicting. 2) DS optimizes the selection of Experts and uses affinity scores to select experts. 3) DS's Experts are finer and sparser. This further reduces the computing cost.&lt;/p&gt;
&lt;p&gt;DeepSeekMoE Architecture
&lt;img alt="20250216_02_original_moe.png" src="/figures/20250216_02_original_moe.png"&gt;&lt;/p&gt;
&lt;p&gt;Assuming that the center of each Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; is &lt;span class="math"&gt;\(\mathbf{e}_{i}\)&lt;/span&gt;, first calculate &lt;span class="math"&gt;\(\textcolor{blue}{s_{i,t} = \sigma(\mathbf{u}_t^\top \mathbf{e}_i)}\)&lt;/span&gt; as the similarity between token &lt;span class="math"&gt;\(t\)&lt;/span&gt; and Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt;: &lt;span class="math"&gt;\(\mathbf{e}_i\)&lt;/span&gt; is used as the center vector of Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt;. The dot product is performed with the input &lt;span class="math"&gt;\(\mathbf{u}_t\)&lt;/span&gt; of the token, and then the affinity of the token to the Expert is calculated through sigmoid, thereby determining whether the token is routed to the Expert.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(g^{\prime}_{i,t}\)&lt;/span&gt; is the gating value of Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt;. Based on the value of &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt; of all Experts, the highest &lt;span class="math"&gt;\(K_r\)&lt;/span&gt; ones are selected.&lt;/p&gt;
&lt;div class="math"&gt;$$
    g'_{i,t} =
    \begin{cases} 
        s_{i,t}, &amp;amp; s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r), \\
        0, &amp;amp; \text{otherwise},
    \end{cases}
$$&lt;/div&gt;
&lt;p&gt;Then calculate the weights of these &lt;span class="math"&gt;\(K_r\)&lt;/span&gt; Experts &lt;span class="math"&gt;\(\textcolor{blue}{g_{i,t} = \frac{g^{\prime}_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}}}\)&lt;/span&gt;. Although the formula here shows that the denominator is &lt;span class="math"&gt;\(N_r\)&lt;/span&gt; &lt;span class="math"&gt;\(g'_{i,t}\)&lt;/span&gt; added together, in fact, &lt;span class="math"&gt;\(N_r - K_r\)&lt;/span&gt; are 0.&lt;/p&gt;
&lt;p&gt;Finally, the new output is &lt;span class="math"&gt;\(\textcolor{blue}{\mathbf{h}'_t = \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i(\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i(\mathbf{u}_t)}\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(K_r\)&lt;/span&gt; are selected from &lt;span class="math"&gt;\(N_r\)&lt;/span&gt; Experts for calculation, &lt;span class="math"&gt;\(K_r \ll N_r\)&lt;/span&gt;, thus saving a lot of calculation. At the same time, compared with the general MoE, DS uses &lt;span class="math"&gt;\(N_s\)&lt;/span&gt; shared Experts, which participate in the calculation of all tokens. By introducing shared experts, the model can share common knowledge between different tasks, avoiding each expert from learning similar content independently, thereby reducing knowledge redundancy.&lt;/p&gt;
&lt;h5&gt;Auxiliary-Loss-Free Load Balancing in DeepSeek-V3&lt;/h5&gt;
&lt;p&gt;In actual operation, this situation may occur: most of the tokens of the model are predicted by a few experts, and the remaining experts are less used, resulting in unbalanced expert load. The previous MOE used auxiliary loss to balance the expert load. The purpose of balanced load is achieved by punishing imbalanced expert utilization (adding more loss). However, there are two problems with this: 1) Too strong auxiliary loss hurts model performance by interfering with learning objectives. 2) Difficult to tune: Balancing between efficiency and accuracy requires careful hyperparameter tuning.&lt;/p&gt;
&lt;p&gt;DS proposed Auxiliary-Loss-Free Load Balancing to achieve balanced routing of tokens to different experts. Specifically, a bias item is added to the gating function to balance the call of the expert through the bias value.&lt;/p&gt;
&lt;p&gt;Specifically, when calculating the weight &lt;span class="math"&gt;\(g'_{i,t}\)&lt;/span&gt;, a bias &lt;span class="math"&gt;\(b_i\)&lt;/span&gt; is added to the affinity score of each token and Expert, becoming &lt;span class="math"&gt;\(s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r)\)&lt;/span&gt;. When an Expert &lt;span class="math"&gt;\(j\)&lt;/span&gt; is overloaded, the corresponding &lt;span class="math"&gt;\(b_j\)&lt;/span&gt; is reduced by &lt;span class="math"&gt;\(\gamma\)&lt;/span&gt;. In this way, the new &lt;span class="math"&gt;\(s_{j,t} + b_j\)&lt;/span&gt; will be reduced a little, so that when &lt;span class="math"&gt;\(\text{Topk}\)&lt;/span&gt; are selected, this Expert will be ranked a little lower and is less likely to be selected because &lt;span class="math"&gt;\(g'_{i,t} = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;h5&gt;Complementary Sequence-Wise Auxiliary Loss&lt;/h5&gt;
&lt;p&gt;DeepSeek-V3 mainly relies on the Auxiliary-Loss-Free load balancing strategy to ensure load balance among Experts. However, in order to prevent extreme imbalance within a single sequence, DeepSeek-V3 also introduces Complementary Sequence-Wise Auxiliary Loss. Usually, the load balancing of the MoE model is performed at the batch level, but for multiple sequences in a batch, some experts in a sequence may be used a lot, while other experts are used very little. The idea of DS is to make Expert utilization as balanced as possible at each sequence level, and the load will not be too concentrated or sparse. At the same time, the intensity of this loss is very low to avoid negative impact on model performance.&lt;/p&gt;
&lt;p&gt;Below we mainly understand the formula in the original text and why this formula can balance the Expert load at the sequence level.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; \mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i, \\
&amp;amp; f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right), \\
&amp;amp; s'_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}, \\
&amp;amp; P_i = \frac{1}{T} \sum_{t=1}^{T} s'_{i,t}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P_i\)&lt;/span&gt; is used to see whether the use of Expert in the sequence is relatively balanced. If it is not balanced, the value of &lt;span class="math"&gt;\(P_i\)&lt;/span&gt; will be relatively large. The specific reason is: first look at &lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; and &lt;span class="math"&gt;\(P_i\)&lt;/span&gt;. For the normalized affinity score &lt;span class="math"&gt;\(s^{\prime}_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}\)&lt;/span&gt;, calculate the average score &lt;span class="math"&gt;\(P_i = \frac{1}{T} \sum_{t=1}^{T} s^{\prime}_{i,t}\)&lt;/span&gt; of Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; in the entire sequence (the length of the sequence is &lt;span class="math"&gt;\(T\)&lt;/span&gt;): If Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; is selected on many tokens &lt;span class="math"&gt;\(t\)&lt;/span&gt; in the sequence, then the corresponding &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt; will be larger than (&lt;span class="math"&gt;\(s_{i,t} \gg s_{j,t}\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \neq i\)&lt;/span&gt;), and &lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; is &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt; for all The normalized value of Expert, then &lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; is relatively large. And &lt;span class="math"&gt;\(P_i\)&lt;/span&gt; is the average value of &lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; over the entire sequence, so it will also be relatively large. Finally, the corresponding loss function &lt;span class="math"&gt;\(\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i\)&lt;/span&gt; is relatively large.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; is the normalized value of the frequency with which Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; is used. The formula &lt;span class="math"&gt;\(f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)&lt;/span&gt; counts the frequency of each Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; being used in the current sequence &lt;span class="math"&gt;\(T\)&lt;/span&gt; tokens (&lt;span class="math"&gt;\(\sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)&lt;/span&gt;, and then normalizes it &lt;span class="math"&gt;\(\frac{N_r}{K_r T}\)&lt;/span&gt;. Similarly, if Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; is called many times in this sequence, then this value will be larger.&lt;/p&gt;
&lt;p&gt;Finally, &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; in the loss function formula is a hyperparameter with a very small value. Its value is small in order not to overly affect the main training objective.&lt;/p&gt;
&lt;h3&gt;2.2. Multi-Token Prediction (MTP)&lt;/h3&gt;
&lt;p&gt;DS also uses MTP, instead of just predicting the next token like general pre-training (the 2017 Attention article only predicts the next token). This has two advantages: 1. Predicting multiple tokens at the same time can improve the efficiency of data use. 2. Allow the model to think more long-term when preparing representation. Unlike the original MTP article, DS does not predict &lt;span class="math"&gt;\(D\)&lt;/span&gt; tokens at the same time (outputting a &lt;span class="math"&gt;\(D \times V\)&lt;/span&gt; matrix, where &lt;span class="math"&gt;\(V\)&lt;/span&gt; is the vocabulary size); in other words, the corresponding prediction and loss function is &lt;span class="math"&gt;\(L_n = -\sum_t \log P_{\theta} (x_{t+n:t+1} \mid x_{t:1})\)&lt;/span&gt;. DS sequentially predicts additional token.&lt;/p&gt;
&lt;p&gt;MTP from Gloeckle et al. (2024), which is  &lt;span class="math"&gt;\(D \times V\)&lt;/span&gt; matrix
&lt;img alt="20250216_04_deepseek_orig_MTP.png" src="/figures/20250216_04_deepseek_orig_MTP.png"&gt;&lt;/p&gt;
&lt;p&gt;The MTP module structure of DS can be simply summarized in one sentence: DS's MTP uses &lt;span class="math"&gt;\(D\)&lt;/span&gt; sequential modules to predict &lt;span class="math"&gt;\(D\)&lt;/span&gt; tokens. These &lt;span class="math"&gt;\(D\)&lt;/span&gt; modules share embedding &lt;span class="math"&gt;\(\text{Emb}(\cdot)\)&lt;/span&gt; and output head &lt;span class="math"&gt;\(\text{OutHead}(\cdot)\)&lt;/span&gt;. The MTP module of the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th token has a dedicated transformer block &lt;span class="math"&gt;\(\text{TRM}_k(\cdot)\)&lt;/span&gt; and a projection matrix &lt;span class="math"&gt;\(M_k \in \mathbb{R}^{d \times 2d}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;DeepSeek V3 MTP: Use D MTP module and &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th module predict &lt;span class="math"&gt;\(k\)&lt;/span&gt;-depth token
&lt;img alt="20250216_05_deepseek_v3_MTP.png" src="/figures/20250216_05_deepseek_v3_MTP.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;prediction&lt;/strong&gt;：For the &lt;span class="math"&gt;\(i\)&lt;/span&gt;th token &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; of the input sequence, at the &lt;span class="math"&gt;\(k\)&lt;/span&gt;th prediction depth (that is, predict &lt;span class="math"&gt;\(t_{i+k+1}\)&lt;/span&gt; below), the MTP module: 1) First get the representation &lt;span class="math"&gt;\(\mathbf{h}_i^{k-1} \in \mathbb{R}^{d}\)&lt;/span&gt; of the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th token at the &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-th depth, and the embediding &lt;span class="math"&gt;\(Emb(t_{i+k}) \in \mathbb{R}^{d}\)&lt;/span&gt; of the &lt;span class="math"&gt;\((i+k)\)&lt;/span&gt;-th token; 2) Then concat them (length is &lt;span class="math"&gt;\(2d\)&lt;/span&gt;), and then generate a new representation
&lt;span class="math"&gt;\(h’^k_i = M_k [\text{RMSNorm}(\mathbf{h}_i^{k-1}); \text{RMSNorm}(Emb(t_{i+k}))]\)&lt;/span&gt; through RMSNorm and the projection matrix &lt;span class="math"&gt;\(M_k \in \mathbb{R}^{d \times 2d}\)&lt;/span&gt;; 3) Then &lt;span class="math"&gt;\(\mathbf{h}’^k_i\)&lt;/span&gt; is taken as input to the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th transformer module &lt;span class="math"&gt;\(\text{TRM}_k(\cdot)\)&lt;/span&gt; to generate the output representation of the current depth &lt;span class="math"&gt;\(\mathbf{h}^k_{1:T-k} = TRM_k(\mathbf{h}’^{k}_{1:T-k})\)&lt;/span&gt;; finally, &lt;span class="math"&gt;\(\text{OutHead}\)&lt;/span&gt; is used to convert &lt;span class="math"&gt;\(\mathbf{h}^k_i\)&lt;/span&gt; into predicted probability &lt;span class="math"&gt;\(p_{i+k+1} = OutHead(\mathbf{h}^k_i) \in \mathbb{R}^{V}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;My understanding with the example step by step:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Traditional next token prediction: &lt;span class="math"&gt;\(t_1\)&lt;/span&gt;-&amp;gt; &lt;span class="math"&gt;\(h_1\)&lt;/span&gt; -&amp;gt; &lt;span class="math"&gt;\(\hat{t}_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(t_2\)&lt;/span&gt; -&amp;gt; ​​&lt;span class="math"&gt;\(h_2\)&lt;/span&gt; -&amp;gt; ​​&lt;span class="math"&gt;\(\hat{t}_3\)&lt;/span&gt;, .... This is what the main model does.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th MTP module of DS, the model will get the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-depth representation &lt;span class="math"&gt;\(\mathbf{h}_i^k\)&lt;/span&gt; based on the &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-depth MTP representation &lt;span class="math"&gt;\(h_i^{k-1}\)&lt;/span&gt; and the embedding of token &lt;span class="math"&gt;\(t_{i+k}\)&lt;/span&gt;, and then further predict the &lt;span class="math"&gt;\((k+1)\)&lt;/span&gt;-th token. Below, we use &lt;span class="math"&gt;\(k=1, 2, 3\)&lt;/span&gt; as an example to illustrate how it works. Specifically, for input token &lt;span class="math"&gt;\(i\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(k=1\)&lt;/span&gt;, first concatenate the representation &lt;span class="math"&gt;\(h_i^{k-1}=h_i^0\)&lt;/span&gt; (from main model) of &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; and the embedding of token &lt;span class="math"&gt;\(t_{i+1}\)&lt;/span&gt; (&lt;span class="math"&gt;\(i+k=i+1\)&lt;/span&gt;) to get &lt;span class="math"&gt;\(h’^1_i\)&lt;/span&gt;, then input &lt;span class="math"&gt;\(TRM_1\)&lt;/span&gt;, get &lt;span class="math"&gt;\(\mathbf{h}^1_{1:T-1} = TRM_1(\mathbf{h}’^{1}_{1:T-1})\)&lt;/span&gt;, and finally get the prediction distribution &lt;span class="math"&gt;\(p_{i+2}\)&lt;/span&gt; of token &lt;span class="math"&gt;\(t_{i+2}\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(k=2\)&lt;/span&gt;, first concatenate the representation &lt;span class="math"&gt;\(h_i^{k-1}=h_i^1\)&lt;/span&gt; (from &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; MTP module) of &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; and the embedding of token &lt;span class="math"&gt;\(t_{i+1}\)&lt;/span&gt; (&lt;span class="math"&gt;\(i+k=i+1\)&lt;/span&gt;) to get &lt;span class="math"&gt;\(h’^1_i\)&lt;/span&gt;. Then input &lt;span class="math"&gt;\(TRM_1\)&lt;/span&gt;, get &lt;span class="math"&gt;\(\mathbf{h}^1_{1:T-1} = TRM_1(\mathbf{h}’^{1}_{1:T-1})\)&lt;/span&gt;, and finally get the prediction distribution &lt;span class="math"&gt;\(p_{i+2}\)&lt;/span&gt; of token &lt;span class="math"&gt;\(t_{i+2}\)&lt;/span&gt;. The embeddings of &lt;span class="math"&gt;\(t_{i+2}\)&lt;/span&gt; ( &lt;span class="math"&gt;\(i+k=i+2\)&lt;/span&gt; ) are concatenated to get &lt;span class="math"&gt;\(h’^2_i\)&lt;/span&gt;, and then input into &lt;span class="math"&gt;\(TRM_2\)&lt;/span&gt;, to get &lt;span class="math"&gt;\(\mathbf{h}^2_{1:T-2} = TRM_2(\mathbf{h}’^{2}_{1:T-2})\)&lt;/span&gt;, and finally get the prediction distribution &lt;span class="math"&gt;\(p_{i+3}\)&lt;/span&gt; of token &lt;span class="math"&gt;\(t_{i+3}\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When &lt;span class="math"&gt;\(k=3\)&lt;/span&gt;, first concatenate the representation &lt;span class="math"&gt;\(h_i^{k-1}=h_i^2\)&lt;/span&gt; (from &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; MTP module) of &lt;span class="math"&gt;\(t_{i+3}\)&lt;/span&gt; and the embeddings of token &lt;span class="math"&gt;\(t_{i+3}\)&lt;/span&gt; ( &lt;span class="math"&gt;\(i+k=i+3\)&lt;/span&gt; ) to get &lt;span class="math"&gt;\(h’^3_i\)&lt;/span&gt;, and then input into &lt;span class="math"&gt;\(\text{TRM}_3\)&lt;/span&gt;, to get &lt;span class="math"&gt;\(\mathbf{h}^3_{1:T-3} = TRM_3(\mathbf{h}’^{3}_{1:T-3})\)&lt;/span&gt;, and finally get the prediction distribution &lt;span class="math"&gt;\(p_{i+4}\)&lt;/span&gt; of token &lt;span class="math"&gt;\(t_{i+4}\)&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Objective function of MTP&lt;/strong&gt; For each predicted depth k, its loss function is still cross-entropy, defined as &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt; = &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt; = &lt;span class="math"&gt;\(\text{CrossEntropy}(P_{2+k:T+1}^{k}, t_{2+k:T+1})\)&lt;/span&gt; = &lt;span class="math"&gt;\(-\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^k[t_i]\)&lt;/span&gt;. Where &lt;span class="math"&gt;\(T\)&lt;/span&gt; is the length of the input sequence, &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; is the &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th true value of the input sequence, and &lt;span class="math"&gt;\(p_i^k[t_i]\)&lt;/span&gt; is the prediciton probability of the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th MTP module for &lt;span class="math"&gt;\(t_i\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The final MTP loss function is the average of all predicted depths (&lt;span class="math"&gt;\(k\)&lt;/span&gt; from 1 to &lt;span class="math"&gt;\(D\)&lt;/span&gt;) multiplied by the weight &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}\)&lt;/span&gt; = &lt;span class="math"&gt;\(\frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;From DS's paper formula 25, &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}\)&lt;/span&gt; is only the loss function of MTP. The final loss function should also add &lt;span class="math"&gt;\(\mathcal{L}_{\text{Main}}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to use MTP during inference?&lt;/strong&gt; The main purpose of MTP is to improve the performance of the main model. During inference, MTP is not used, but only the main model is used to predict the next token.&lt;/p&gt;
&lt;h3&gt;stay tuned!&lt;/h3&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf"&gt;DeepSeek-V3 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category><category term="AIG"></category></entry><entry><title>DeepSeek V3</title><link href="/pages/2025/02/16/deepseek-v3/" rel="alternate"></link><published>2025-02-16T00:00:00-06:00</published><updated>2025-02-16T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2025-02-16:/pages/2025/02/16/deepseek-v3/</id><summary type="html">&lt;h2&gt;1. What the problem to solve?&lt;/h2&gt;
&lt;p&gt;春节回去的时候正好碰上DeepSeek发布新的模型，一时间各路媒体讨论的沸沸扬扬，几乎上升到国运的高度。讨论的最重要的应 …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. What the problem to solve?&lt;/h2&gt;
&lt;p&gt;春节回去的时候正好碰上DeepSeek发布新的模型，一时间各路媒体讨论的沸沸扬扬，几乎上升到国运的高度。讨论的最重要的应该是两点：第一个是媲美其他主流模型的benchmark分数；第二个是说训练速度快了很多，使用了少的多的GPU时间。可是各路讨论多是繁华之论以及振奋人心的消息，而没有具体的DS为什么好，怎么好的。回来的第一个周末，花了周末的时间读了一下DS V3的论文。之所以选择V3因为R1是之于V3进行了RL和SFT来增强模型的推理能力，基本的模型还是V3，而且DS V3的论文也更详细。文章的第二部分讲述了DS在模型的架构上（science）有一些什么样的改进，第三部分讲述了在模型的训练上（engineering）有什么改进，然后两部分讲了pretraining和posttraining以及evaluation相关的东西。&lt;/p&gt;
&lt;h2&gt;2. How to solve / Model architecture?&lt;/h2&gt;
&lt;h3&gt;2.1. Basic Architecture&lt;/h3&gt;
&lt;p&gt;模型的基本架构里面主要有两点：1是把原来的Attention改成了Multi-head Latent Attention；2是把原来的FFN改成了DeepSeekMoE。这样做的目的一是减少了计算量；二是减少了cache的data，从而节约了GPU的内存。具体如下：&lt;/p&gt;
&lt;p&gt;DeekSeek模型架构
&lt;img alt="20250216_01_deepseek_v3_architecture.png" src="/figures/20250216_01_deepseek_v3_architecture.png"&gt;&lt;/p&gt;
&lt;h4&gt;2.1.1. Multi-head latent attention (MLA)&lt;/h4&gt;
&lt;p&gt;相比较于原始的&lt;a href="https://arxiv.org/abs/1706.03762"&gt;Attention&lt;/a&gt;模型里面介绍的注意力机制, DS使用Multi-head latent attention (MLA)的架构，跟原来的Multi-head attention不一样的是这里面从 &lt;span class="math"&gt;\(h_t\)&lt;/span&gt; 来计算 &lt;span class="math"&gt;\(k, q, v\)&lt;/span&gt; 的时候，先进行降维，把 &lt;span class="math"&gt;\(h_t \in \mathbb{R}^d\)&lt;/span&gt; 投影到一个低维空间 &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV} = W^{DKV} h_t}\)&lt;/span&gt; ，然后再升维到 &lt;span class="math"&gt;\(\textcolor{blue}{k_t^C=W^{UK}c_t^{KV}}\)&lt;/span&gt; ；以及投影到一个低维的 &lt;span class="math"&gt;\(\textcolor{blue}{c_t^Q=W^{DQ}h_t}\)&lt;/span&gt;，然后再升维到 &lt;span class="math"&gt;\(\textcolor{blue}{q_t^C = W^{UQ} c_t^Q}\)&lt;/span&gt;。这样的话在进行inference的时候只要保存低维的 &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(\textcolor{blue}{c_t^Q}\)&lt;/span&gt;. 比如说，原来的 &lt;span class="math"&gt;\(\text{dim}(h_t) = 7168\)&lt;/span&gt;, 通过一个降维矩阵 &lt;span class="math"&gt;\(\text{dim}(W^{DKV})=4096 \times 7186\)&lt;/span&gt;把它降到4096维，那么需要保存的矩阵 &lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt;就小了很多。 具体地说，
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp;  \textcolor{blue}{c_t^{KV}} = W^{DKV} h_t, \\
&amp;amp;  [k_{t,1}^{C}, k_{t,2}^{C}, ..., k_{t,n_h}^{C}] = k_t^C = W^{UK} c_t^{KV}, \\
&amp;amp;  \textcolor{blue}{k_t^R} = \text{RoPE}(W^{KR} h_t), \\
&amp;amp;  k_{t,i} = \left[k_{t,i}^{C}; k_t^R \right], \\
&amp;amp;  [v_{t,1}^{C}, v_{t,2}^{C}, ..., v_{t,n_h}^{C}] = v_t^C = W^{UV} c_t^{KV},
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;这里 &lt;span class="math"&gt;\(W^{DKV} \in \mathbb{R}^{d_c \times d}\)&lt;/span&gt; 是一个降维投影矩阵; &lt;span class="math"&gt;\(W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}\)&lt;/span&gt; 是对应的 keys 和 values 的升维矩阵。在做inference的时候，只需要&lt;span class="math"&gt;\(\textcolor{blue}{c_t^{KV}}\)&lt;/span&gt;和&lt;span class="math"&gt;\(\textcolor{blue}{k_t^R}\)&lt;/span&gt;，即节省了计算量，也节约了内存。&lt;/p&gt;
&lt;p&gt;对Query，也有同样的处理来降维
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; \textcolor{blue}{c_t^Q} = W^{DQ} h_t, \\
&amp;amp; [q_{t,1}^{C}, q_{t,2}^{C}, ..., q_{t,n_h}^{C}] = q_t^C = W^{UQ} c_t^Q, \\
&amp;amp; [q_{t,1}^{R}, q_{t,2}^{R}, ..., q_{t,n_h}^{R}] = q_t^R = \text{RoPE}(W^{QR} c_t^Q), \\
&amp;amp; q_{t,i} = [q_{t,i}^{C}, q_{t,i}^{R}],
\end{aligned}
$$&lt;/div&gt;
&lt;h4&gt;2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing&lt;/h4&gt;
&lt;h5&gt;Basic Architecture of DeepSeekMoE&lt;/h5&gt;
&lt;p&gt;在2017年的Attention论文里面，decoder先是计算Attention，然后再计算FFN从而predict最后的token。DS里面，FFN被进一步改成了MoE。MoE的意思是模型是由一些列的Expert构成。每个Expert可以是一个MLP，一个CNN或者其他的神经网络模型，或者每个Expert可以是一个tansformer的encoder/decoder。每个 Expert 负责学习不同的数据模式，MoE 通过 Gating Network 选择合适的Expert，使得不同的数据被送往最擅长Expert家处理。每预测下一个token的时候，MoE不是使用某一个Expert，也不是使用所有的Expert，而是选择其中的Top &lt;span class="math"&gt;\(K\)&lt;/span&gt; 个Expert来进行预测。选哪 &lt;span class="math"&gt;\(K\)&lt;/span&gt; 个Expert是通过一个Gating网络来确定。通常 &lt;span class="math"&gt;\(K \ll N\)&lt;/span&gt;，所以MoE是一个稀疏网络。使用MoE第一个好处是通过更精细化的划分，模型有更大的灵活性，不同的Expert可以学习不同的数据。比如如果 &lt;span class="math"&gt;\(N=16\)&lt;/span&gt;，&lt;span class="math"&gt;\(K=2\)&lt;/span&gt;，那么就有 &lt;span class="math"&gt;\(C(16, 2)=160\)&lt;/span&gt; 个组合。如果每个Expert再split成4个小的Expert，那么总共的组合就是 &lt;span class="math"&gt;\(C(64, 8)=4,426,165,368\)&lt;/span&gt;。这样极大的增加了模型的灵活性。第二个好处是节省计算资源，因为每次只需要激活很少的Expert来参加计算，而不像transfomer那样所有的参数都参加计算。也就是说，MoE可以训练一个更大更多参数的模型，从而记住更多的知识，但是每次实际只有很少一部分参数参加计算。而且应为每个Expert是独立的模型，没有共享参数，所以他们可以并行进行计算。&lt;/p&gt;
&lt;p&gt;MoE的架构见下图
&lt;img alt="20250216_03_deepseek_v2_moe.png" src="/figures/20250216_03_deepseek_v2_moe.png"&gt;&lt;/p&gt;
&lt;p&gt;在DS的FFN里面，DeepSeekMoE在MOE的基础上更进一步：1）他们把Expert进一步细化，然后既有一些Expert用来做routing，具体地说，对routingd的Expert，预测只是route到这些Expert的某一些上面。同时，他们还有一些Expert是共享Expert，也就是说，在预测的时候，他们每次都是被使用。2）DS优化了Expert的选择，使用affinity score来选择专家。3）DS的Expert更finer，同时也更稀疏。这样进一步降低了运算成本。&lt;/p&gt;
&lt;p&gt;DeepSeekMoE的架构见下图
&lt;img alt="20250216_02_original_moe.png" src="/figures/20250216_02_original_moe.png"&gt;&lt;/p&gt;
&lt;p&gt;假设每个Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 的中心是 &lt;span class="math"&gt;\(\mathbf{e}_{i}\)&lt;/span&gt;，首先计算 &lt;span class="math"&gt;\(\textcolor{blue}{s_{i,t} = \sigma(\mathbf{u}_t^\top \mathbf{e}_i)}\)&lt;/span&gt; 作为 token &lt;span class="math"&gt;\(t\)&lt;/span&gt; 与 Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 的相似度： &lt;span class="math"&gt;\(\mathbf{e}_i\)&lt;/span&gt; 作为Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 的中心向量，通过与token的输入 &lt;span class="math"&gt;\(\mathbf{u}_t\)&lt;/span&gt; 进行点积，再经过 sigmoid 计算出 token 对该Expert的 affinity，从而决定 token 是否被路由到该Expert。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(g^{\prime}_{i,t}\)&lt;/span&gt; 是Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt;的gating value，根据所有的Expert的 &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt;的值，来选择最高的 &lt;span class="math"&gt;\(K_r\)&lt;/span&gt;个。
&lt;/p&gt;
&lt;div class="math"&gt;$$
    g'_{i,t} =
    \begin{cases} 
        s_{i,t}, &amp;amp; s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r), \\
        0, &amp;amp; \text{otherwise},
    \end{cases}
$$&lt;/div&gt;
&lt;p&gt;然后算出这 &lt;span class="math"&gt;\(K_r\)&lt;/span&gt;个Expert的权重 &lt;span class="math"&gt;\(\textcolor{blue}{g_{i,t} = \frac{g^{\prime}_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}}}\)&lt;/span&gt;。尽管这儿的公式看分母是 &lt;span class="math"&gt;\(N_r\)&lt;/span&gt; 个 &lt;span class="math"&gt;\(g'_{i,t}\)&lt;/span&gt; 相加，但是实际上有 &lt;span class="math"&gt;\(N_r - K_r\)&lt;/span&gt; 个为0.&lt;/p&gt;
&lt;p&gt;最后得到新的output为 &lt;span class="math"&gt;\(\textcolor{blue}{\mathbf{h}'_t = \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i(\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i(\mathbf{u}_t)}\)&lt;/span&gt;. 注意的是，这里面从 &lt;span class="math"&gt;\(N_r\)&lt;/span&gt; 个Expert里面选择了 &lt;span class="math"&gt;\(K_r\)&lt;/span&gt;个来进行计算，&lt;span class="math"&gt;\(K_r \ll N_r\)&lt;/span&gt;，从而节约了大量的计算。同时，相比较于一般的MoE，DS使用了 &lt;span class="math"&gt;\(N_s\)&lt;/span&gt;个 shared Expert，这些Expert参加所有的token的运算。通过引入共享专家，模型可以在不同任务之间共享通用知识，避免每个专家都独立学习相似的内容，从而减少了知识冗余。&lt;/p&gt;
&lt;h5&gt;Auxiliary-Loss-Free Load Balancing in DeepSeek-V3&lt;/h5&gt;
&lt;p&gt;实际运行的时候，可能会出现这种情况：模型的token大部分都是有某几个Expert来predict，剩下来的Expert使用率会比较少，导致Expert负载不均衡。以前的MOE使用auxiliary loss来让Expert负载均衡。通过惩罚imbalanced Expert utilization （加上更多的loss）来来达到均衡负载的目的。但是这样做有两个问题：1）Too strong auxiliary loss hurts model performance by interfering with learning objectives. 2）Difficult to tune: Balancing between efficiency and accuracy requires careful hyperparameter tuning.&lt;/p&gt;
&lt;p&gt;DS提出了Auxiliary-Loss-Free Load Balancing来达到balanced routing of tokens to different Experts.具体的说实在gating function里面添加一个bias的项，通过bias的值来平衡Expert的调用。&lt;/p&gt;
&lt;p&gt;具体的说，在计算权重&lt;span class="math"&gt;\(g'_{i,t}\)&lt;/span&gt;的时候，对每个token和Expert的affinity score都添加了一个bias &lt;span class="math"&gt;\(b_i\)&lt;/span&gt;，变成&lt;span class="math"&gt;\(s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r)\)&lt;/span&gt;, 当某个Expert &lt;span class="math"&gt;\(j\)&lt;/span&gt;出现overload的时候，就把对应的&lt;span class="math"&gt;\(b_j\)&lt;/span&gt;减小&lt;span class="math"&gt;\(\gamma\)&lt;/span&gt; 。这样的话新的&lt;span class="math"&gt;\(s_{j,t} + b_j\)&lt;/span&gt;就会减小一点，这样的话 &lt;span class="math"&gt;\(\textcolor{blue}{Top k}\)&lt;/span&gt; 个选择的时候这个Expert就会排名靠后一点，更可能不会被选中因为&lt;span class="math"&gt;\(g'_{i,t} = 0\)&lt;/span&gt;。&lt;/p&gt;
&lt;h5&gt;Complementary Sequence-Wise Auxiliary Loss&lt;/h5&gt;
&lt;p&gt;DeepSeek-V3 主要依赖 Auxiliary-Loss-Free 负载均衡策略来确保 Experts 间的负载均衡。然而，为了防止单个序列内部的极端不平衡，DeepSeek-V3 还引入了 Complementary Sequence-Wise Auxiliary Loss. 通常MoE模型的负载均衡都在batch level进行，但是对一个batch的多个sequence，某个sequence 内部某些Expert可能用的很多，而其他Expert用的很少。DS的想法是要在每个sequence level，也尽可能的让Expert utilization均衡，负载不会过于集中或稀疏。同时该损失的强度非常低，以避免对模型性能产生负面影响。&lt;/p&gt;
&lt;p&gt;下面我们主要来理解原文中的公式，为什么这个公式就能够均衡 Sequence level的Expert 负载。&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; \mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i, \\
&amp;amp; f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbf{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right), \\
&amp;amp; s'_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}, \\
&amp;amp; P_i = \frac{1}{T} \sum_{t=1}^{T} s^{\prime}_{i,t}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;&lt;span class="math"&gt;\(P_i\)&lt;/span&gt;是看在 sequence 上面 Expert 的使用是不是比较均衡。如果不均衡的话，&lt;span class="math"&gt;\(P_i\)&lt;/span&gt;的值就会比较大。具体原因：首先看&lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; 和 &lt;span class="math"&gt;\(P_i\)&lt;/span&gt;。对归一化的affinity score &lt;span class="math"&gt;\(s^{\prime}_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}\)&lt;/span&gt;, 计算Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 在整个sequence (Sequence 的长度为 &lt;span class="math"&gt;\(T\)&lt;/span&gt;) 上面的的平均得分 &lt;span class="math"&gt;\(P_i = \frac{1}{T} \sum_{t=1}^{T} s^{\prime}_{i,t}\)&lt;/span&gt;: 如果Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 在sequence的很多token &lt;span class="math"&gt;\(t\)&lt;/span&gt;上被选中，那么对应的 &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt; 就会被其他的&lt;span class="math"&gt;\(s_{j,t}\)&lt;/span&gt;大 (&lt;span class="math"&gt;\(s_{i,t} \gg s_{j,t}\)&lt;/span&gt; for &lt;span class="math"&gt;\(j \neq i\)&lt;/span&gt;), 而 &lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt; 是 &lt;span class="math"&gt;\(s_{i,t}\)&lt;/span&gt;对所有的 Expert的归一化值，那么&lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt;就相应的比较大。而&lt;span class="math"&gt;\(P_i\)&lt;/span&gt;是&lt;span class="math"&gt;\(s^{\prime}_{i,t}\)&lt;/span&gt;在整个 sequence 上的平均值，所以它也会比较大。最后相对应的损失函数 &lt;span class="math"&gt;\(\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i\)&lt;/span&gt; 就比较大。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(f_i\)&lt;/span&gt; 是 Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 被使用的频率的归一化值。公式 &lt;span class="math"&gt;\(f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)&lt;/span&gt; 统计了每个 Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 在当前序列 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 个token中被使用的频率 （&lt;span class="math"&gt;\(\sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)&lt;/span&gt;），然后做了归一化 &lt;span class="math"&gt;\(\frac{N_r}{K_r T}\)&lt;/span&gt;. 同样，如果 Expert &lt;span class="math"&gt;\(i\)&lt;/span&gt; 在这个 sequence 中被调用的次数很多，那么这个值也会更大。&lt;/p&gt;
&lt;p&gt;最后，损失函数公式里的 &lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; 是个取值很小的超参数。它的值很小是为了不过度影响主要训练目标。&lt;/p&gt;
&lt;h3&gt;2.2. Multi-Token Prediction (MTP)&lt;/h3&gt;
&lt;p&gt;DS还使用了MTP，而不是像一般的pre-training那样仅仅只预测 next token（2017的Attention文章就是只predict next token）。这有两个好处：1. 同时预测多个token可以提高data的使用效率。2. 让模型在准备representation的时候能够考虑的更长远。跟原始的MTP文章不同的是，DS不是同时predict &lt;span class="math"&gt;\(D\)&lt;/span&gt; 个token (输出一个 &lt;span class="math"&gt;\(D \times V\)&lt;/span&gt; 的矩阵， 其中 &lt;span class="math"&gt;\(V\)&lt;/span&gt; 是词表大小); 换个表达方式，对应的prediction 和损失函数是 &lt;span class="math"&gt;\(L_n = -\sum_t \log P_{\theta} (x_{t+n:t+1} \mid x_{t:1})\)&lt;/span&gt;.  DS是sequentially predict additional token。&lt;/p&gt;
&lt;p&gt;MTP from Gloeckle et al. (2024), which is  &lt;span class="math"&gt;\(D \times V\)&lt;/span&gt; 的矩阵
&lt;img alt="20250216_04_deepseek_orig_MTP.png" src="/figures/20250216_04_deepseek_orig_MTP.png"&gt;&lt;/p&gt;
&lt;p&gt;DS的MTP模块结构用一句话来简单概括就是：DS的MTP使用 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 个sequential modules来预测 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 个tokens。这 &lt;span class="math"&gt;\(D\)&lt;/span&gt; 个modules 共享 embedding &lt;span class="math"&gt;\(\text{Emb}(\cdot)\)&lt;/span&gt;，共享输出头 &lt;span class="math"&gt;\(\text{OutHead}(\cdot)\)&lt;/span&gt; 。 第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个token的 MTP 模块有一个专用的 transformer 块 &lt;span class="math"&gt;\(\text{TRM}_k(\cdot)\)&lt;/span&gt;，以及一个投影矩阵 &lt;span class="math"&gt;\(M_k \in \mathbb{R}^{d \times 2d}\)&lt;/span&gt;  &lt;/p&gt;
&lt;p&gt;DeepSeek V3 MTP: Use D MTP module and &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th module predict &lt;span class="math"&gt;\(k\)&lt;/span&gt;-depth token
&lt;img alt="20250216_05_deepseek_v3_MTP.png" src="/figures/20250216_05_deepseek_v3_MTP.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;prediction&lt;/strong&gt;：对于输入sequence的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt; 个token &lt;span class="math"&gt;\(t_i\)&lt;/span&gt;，在第 &lt;span class="math"&gt;\(k\)&lt;/span&gt; 个prediction depth (也就是下面要 predict &lt;span class="math"&gt;\(t_{i+k+1}\)&lt;/span&gt;)，MTP module： 1）首先得到 &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th token  在第 &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-th 深度的表示 &lt;span class="math"&gt;\(\mathbf{h}_i^{k-1} \in \mathbb{R}^{d}\)&lt;/span&gt;， 以及第 &lt;span class="math"&gt;\((i+k)\)&lt;/span&gt;-th 个 token 的 embediding  &lt;span class="math"&gt;\(Emb(t_{i+k}) \in \mathbb{R}^{d}\)&lt;/span&gt;；  2）然后把他们concat起来（长度为 &lt;span class="math"&gt;\(2d\)&lt;/span&gt;），再通过 RMSNorm 和 投影矩阵 &lt;span class="math"&gt;\(M_k \in \mathbb{R}^{d \times 2d}\)&lt;/span&gt;} 来生成新的表示 
&lt;span class="math"&gt;\(h’^k_i = M_k [\text{RMSNorm}(\mathbf{h}_i^{k-1}); \text{RMSNorm}(Emb(t_{i+k}))]\)&lt;/span&gt;；3）然后 &lt;span class="math"&gt;\(\mathbf{h}’^k_i\)&lt;/span&gt;  作为输入到第 &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th  个transformer 模块  &lt;span class="math"&gt;\(\text{TRM}_k(\cdot)\)&lt;/span&gt; ，生成当前深度的输出表示 &lt;span class="math"&gt;\(\mathbf{h}^k_{1:T-k} = TRM_k(\mathbf{h}’^{k}_{1:T-k})\)&lt;/span&gt;；最后使用 &lt;span class="math"&gt;\(\text{OutHead}\)&lt;/span&gt; 将 &lt;span class="math"&gt;\(\mathbf{h}^k_i\)&lt;/span&gt; 转换成预测概率 &lt;span class="math"&gt;\(p_{i+k+1} = OutHead(\mathbf{h}^k_i) \in \mathbb{R}^{V}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;我的理解：&lt;/p&gt;
&lt;p&gt;1) 传统的next token prediction：&lt;span class="math"&gt;\(t_1\)&lt;/span&gt;-&amp;gt; &lt;span class="math"&gt;\(h_1\)&lt;/span&gt; -&amp;gt; &lt;span class="math"&gt;\(\hat{t}_2\)&lt;/span&gt;, &lt;span class="math"&gt;\(t_2\)&lt;/span&gt; -&amp;gt; &lt;span class="math"&gt;\(h_2\)&lt;/span&gt; -&amp;gt; &lt;span class="math"&gt;\(\hat{t}_3\)&lt;/span&gt;, .... 也就是main model做的事情.&lt;/p&gt;
&lt;p&gt;2) 在DS的 &lt;span class="math"&gt;\(k\)&lt;/span&gt;-th MTP 模块中，模型会根据第 &lt;span class="math"&gt;\((k-1)\)&lt;/span&gt;-depth 的 MTP representation &lt;span class="math"&gt;\(h_i^{k-1}\)&lt;/span&gt; 和 token &lt;span class="math"&gt;\(t_{i+k}\)&lt;/span&gt; 的embedding来的到 &lt;span class="math"&gt;\(k\)&lt;/span&gt;-depth 的representaion &lt;span class="math"&gt;\(\mathbf{h}_i^k\)&lt;/span&gt;，从而更进一步来预测 &lt;span class="math"&gt;\((k+1)\)&lt;/span&gt;-th token。下面用 &lt;span class="math"&gt;\(k=1, 2, 3\)&lt;/span&gt; 作为例子来说明是怎么工作的。 具体来说, 对输入 token &lt;span class="math"&gt;\(i\)&lt;/span&gt; :&lt;/p&gt;
&lt;p&gt;在 &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; 的时候，先把 &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 的representaion &lt;span class="math"&gt;\(h_i^{k-1}=h_i^0\)&lt;/span&gt; （from main model） 和 token &lt;span class="math"&gt;\(t_{i+1}\)&lt;/span&gt; (&lt;span class="math"&gt;\(i+k=i+1\)&lt;/span&gt;) 的embedding concat在一起得到 &lt;span class="math"&gt;\(h’^1_i\)&lt;/span&gt;，然后输入 &lt;span class="math"&gt;\(TRM_1\)&lt;/span&gt;， 得到 &lt;span class="math"&gt;\(\mathbf{h}^1_{1:T-1} = TRM_1(\mathbf{h}’^{1}_{1:T-1})\)&lt;/span&gt;， 最后得到token &lt;span class="math"&gt;\(t_{i+2}\)&lt;/span&gt; 的prediction 分布 &lt;span class="math"&gt;\(p_{i+2}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; 的时候，先把 &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 的representaion &lt;span class="math"&gt;\(h_i^{k-1}=h_i^1\)&lt;/span&gt; （from &lt;span class="math"&gt;\(k=1\)&lt;/span&gt; MTP module） 和 token &lt;span class="math"&gt;\(t_{i+2}\)&lt;/span&gt; ( &lt;span class="math"&gt;\(i+k=i+2\)&lt;/span&gt; ) 的embedding concat在一起得到 &lt;span class="math"&gt;\(h’^2_i\)&lt;/span&gt;，然后输入 &lt;span class="math"&gt;\(TRM_2\)&lt;/span&gt;， 得到 &lt;span class="math"&gt;\(\mathbf{h}^2_{1:T-2} = TRM_2(\mathbf{h}’^{2}_{1:T-2})\)&lt;/span&gt;， 最后得到token &lt;span class="math"&gt;\(t_{i+3}\)&lt;/span&gt;  的prediction 分布 &lt;span class="math"&gt;\(p_{i+3}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;在 &lt;span class="math"&gt;\(k=3\)&lt;/span&gt; 的时候，先把 &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 的representaion &lt;span class="math"&gt;\(h_i^{k-1}=h_i^2\)&lt;/span&gt; （from &lt;span class="math"&gt;\(k=2\)&lt;/span&gt; MTP module） 和 token &lt;span class="math"&gt;\(t_{i+3}\)&lt;/span&gt; ( &lt;span class="math"&gt;\(i+k=i+3\)&lt;/span&gt; ) 的embedding concat在一起得到 &lt;span class="math"&gt;\(h’^3_i\)&lt;/span&gt;，然后输入 &lt;span class="math"&gt;\(\text{TRM}_3\)&lt;/span&gt;， 得到 &lt;span class="math"&gt;\(\mathbf{h}^3_{1:T-3} = TRM_3(\mathbf{h}’^{3}_{1:T-3})\)&lt;/span&gt;， 最后得到token &lt;span class="math"&gt;\(t_{i+4}\)&lt;/span&gt;  的prediction 分布 &lt;span class="math"&gt;\(p_{i+4}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MTP 的目标函数&lt;/strong&gt; 对每个predict depth k，其损失函数仍然是cross-entropy，定义为 &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt; = &lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt; = &lt;span class="math"&gt;\(\text{CrossEntropy}(P_{2+k:T+1}^{k}, t_{2+k:T+1})\)&lt;/span&gt; = &lt;span class="math"&gt;\(-\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^k[t_i]\)&lt;/span&gt;。其中 &lt;span class="math"&gt;\(T\)&lt;/span&gt; 是输入序列的长度, &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 是输入序列的第 &lt;span class="math"&gt;\(i\)&lt;/span&gt;-th 个真实值,  &lt;span class="math"&gt;\(p_i^k[t_i]\)&lt;/span&gt; 是第&lt;span class="math"&gt;\(k\)&lt;/span&gt;-th 个 MTP 模块对  &lt;span class="math"&gt;\(t_i\)&lt;/span&gt; 的prediciton probability.&lt;/p&gt;
&lt;p&gt;最终 MTP 的损失函数是对所有的预测深度 (&lt;span class="math"&gt;\(k\)&lt;/span&gt; 从1 到&lt;span class="math"&gt;\(D\)&lt;/span&gt;) 的平均 乘以权重 &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; ，&lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}\)&lt;/span&gt; = &lt;span class="math"&gt;\(\frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k\)&lt;/span&gt; &lt;/p&gt;
&lt;p&gt;从DS的paper公式25看，&lt;span class="math"&gt;\(\mathcal{L}_{\text{MTP}}\)&lt;/span&gt; 只是 MTP 的损失函数。最终的损失函数应该还要加上 &lt;span class="math"&gt;\(\mathcal{L}_{\text{Main}}\)&lt;/span&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MTP 在推理的时候怎么用呢？&lt;/strong&gt; MTP的主要目的是用来提高 main model的表现，在推理的时候，并不要使用MTP，而只是使用 main model来predict next token。&lt;/p&gt;
&lt;h2&gt;3. InfraStructures / 训练工程优化&lt;/h2&gt;
&lt;h3&gt;3.1. Compure clusters&lt;/h3&gt;
&lt;p&gt;DS-V3 是在2048块H800上训练的，每个cluster有8块H800，nodes内通过NVLink和NVSwitch相连，nodes间通过InfiniBand来通信。&lt;/p&gt;
&lt;h3&gt;3.2. 训练框架&lt;/h3&gt;
&lt;p&gt;正常使用GPU进行训练的时候，有两个非常大的挑战：显存效率和计算效率，也就是显卡的显存和计算能力被使用了多少，有多少是浪费了。因为现在的模型太大，一张显卡或者一个cluster都放不下，所以通常需要把模型分散到不同的机器和显卡上。显卡，cluster组建集群参见kubernets的知识。为了把模型分散到不同的显卡上，通常有这么几个办法：1. &lt;a href="https://pytorch.org/docs/stable/distributed.pipelining.html"&gt;Pipeline Parallelism / PP&lt;/a&gt;：这是对模型进行分割，对比较深的模型，模型的pipeline可以分到不同的显卡上。比如一个模型有32个transfomer，有八个显卡，那么可以每个显卡放4个，然后依次计算，这样可以使用micro-batch来充分的利用GPU。通常的PP会导致GPU闲置（见下图PP bubbles） 2. &lt;a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/tensor_parallelism"&gt;Tensor Parallelism / TP&lt;/a&gt;：这是对数据进行分割，对比较大的矩阵运算，可以分散到不同的显卡运算，这样既可以防止一张显卡容不下巨大的数据，也可以并行进行，加快速度。这些名字有时候不是很准确，其他还有Data Parallism, model parallelism. DS-V3因为有很多的Expert，所以他们还有 Expert Parallelism / EP。&lt;/p&gt;
&lt;p&gt;PP bubbles:
&lt;img alt="pipeline parrallel" src="/figures/20250308_06_deepseek_v3_pp.png"&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;setup&lt;/th&gt;
&lt;th&gt;scenario&lt;/th&gt;
&lt;th&gt;strategy&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;single node/multi-GPU&lt;/td&gt;
&lt;td&gt;fits on single GPU&lt;/td&gt;
&lt;td&gt;DistributedDataParallel or ZeRO&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;doesn’t fit on single GPU&lt;/td&gt;
&lt;td&gt;PipelineParallel, ZeRO or TensorParallel&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;largest model layer doesn’t fit&lt;/td&gt;
&lt;td&gt;TensorParallel or ZeRO&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;multi-node/multi-GPU&lt;/td&gt;
&lt;td&gt;fast inter-node connectivity (NVLink or NVSwitch)&lt;/td&gt;
&lt;td&gt;ZeRO or 3D parallelism (PipelineParallel, TensorParallel, DataParallel)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;slow inter-node connectivity&lt;/td&gt;
&lt;td&gt;ZeRO or 3D parallelism (PipelineParallel, TensorParallel, DataParallel)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;DS-V3是基于&lt;a href="https://mp.weixin.qq.com/s?__biz=Mzk0MjE3MzQ5Mg==&amp;amp;mid=2247485512&amp;amp;idx=1&amp;amp;sn=5a387c0abeb5613cbde9fa116544c25b&amp;amp;chksm=c2c67413f5b1fd0568cfa859495a89c7051e93d44123c483f7b4643c7c3787c3e2bebc6b2fed&amp;amp;cur_album_id=2981863252852998146&amp;amp;scene=189#wechat_redirect"&gt;HAI-LLM&lt;/a&gt;训练框架。DS-V3通过16路PP，64路EP分布在8个节点上，以及ZeRO-1 DP. &lt;/p&gt;
&lt;p&gt;DS主要做了这三个工程优化：1. 设计了DualPile的办法来优化PP。DualPipe有更少的 pipeline bubbles（指GPU闲置），它将Forward和Backword过程的计算和通信阶段重叠起来，从而解决了跨节点专家并行性带来的沉重通信开销的挑战。2. 其次，开发了高效的 cross-node all-to-all 通信内核，充分利用 IB 和 NVLink 带宽，节省通信专用的 Streaming Multiprocessors (SMs)。最后，优化了训练过程中的内存占用，从而能够在不使用昂贵的 TP 的情况下训练 DeepSeek-V3。&lt;/p&gt;
&lt;h4&gt;3.2.1 DualPipe&lt;/h4&gt;
&lt;p&gt;在深度学习的大模型训练中，计算（Computation） 和 通信（Communication） 是两个关键环节。1. Computation： 主要指前向Forward 和 BackPropagation computation；2. Communication： 包括不同 GPU 或计算节点间的数据传输，如参数更新、梯度交换等。跨节点 Expert Parallelism 引入的通信开销导致计算与通信比率低效。DualPipe是一个双Pipeline的架构，在Pipeline的两端同时输入 Micro-Batches 的数据，这样 Forward 和 Backward 计算可以同步进行，减少 GPU 空闲时间。同时采用Computation与Communication重叠的策略，使得Forward 和 BackPropagation 的计算任务以及数据通信能够在时间轴上更紧密地交错。通过把Forward和Backward chunk成小块，然后调整GPU SMs的使用比例，一部分用于Computation，另一部分用于Communication。&lt;/p&gt;
&lt;p&gt;DualPipe with less bubble
&lt;img alt="DualPipe and less bubble" src="/figures/20250308_07_deepseek_v3_dualpipe.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class DualPipeModel(nn.Module):
    def __init__(self, dim, rank, world_size):
        ...

    def forward_backward_pipeline(self, input_tensor):
        # 创建两个 CUDA 流，一个用于计算，一个用于通信
        compute_stream = torch.cuda.Stream()
        comm_stream = torch.cuda.Stream()

        # Forward Pass
        with torch.cuda.stream(compute_stream):
            attn_out = self.attention(input_tensor)
            mlp_out = self.mlp(attn_out)

        # Backward Pass
        with torch.cuda.stream(compute_stream):
            loss = mlp_out.sum()  
            loss.backward()

        # All-to-All Dispatch &amp;amp; Combine
        with torch.cuda.stream(comm_stream):
            tensor_to_send = input_tensor.clone()
            received_tensor = torch.zeros_like(input_tensor)
            # All-to-All 
            dist.all_to_all_single(received_tensor, tensor_to_send)

        # synchronize
        compute_stream.synchronize()
        comm_stream.synchronize()

        return received_tensor
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;3.2.2. Cross-Node All-to-All Communication&lt;/h4&gt;
&lt;p&gt;这一部分牵涉到更底层的工程细节，包括传说中的通过Nvidia的底层语言 PTX (Parallel Thread Execution) 来控制GPU的线程执行，减少通信对计算的影响，实现更高效的GPU利用，以及怎么充分利用集群的网络拓扑结构（IB &amp;amp; NVLink）。DS-V3 的原始paper里面也更多的是high-profile的描述而没有太多的细节。总体的思路是因为节点间IB的带宽比节点内NVLink的带宽要低，所以要想办法来合理分配通信路径，减少IB带宽的阻碍。&lt;/p&gt;
&lt;p&gt;为了对 Cross-Node All-to-All Communication（包括 Dispatching 和 Combining）进行优化，DS设计了优化的网络拓扑结构（IB &amp;amp; NVLink）。在 dispatching 的时候，MOE gating algorithm 决定每个token 会被发送到哪些 Expert，每个 token 先通过 IB 发送到目标 node，然后再通过 IB-to-NVLink 将数据发送到最终的GPU。前面在模型架构已经提到，每个 token 最多发送给4个Node（NVLink的带宽为160GB/s，IB为50GB/s，大概是3.2倍，所以到了Node，token还能平均有3.2个Expert）。在 Combining 的时候反过来，每个 Expert 通过 NVLink 汇总数据的结果，然后通过 NVLink-to-IB 到 node汇总，IB 的接受和汇总同样由动态调整的warp处理。&lt;/p&gt;
&lt;h4&gt;3.2.3. Minimal Overhead&lt;/h4&gt;
&lt;p&gt;这一部分讲的怎么节省GPU内存的开支。第一个是在back propagaton的时候从新计算RMSNorm和升维投影，这样就不需要储存他们的值。第二个是咋Exponential Moving Average (EMA) 计算模型表现的时候，EMA参数放到CPU内存。第三个是在multi-token预测的时候，共享Embedding和Output Head的参数。&lt;/p&gt;
&lt;h3&gt;3.3. FP8 Training&lt;/h3&gt;
&lt;p&gt;首先加一个FP的知识，计算机用二进制表示浮点数，是分为正负号（Sign）、指数部分（exponent）、尾数部分（mantissa）三部分的，完整的表示为 &lt;span class="math"&gt;\(X = (-1)^s \times  2^E \times M\)&lt;/span&gt;. 近似的可以认为：FP32和FP16比，FP越长，精度越高。在某些精度下已经等于0的数字，在更高的精度下就不是0，这样的话计算就不容易出现错误。但是更高的精度就需要更多的内存来存储，但是GPU通常内存有限。所以这也是一个需要优化的地方。&lt;/p&gt;
&lt;p&gt;&lt;img alt="FP" src="/figures/20250308_08_deepseek_v3_nvidia_fp.png"&gt;
&lt;em&gt;Fig. NVIDIA FP8 Tensor Core&lt;/em&gt;&lt;/p&gt;
&lt;h4&gt;3.3.1. Mixed Precision Framework&lt;/h4&gt;
&lt;p&gt;DS-V3 使用混合精度的FP8，大部分需要很多计算的地方都是FP8，但是在少量重要的操作上保留了原来的精度，具体见下图。1. 在GEMM矩阵相乘的时候，输入是FP8输出是BF16或者FP32. 在下图，在forward pass，activation backward pass和weight backward pass这几步矩阵相乘都是FP8的输入精度，理论上这样跟BF16比可以快一倍。在需要高精度的地方，比如embedding，output head，MOE gating module，normalizaiton，以及attention等等，仍然保持高精度。 &lt;/p&gt;
&lt;p&gt;Mixed Precision
&lt;img alt="Mixed Precision" src="/figures/20250308_09_deepseek_v3_overll_mixed_precision.png"&gt;&lt;/p&gt;
&lt;h4&gt;3.3.2. Improved Precision from Quantization and Multiplication&lt;/h4&gt;
&lt;p&gt;Fine-Grained Quantization 主要是为了解决低精度训练中 overflow 和 underflow 的问题。这些问题主要来源于 FP8 格式受限的动态范围，特别是 FP8 指数位减少导致的表示范围受限。为了解决这个问题，Fine-Grained Quantization 采用了一种更精细的量化方法，在更细粒度上进行 scaling：对activations，采用 &lt;span class="math"&gt;\(1 \times 128 \text{ tile-based scaling}\)&lt;/span&gt;，每个tile分别计算scaling； 对于weights，采用 &lt;span class="math"&gt;\(128 \times 128 \text{ block-based scaling}\)&lt;/span&gt;，每个bloack单独计算scaling factor。这样避免模型的参数会被outliers主导从而导致训练失败。在进行低精度FP8的矩阵运算 MMA (Matrix Multiply-Accumulate) 的时候，把计算的结果先累积在Tensor Core（FP8），当计算达到一定量的批次（&lt;span class="math"&gt;\(N_c\)&lt;/span&gt;）的时候，将部分结果提升到 CUDA Core（FP32），这样在保持高throttle的同时，提高了最终的计算精度。&lt;/p&gt;
&lt;p&gt;&lt;img alt="10. Quantization and Multiplication" src="/figures/20250308_10_deepseek_v3_quantization_multiplicaiton.png"&gt;&lt;/p&gt;
&lt;h3&gt;3.4. Inference and Deployment&lt;/h3&gt;
&lt;p&gt;Stay tuned!&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf"&gt;DeepSeek-V3 Technical Report&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category><category term="AIG"></category></entry><entry><title>Prediction in decoder and KV-Cache</title><link href="/pages/2024/04/21/prediction-in-decoder-and-kv-cache/" rel="alternate"></link><published>2024-04-21T00:00:00-05:00</published><updated>2024-04-21T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2024-04-21:/pages/2024/04/21/prediction-in-decoder-and-kv-cache/</id><summary type="html">&lt;h2&gt;&lt;span style="color:blue"&gt;1. Prediciton in Decoder&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;在前面&lt;a href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/"&gt;GPT summary&lt;/a&gt;里面对GPT的模型有一个综合的介绍，这里用一个fake example来解释一步步GPT是怎么做的，self attention是怎么计算的，KV cache是怎么回事。&lt;/p&gt;
&lt;p&gt;GPT是decoder only的模型，根据前面的token来预测下一个token。比如有一个句子 "it is sunny today."，现在有初始输入 …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;&lt;span style="color:blue"&gt;1. Prediciton in Decoder&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;在前面&lt;a href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/"&gt;GPT summary&lt;/a&gt;里面对GPT的模型有一个综合的介绍，这里用一个fake example来解释一步步GPT是怎么做的，self attention是怎么计算的，KV cache是怎么回事。&lt;/p&gt;
&lt;p&gt;GPT是decoder only的模型，根据前面的token来预测下一个token。比如有一个句子 "it is sunny today."，现在有初始输入 prompt “it is”，下面看怎么预测 “sunny” 和 “today”。假设单词表 &lt;code&gt;vocabulary = {"it", "is", "sunny", "today", "&amp;lt;EOS&amp;gt;"}&lt;/code&gt;. 输入的prompt “it is”，对应的token id 是 &lt;code&gt;[0,1]&lt;/code&gt;,&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style="color:blue"&gt;1. Predict 1st token&lt;/span&gt;&lt;/strong&gt;. prompt &lt;code&gt;[“it", "is”]&lt;/code&gt;，token id 是 &lt;code&gt;[0,1]&lt;/code&gt;。 为了简化问题，假设embedding+position encoding为 "it" (pos 1): &lt;code&gt;[0.1, 0.3, 0.3, 0.5]&lt;/code&gt;，"is" (pos 2): &lt;code&gt;[0.6, 0.6, 0.8, 0.8]&lt;/code&gt;。为了简化计算且不失一般性，假设 &lt;span class="math"&gt;\(W\)&lt;/span&gt;为单位阵 &lt;span class="math"&gt;\(I\)&lt;/span&gt;，也就是 &lt;span class="math"&gt;\(W_K = W_Q = W_V = I_4\)&lt;/span&gt;
    - 现有序列&lt;code&gt;["it", "is"]&lt;/code&gt;，对应的输入矩阵为 
        &lt;/p&gt;
&lt;div class="math"&gt;$$X =
            \begin{bmatrix}
            0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
            0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8
            \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
计算 self-attention. 因为&lt;span class="math"&gt;\(W=I\)&lt;/span&gt;, 所以 &lt;span class="math"&gt;\(𝑄 = 𝐾 = 𝑉 = 𝑋\)&lt;/span&gt;. self attention = &lt;span class="math"&gt;\(\frac{QK^T}{\sqrt{d_k}}\)&lt;/span&gt;, &lt;span class="math"&gt;\(d_k = 4\)&lt;/span&gt;, 
        &lt;/p&gt;
&lt;div class="math"&gt;$$
            QK^T = \begin{bmatrix}
                0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
                0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8
                \end{bmatrix}
                \begin{bmatrix}
                0.1 &amp;amp; 0.6 \\
                0.3 &amp;amp; 0.6 \\
                0.3 &amp;amp; 0.8 \\
                0.5 &amp;amp; 0.8
                \end{bmatrix}
                \propto \begin{bmatrix}
                \text{["it" "it"}] &amp;amp;  \text{["it" "is"]}\\
                \text{["is" "it"}] &amp;amp;  \text{["is" "is"]}
                \end{bmatrix}
        $$&lt;/div&gt;
&lt;p&gt;
除以&lt;span class="math"&gt;\(\sqrt{d_k}\)&lt;/span&gt;以后得到 
        &lt;/p&gt;
&lt;div class="math"&gt;$$\frac{QK^T}{\sqrt{d_k}} = 
                \begin{bmatrix}
                0.16 &amp;amp; 0.52 \\
                0.52 &amp;amp; 1.18
                \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
因为GPT是从前面的token来预测当前token，后面的token是看不到的。所以需要mask，表现在self attention矩阵上就是跟未来的token的权重为0. 更直白一点，self attention weights矩阵是下三角阵。
        &lt;/p&gt;
&lt;div class="math"&gt;$$
            \frac{QK^T}{\sqrt{d_k}} \text{ with mask } = \begin{bmatrix}
                0.16 &amp;amp; -∞ \\
                0.52 &amp;amp; 1.18
                \end{bmatrix}
        $$&lt;/div&gt;
&lt;p&gt;
进行softmax以后得到
        &lt;/p&gt;
&lt;div class="math"&gt;$$
            \text{Attention weights} = \begin{bmatrix}
                1.0 &amp;amp; 0.0 \\
                0.341 &amp;amp; 0.659
                \end{bmatrix}
        $$&lt;/div&gt;
&lt;p&gt;
再计算 weights 矩阵 和 &lt;span class="math"&gt;\(V\)&lt;/span&gt;的乘积, &lt;span class="math"&gt;\(\text{weights} \cdot V\)&lt;/span&gt;:
        &lt;/p&gt;
&lt;div class="math"&gt;$$
            \text{Attention} =
                \begin{bmatrix}
                1.0 \times 0.1 + 0.0 \times 0.6 &amp;amp; \ldots \\ % \text{"it" attends only to itself}
                0.341 \times 0.1 + 0.659 \times 0.6 &amp;amp; \ldots % \text{"is" attends to both}
                \end{bmatrix}
                =
                \begin{bmatrix}
                0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
                0.447 &amp;amp; 0.529 &amp;amp; 0.653 &amp;amp; 0.706
                \end{bmatrix}
        $$&lt;/div&gt;
&lt;p&gt;
得到attention以后的V的weighted average以后，position 2的值&lt;code&gt;[0.447, 0.529, 0.653, 0.706]&lt;/code&gt;再经过FFN（这里简化成一个 MLP）最有投影到 vocabulary 空间上得到logits。然后softmax over logits得到概率，比如 &lt;code&gt;"sunny" = 0.7&lt;/code&gt;, &lt;code&gt;"today" = 0.2&lt;/code&gt;, etc.). 最后根据position 2 的 Logits 得到 Sample &lt;code&gt;"sunny"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style="color:blue"&gt;1. Predict 2nd token&lt;/span&gt;&lt;/strong&gt;. 有了 &lt;code&gt;["it", "is", "sunny"]&lt;/code&gt;以后，继续预测下一个单词。sunny 对应的 embedding为 &lt;code&gt;[1.1, 0.9, 1.3, 1.1]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;现在的输入矩阵相应的有3行，
        &lt;/p&gt;
&lt;div class="math"&gt;$$X =
        \begin{bmatrix}
        0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
        0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8 \\
        1.1 &amp;amp; 0.9 &amp;amp; 1.3 &amp;amp; 1.1 \\
        \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
因为&lt;span class="math"&gt;\(W = I\)&lt;/span&gt;, 所以仍然&lt;span class="math"&gt;\(𝑄 = 𝐾 = 𝑉 = X\)&lt;/span&gt;，计算新的 attention 矩阵
        &lt;/p&gt;
&lt;div class="math"&gt;$$
        \frac{QK^\top}{\sqrt{d_k}} =
        \begin{bmatrix}
        0.16 &amp;amp; 0.52 &amp;amp; 0.74 \\
        0.52 &amp;amp; 1.18 &amp;amp; 1.66 \\
        0.74 &amp;amp; 1.66 &amp;amp; 2.34
        \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
同样，GPT decoder下，因为每个token只能跟它和它之前的token做attention，它之后的token都没有attention。
        &lt;/p&gt;
&lt;div class="math"&gt;$$
        \text{Causal Mask} =
        \begin{bmatrix}
        0.16 &amp;amp; -\infty &amp;amp; -\infty \\
        0.52 &amp;amp; 1.18 &amp;amp; -\infty \\
        0.74 &amp;amp; 1.66 &amp;amp; 2.34
        \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
把这个矩阵做softmax归一化，沿着dimension 1 做normalization，得到
            &lt;/p&gt;
&lt;div class="math"&gt;$$\text{Attention weights} = \begin{bmatrix}
            1.0 &amp;amp; 0.0 &amp;amp; 0.0 \\
            0.341 &amp;amp; 0.659 &amp;amp; 0.0 \\
            0.136 &amp;amp; 0.309 &amp;amp; 0.555
            \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
把这个矩阵再跟&lt;span class="math"&gt;\(V\)&lt;/span&gt;相乘，最后得到
        &lt;/p&gt;
&lt;div class="math"&gt;$$\text{Attention}  \begin{bmatrix}
        0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
        0.447 &amp;amp; 0.529 &amp;amp; 0.653 &amp;amp; 0.706 \\
        0.847 &amp;amp; 0.779 &amp;amp; 1.041 &amp;amp; 0.961
        \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
根据最后一个输入的hidden status，&lt;code&gt;[0.847, 0.779, 1.041, 0.961]&lt;/code&gt;，同样经过FFN，最后再把它投影到vocabulary 空间上得到logits。然后根据softmax选择概率最大的单词 &lt;code&gt;today&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;&lt;span style="color:blue"&gt;2. PV cache in decoder models&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;从上面的计算可以看出，在预测第一个单词的时候，&lt;span class="math"&gt;\(K = Q = X\)&lt;/span&gt;, 
            &lt;/p&gt;
&lt;div class="math"&gt;$$X =
            \begin{bmatrix}
            0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
            0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8
            \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
当预测第二个单词的时候，新的&lt;span class="math"&gt;\(K，V\)&lt;/span&gt;第第一行，第二行（对应的"it", "is"）仍然还是原来的值，这个时候如果从新计算的话，会额外的占用GPU资源（从embedding开始，跟&lt;span class="math"&gt;\(W\)&lt;/span&gt;做矩阵相乘）。其实只要在每一步计算的时候，把前面的已经计算的值cache下来，这样就节省了计算资源。KV cache就是这个思路。&lt;/p&gt;
&lt;p&gt;从新梳理一下，在计算第一个token的时候，cache下 &lt;span class="math"&gt;\(K_{cache} =V_{cache}\)&lt;/span&gt; 
            &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix}
            0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
            0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8
            \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt; 
然后当预测第二个token的时候，计算新predict的token的&lt;span class="math"&gt;\(K\)&lt;/span&gt;和&lt;span class="math"&gt;\(Q\)&lt;/span&gt;, &lt;span class="math"&gt;\(Q_{new} = [1.1,0.9,1.3,1.1]\)&lt;/span&gt;， &lt;span class="math"&gt;\(K_{new} = V_{new} = [1.1,0.9,1.3,1.1]\)&lt;/span&gt;。这样就得到新的 &lt;span class="math"&gt;\(K_{cache}\)&lt;/span&gt;, 
            &lt;/p&gt;
&lt;div class="math"&gt;$$\begin{bmatrix}
                0.1 &amp;amp; 0.3 &amp;amp; 0.3 &amp;amp; 0.5 \\
                0.6 &amp;amp; 0.6 &amp;amp; 0.8 &amp;amp; 0.8 \\
                1.1 &amp;amp; 0.9 &amp;amp; 1.3 &amp;amp; 1.1 \\
                \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;
同样，因为我们的&lt;span class="math"&gt;\(W\)&lt;/span&gt;设置，&lt;span class="math"&gt;\(V_{cache}\)&lt;/span&gt; 也是这个矩阵。这样就可以算出新的attention weight为 &lt;span class="math"&gt;\(Q_{new} \cdot K_{cache}^T / \sqrt{4} =  [0.74, 1.66, 2.34]\)&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import torch

# Setup
W_Q = W_K = W_V = torch.eye(4)
embeddings = {0: torch.tensor([0.1, 0.3, 0.3, 0.5]),  # &amp;quot;it&amp;quot;
              1: torch.tensor([0.6, 0.6, 0.8, 0.8]),  # &amp;quot;is&amp;quot;
              2: torch.tensor([1.1, 0.9, 1.3, 1.1])}  # &amp;quot;sunny&amp;quot;

# Initialize cache
K_cache = torch.empty(0, 4)
V_cache = torch.empty(0, 4)

def self_attention_with_cache(X_new, K_cache, V_cache):
    Q = X_new @ W_Q
    K_new = X_new @ W_K
    V_new = X_new @ W_V
    K_cache = torch.cat([K_cache, K_new], dim=0)
    V_cache = torch.cat([V_cache, V_new], dim=0)
    scores = Q @ K_cache.T / (4 ** 0.5)
    weights = torch.softmax(scores, dim=-1)
    output = weights @ V_cache
    return output, K_cache, V_cache

# Process prompt
prompt = torch.stack([embeddings[0], embeddings[1]])
for i in range(prompt.size(0)):
    output, K_cache, V_cache = self_attention_with_cache(prompt[i:i+1], K_cache, V_cache)
print(&amp;quot;Prompt output:&amp;quot;, output)

# Predict next token
new_token = embeddings[2].unsqueeze(0)
output, K_cache, V_cache = self_attention_with_cache(new_token, K_cache, V_cache)
print(&amp;quot;Next token output:&amp;quot;, output)
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category><category term="AGI"></category><category term="GPT"></category></entry><entry><title>Image Generation 2: Latent Diffusion model / Stable Diffusion</title><link href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/" rel="alternate"></link><published>2023-10-01T00:00:00-05:00</published><updated>2023-10-01T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2023-10-01:/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/</id><summary type="html">&lt;p&gt;In the &lt;a href="https://songhuiming.github.io/pages/2023/07/04/image-generation-1-diffusion-model/"&gt;previous blog&lt;/a&gt; we introduced diffusion model (DDPM) which is to learn the step (time &lt;span class="math"&gt;\(t\)&lt;/span&gt;) and the noise function (NN model) by adding Gaussian noise to an image step by step and reversing the process by denosiing from Gaussian noise to an image. Diffusion model is the most …&lt;/p&gt;&lt;script type='text/javascript'&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;In the &lt;a href="https://songhuiming.github.io/pages/2023/07/04/image-generation-1-diffusion-model/"&gt;previous blog&lt;/a&gt; we introduced diffusion model (DDPM) which is to learn the step (time &lt;span class="math"&gt;\(t\)&lt;/span&gt;) and the noise function (NN model) by adding Gaussian noise to an image step by step and reversing the process by denosiing from Gaussian noise to an image. Diffusion model is the most important part of Gen AI, however, there are still a few steps from Diffusion model to Gen AI: 
1. How to comsume the prompts (instructions to Gen AI to generate a response) in the model? 
2. Diffusion model needs huge computaiton as it is running on image pixel directly. 
To sovle this, latent diffusion model (stable diffusion) was introduced to project the pixel level image to the smaller latent space with perceptual conpression. &lt;/p&gt;
&lt;h2&gt;What problem Latent Diffusion Models (LDM) solve?&lt;/h2&gt;
&lt;p&gt;Latent diffusion model (LDM) applies diffusion model on the latent space from encoder rather than the pixel level data. Diffusion model (DDPM) directly adds noise to the image and uses UNet to learn the noise in the denoise step. One challenge here is that it requires a lot of computation with high dimension RGB images in the training step. Also because the noise and denoise needs to be run for many different steps, it makes the computation more expensive and time consumable to run. To solve this problem, the LDM paper (&lt;a href="https://arxiv.org/abs/2112.10752"&gt;High-Resolution Image Synthesis with Latent Diffusion Models
&lt;/a&gt;) proposed an approach to use an encoder (&lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt;) to convert the image to a latent space &lt;span class="math"&gt;\(z\)&lt;/span&gt; (which has less dimension than the original image) and and a decoder (&lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt;) to convert the latent space to image. Diffusion model is then trained on the latent space with better scaling properties because of lower spatial dimension. The reduced complexity also makes it more efficient in image generation from the latent space.&lt;/p&gt;
&lt;p&gt;&lt;img alt="stable diffusion 02" src="/figures/20231001_stable_diffusion_model_02.png"&gt;&lt;/p&gt;
&lt;p&gt;The reason of DP can be done in the latent space is that most bits of a digital image correspond to imperceptible details. After the perceptual compression through the encoder projected to the latent space, most of the perceptual information will still be kept and it only eliminates the imperceptible details. The LDM as a generative model learns the semantic and conceptual composition of the data.&lt;/p&gt;
&lt;p&gt;In order to generate images from text prompt &lt;span class="math"&gt;\(y\)&lt;/span&gt;, that is, to predict the probability of latent space &lt;span class="math"&gt;\(z\)&lt;/span&gt; given prompt &lt;span class="math"&gt;\(y\)&lt;/span&gt;, the authors also design the archtecture of conditional denoising autoencoder &lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t, y)\)&lt;/span&gt; which is UNet similar to DM to control the synthesis process with input &lt;span class="math"&gt;\(y\)&lt;/span&gt; by concating it with the latent space. &lt;/p&gt;
&lt;h2&gt;How does Latent Diffusion Model work?&lt;/h2&gt;
&lt;h3&gt;1. Perceptual Image Compression&lt;/h3&gt;
&lt;p&gt;Given an image &lt;span class="math"&gt;\(x \in \mathbb{R}^{H × W ×3}\)&lt;/span&gt; in RGB space, the encoder &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; encodes &lt;span class="math"&gt;\(x\)&lt;/span&gt; into a latent representation &lt;span class="math"&gt;\(z = \mathcal{E}(x) \in \mathbb{R}^{h×w×c}\)&lt;/span&gt;, and the decoder &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; reconstructs the image from the latent space with &lt;span class="math"&gt;\( \tilde{x} = \mathcal{D}(z) = \mathcal{D}(\mathcal{E}(x))\)&lt;/span&gt;. The encoder downsamples the image by a factor &lt;span class="math"&gt;\(f = H/h = W/w\)&lt;/span&gt; where downsampling factors &lt;span class="math"&gt;\(f = 2^m\)&lt;/span&gt;, with &lt;span class="math"&gt;\(m \in \mathcal{N}\)&lt;/span&gt;. For example, if &lt;span class="math"&gt;\(m=3\)&lt;/span&gt;, the original image &lt;span class="math"&gt;\(x\)&lt;/span&gt; size will be reduced about &lt;span class="math"&gt;\(64\)&lt;/span&gt; times less.&lt;/p&gt;
&lt;h3&gt;2. Latent Diffusion Models&lt;/h3&gt;
&lt;p&gt;Similar to &lt;a href="https://songhuiming.github.io/pages/2023/07/04/image-generation-1-diffusion-model/"&gt;DP model&lt;/a&gt; which is to find the &lt;span class="math"&gt;\(\epsilon_{\theta}(x_t, t)\)&lt;/span&gt; to approximate the error &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; given input image &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; at step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, latent diffusion model is to find &lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t)\)&lt;/span&gt; given the latent space &lt;span class="math"&gt;\(z_t\)&lt;/span&gt; at step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, where &lt;span class="math"&gt;\(\epsilon_{\theta}\)&lt;/span&gt; is UNet from 2D convolutional layers. So, starting from an image &lt;span class="math"&gt;\(x_t\)&lt;/span&gt;, LDM includes these steps:
1. Encoder &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; to convert an image &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; to latent space &lt;span class="math"&gt;\(z_t\)&lt;/span&gt;
2. Build UNet to learn &lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t)\)&lt;/span&gt; 
3. Sample from &lt;span class="math"&gt;\(P(z)\)&lt;/span&gt; and apply the decoder &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; to convert &lt;span class="math"&gt;\(z_t\)&lt;/span&gt; back to &lt;span class="math"&gt;\(x_t\)&lt;/span&gt;&lt;/p&gt;
&lt;h3&gt;3. Conditioning mechanism to generate image from prompt/condition&lt;/h3&gt;
&lt;p&gt;To generate contents from input like prompt &lt;span class="math"&gt;\(y\)&lt;/span&gt;, the model will not only depends on the time step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, but also the input &lt;span class="math"&gt;\(y\)&lt;/span&gt;, that is the conditional distribution of &lt;span class="math"&gt;\(p(z_t | y)\)&lt;/span&gt;. Accordingly, &lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t)\)&lt;/span&gt; is not only the funcion of &lt;span class="math"&gt;\(z_t\)&lt;/span&gt; and &lt;span class="math"&gt;\(t\)&lt;/span&gt;, but also with input &lt;span class="math"&gt;\(y\)&lt;/span&gt;. That is, &lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t, y)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="stable diffusion 01" src="/figures/20231001_stable_diffusion_model_01.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Notations for the figure above&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="math"&gt;\(x\)&lt;/span&gt;: input of the image. If RGB, its dimension is &lt;span class="math"&gt;\(H \times W \times 3\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; are the encoder and decoder, where &lt;span class="math"&gt;\(\mathcal{E}\)&lt;/span&gt; is to project the image to a low dimension space &lt;span class="math"&gt;\(z = \mathcal{E}(x)\)&lt;/span&gt; which will save the calculation in the later steps, and  &lt;span class="math"&gt;\(\mathcal{D}\)&lt;/span&gt; is to project from the latent space to the image.&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\tau_{\theta}\)&lt;/span&gt; is the encoder to project the conditional input (e.g., prompt text) to the intermediate space&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(\epsilon_{\theta}(z_t, t, \tau_{\theta}(y)\)&lt;/span&gt; is the conditional denosing autoencoder to denoise from &lt;span class="math"&gt;\(z_t\)&lt;/span&gt; to &lt;span class="math"&gt;\(z_{t-1}\)&lt;/span&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors develop the attention mechanism to model the cross-attention relationships among the inputs by augmenting the UNet backbone with cross-attention mechanism. The attention is between the condition encoder &lt;span class="math"&gt;\(\tau_{\theta}(y)\)&lt;/span&gt; and the each intermediate representation layer &lt;span class="math"&gt;\(i\)&lt;/span&gt; of UNet. To do this, they first build an encoder &lt;span class="math"&gt;\(\tau_{\theta}\)&lt;/span&gt; to project &lt;span class="math"&gt;\(y\)&lt;/span&gt; to intermedidate representation &lt;span class="math"&gt;\(\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}\)&lt;/span&gt;. The cross-attention is between the &lt;span class="math"&gt;\(\tau_{\theta}(y)\)&lt;/span&gt; and the projection of the latent space &lt;span class="math"&gt;\(\varphi_i(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}\)&lt;/span&gt; in the UNet with attention defined as 
&lt;/p&gt;
&lt;div class="math"&gt;$$ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^{T}}{\sqrt{d}} \right) \cdot V$$&lt;/div&gt;
&lt;p&gt; and&lt;/p&gt;
&lt;div class="math"&gt;$$\color{blue}{Q = \varphi_{i}(z_t) \cdot W_{Q}^{(i)} , \ \ K = \tau_{\theta}(y) \cdot  W_{K}^{(i)} , \ \ V = \tau_{\theta}(y) \cdot W_{V}^{(i)} }$$&lt;/div&gt;
&lt;p&gt; 
Here &lt;span class="math"&gt;\( \varphi_{i}(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}\)&lt;/span&gt; denotes a fattened intermediate representation layer &lt;span class="math"&gt;\(i\)&lt;/span&gt; of the UNet implementing &lt;span class="math"&gt;\(\epsilon_{\theta}\)&lt;/span&gt;. And &lt;span class="math"&gt;\(\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}\)&lt;/span&gt; is the embedding of the conditinal input.  The projection coefficient &lt;span class="math"&gt;\(\color{blue}{W_{Q}^{(i)} \in \mathbb{R} ^{d_{\epsilon}^i \times d}}\)&lt;/span&gt;,  &lt;span class="math"&gt;\(\color{blue}{W_{K}^{(i)} \in \mathbb{R} ^{d_{\tau}  \times d }}\)&lt;/span&gt;, &lt;span class="math"&gt;\(\color{blue}{W_{V}^{(i)} \in \mathbb{R} ^{d_{\tau}  \times d }}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: it seems the notations in the &lt;a href="https://arxiv.org/pdf/2112.10752.pdf"&gt;original paper&lt;/a&gt; is not correct. In the original paper, the projection is written as
&lt;/p&gt;
&lt;div class="math"&gt;$$\color{red}{Q = W_{Q}^{(i)} \cdot \varphi_{i}(z_t), \ \ K = W_{K}^{(i)} \cdot \tau_{\theta}(y), \ \ V = W_{V}^{(i)} \cdot \tau_{\theta}(y)}$$&lt;/div&gt;
&lt;p&gt; and the size of the weights matrix  &lt;span class="math"&gt;\(\color{red} {W_{V}^{(i)} \in \mathbb{R} ^{d \times d_{\epsilon}^i} }\)&lt;/span&gt;,  &lt;span class="math"&gt;\(\color{red} {W_{Q}^{(i)} \in \mathbb{R} ^{d \times d_{\tau}} }\)&lt;/span&gt;, &lt;span class="math"&gt;\(\color{red} {W_{K}^{(i)} \in \mathbb{R} ^{d \times d_{\tau}} }\)&lt;/span&gt; which is not right. Because &lt;span class="math"&gt;\(\color{red} { \varphi_{i}(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}}\)&lt;/span&gt;,  it is easy to find that the matrix multipicaiton &lt;span class="math"&gt;\(\color{red}{Q = W_{Q}^{(i)} \cdot \varphi_{i}(z_t)}\)&lt;/span&gt; cannot be conducted if the size is as described in the paper.&lt;/p&gt;
&lt;p&gt;Based on this, the conditional LDM is to learn &lt;span class="math"&gt;\(\epsilon_{\theta}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\tau_{\theta}\)&lt;/span&gt; from 
&lt;/p&gt;
&lt;div class="math"&gt;$$ L_{LDM} := \mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0, 1), t}  \left[ ||\epsilon - \epsilon_{\theta}(z_t, t, \tau_{\theta}(y))||_2^2 \right] $$&lt;/div&gt;
&lt;h3&gt;Code for LDM&lt;/h3&gt;
&lt;h4&gt;1. Encoder-Decoder - project image to latent space and project from latent space to image&lt;/h4&gt;
&lt;p&gt;In order to avoid arbitrarily high-variance latent spaces, the authors experiment with two different kinds of regularizations. The first variant is KL-reg which imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE. The second is VQ-reg which uses a vector quantization layer within the decoder.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class AutoencoderKL(pl.LightningModule):
    def __init__(self, ddconfig, ...):
        super().__init__()

    ......

    def encode(self, x):
        h = self.encoder(x)
        moments = self.quant_conv(h)
        posterior = DiagonalGaussianDistribution(moments)
        return posterior

    def decode(self, z):
        z = self.post_quant_conv(z)
        dec = self.decoder(z)
        return dec
    ... ...


class VQModel(pl.LightningModule):
    def __init__(self, ddconfig, ...):
        super().__init__()

    ... ...

    def encode(self, x):
        h = self.encoder(x)
        h = self.quant_conv(h)
        quant, emb_loss, info = self.quantize(h)
        return quant, emb_loss, info

    def encode_to_prequant(self, x):
        h = self.encoder(x)
        h = self.quant_conv(h)
        return h

    def decode(self, quant):
        quant = self.post_quant_conv(quant)
        dec = self.decoder(quant)
        return dec

    def decode_code(self, code_b):
        quant_b = self.quantize.embed_code(code_b)
        dec = self.decode(quant_b)
        return dec
    ......
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;2. Embedding funciton for conditional input from &lt;span class="math"&gt;\(y\)&lt;/span&gt; to &lt;span class="math"&gt;\(\tau_{\theta}(y)\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;Deifferent embedding functions can be applied to convert the conditonal input to the numeric embeding. Like BERT embedding for text, or CLIP embedding for text or image depending on the input data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
class TransformerEmbedder(AbstractEncoder):
    &amp;quot;&amp;quot;&amp;quot;Some transformer encoder layers&amp;quot;&amp;quot;&amp;quot;
    def __init__(self, n_embed, n_layer, vocab_size, max_seq_len=77, device=&amp;quot;cuda&amp;quot;):
        super().__init__()
        ...

class BERTEmbedder(AbstractEncoder):
    &amp;quot;&amp;quot;&amp;quot;Uses the BERT tokenizr model and add some transformer encoder layers&amp;quot;&amp;quot;&amp;quot;
    def __init__(self, n_embed, n_layer, vocab_size=30522, max_seq_len=77,
                 device=&amp;quot;cuda&amp;quot;,use_tokenizer=True, embedding_dropout=0.0):
        super().__init__()
        ...

class FrozenCLIPTextEmbedder(nn.Module):
    &amp;quot;&amp;quot;&amp;quot;
    Uses the CLIP transformer encoder for text.
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, version='ViT-L/14', device=&amp;quot;cuda&amp;quot;, max_length=77, n_repeat=1, normalize=True):
        super().__init__()
        ...


class FrozenClipImageEmbedder(nn.Module):
    &amp;quot;&amp;quot;&amp;quot;
    Uses the CLIP image encoder.
    &amp;quot;&amp;quot;&amp;quot;
    def __init__(self, **vargs):
        super().__init__()
        ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;3. Attention - attention between K and Q for new V&lt;/h4&gt;
&lt;p&gt;Query is from the image in DDPM or the latent space is LDM. Key and Value are from the context or the conditions in LDM.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class CrossAttention(nn.Module):
    def __init__(self, query_dim, context_dim=None, heads=8, dim_head=64, dropout=0.):
        super().__init__()
        inner_dim = dim_head * heads
        context_dim = default(context_dim, query_dim)

        self.to_q = nn.Linear(query_dim, inner_dim, bias=False)
        self.to_k = nn.Linear(context_dim, inner_dim, bias=False)
        self.to_v = nn.Linear(context_dim, inner_dim, bias=False)

    def forward(self, x, context=None, mask=None):
        h = self.heads
        q = self.to_q(x)
        context = default(context, x)
        k = self.to_k(context)
        v = self.to_v(context)

        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -&amp;gt; (b h) n d', h=h), (q, k, v))
        sim = einsum('b i d, b j d -&amp;gt; b i j', q, k) * self.scale

        # attention, what we cannot get enough of
        attn = sim.softmax(dim=-1)
        ... ...
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;4. UNet - learn the noise for step &lt;span class="math"&gt;\(t\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;From the code, it shows that for each level of UNet, there is attention block between condition input &lt;span class="math"&gt;\(\tau_{\theta}(y)\)&lt;/span&gt; and the UNet intermediate representation &lt;span class="math"&gt;\(\varphi_i(z_t)\)&lt;/span&gt;. The attention mechanism is applied in both down sample and up sample levels in UNet.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class UNetModel(nn.Module):
    def __init__(self, ......):
        super().__init__()
        ......

        # param channel_mult: channel multiplier for each level of the UNet.
        for level, mult in enumerate(channel_mult):
            for _ in range(num_res_blocks):
                layers = [ResBlock(...)]

                ch = mult * model_channels
                if ds in attention_resolutions:
                    ......
                    layers.append(
                        AttentionBlock(
                            ch,
                            use_checkpoint=use_checkpoint,
                            num_heads=num_heads,
                            num_head_channels=dim_head,
                            use_new_attention_order=use_new_attention_order,
                        ) if not use_spatial_transformer else SpatialTransformer(
                            ch, num_heads, dim_head, depth=transformer_depth, context_dim=context_dim
                        )
                    )

        # Similar attention is applied in the upsampling steps
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;5. Loss funcion - loss between the model output and the target, target to optimize&lt;/h4&gt;
&lt;p&gt;Steps here are:
1. Sample data from the funcion &lt;span class="math"&gt;\(q(x)\)&lt;/span&gt;
2. Input the sampled data and step &lt;span class="math"&gt;\(t\)&lt;/span&gt; into the model
3. Calc the MSE loss between the model output and the target&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def p_losses(self, x_start, t, noise=None):
    noise = default(noise, lambda: torch.randn_like(x_start))
    x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)
    model_out = self.model(x_noisy, t)

    loss_dict = {}
    if self.parameterization == &amp;quot;eps&amp;quot;:
        target = noise
    elif self.parameterization == &amp;quot;x0&amp;quot;:
        target = x_start
    else:
        raise NotImplementedError(f&amp;quot;Paramterization {self.parameterization} not yet supported&amp;quot;)

    loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2112.10752"&gt;High-Resolution Image Synthesis with Latent Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/CompVis/latent-diffusion"&gt;Latent Diffusion Models&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category><category term="AIG"></category></entry><entry><title>Image Generation 1: Diffusion model</title><link href="/pages/2023/07/04/image-generation-1-diffusion-model/" rel="alternate"></link><published>2023-07-04T00:00:00-05:00</published><updated>2023-07-04T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2023-07-04:/pages/2023/07/04/image-generation-1-diffusion-model/</id><summary type="html">&lt;p&gt;The &lt;a href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/"&gt;previous notes introduced the text generation models (GPT family)&lt;/a&gt;. This reading note is about image generator papers.&lt;/p&gt;
&lt;p&gt;Similar to text generator which generate the next token, OpenAI has &lt;a href="https://openai.com/research/image-gpt"&gt;image-GPT&lt;/a&gt; which is a large transformer trained on next pixel prediction in which the pixels are concated into a vector to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The &lt;a href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/"&gt;previous notes introduced the text generation models (GPT family)&lt;/a&gt;. This reading note is about image generator papers.&lt;/p&gt;
&lt;p&gt;Similar to text generator which generate the next token, OpenAI has &lt;a href="https://openai.com/research/image-gpt"&gt;image-GPT&lt;/a&gt; which is a large transformer trained on next pixel prediction in which the pixels are concated into a vector to build the autoregressive model. This model is time consuming as one image has lots of pixels compared to the tokens in the sentence. &lt;/p&gt;
&lt;p&gt;Other models usually generate image from an embedding vector rather than pixel by pixel. These generative models usually have the input from the probability distribution (e.g., normal distribution). The sample vector is put into the image generator model to generate the image. These models include: 1) VAE; 2) Flow-based models; 3) GAN and 4) Diffusion models.&lt;/p&gt;
&lt;p&gt;&lt;img alt="00_summary different image generator models" src="/figures/20230628_01_diffusion_summary_00.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;VAE&lt;/strong&gt;: VAE had an encoder to convert the input image to a vector and the decoder model to convert the vector (normal distribution) to image. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Flow-based model&lt;/strong&gt;: it trains an encoder model to convert the input image to a vector (normally distributed) and the encoder is invertible that can be used to convert the vector to image. To make the encoder invertible, the input and the output of the encoder mush have the same size. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diffusion model&lt;/strong&gt;: diffusion model has the forward diffusion process that gradually adds Gaussian noise to an image until the image converge to Gaussian noise, and the reverse diffusion process where a neural network is trained to denoise from the Gaussian noise to an image. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GAN&lt;/strong&gt;: GAN consists of two simultaneously trained model, one is Generator trained to generate fake data and the other is Discriminator trained to discriminate the fake data from the real data. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;Generative Models&lt;/th&gt;
&lt;th style="text-align: left;"&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt="02 VAE" src="/figures/20230628_02_VAE.png"&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt="03 Flow-based model" src="/figures/20230628_03_Flow_based_model.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt="04 GAN model" src="/figures/20230628_04_GAN.png"&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;img alt="05 Diffusion model" src="/figures/20230628_05_Diffusion.png"&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As most of the text to image generative models like DALLE2, ImageGen, CLIP are all based on the diffusion model, we will start from the &lt;a href="https://arxiv.org/abs/2006.11239"&gt;diffusion model&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;What are Diffusion Models?&lt;/h2&gt;
&lt;p&gt;Diffusion Models include two steps: the first step is to add noice to the image step by step to convert the image to white noise, which is called Forward Diffusion Process; the second is to denoise from the white noise back to the image, which is called Reverse Diffusion Process. &lt;/p&gt;
&lt;h3&gt;Forward diffusion process&lt;/h3&gt;
&lt;p&gt;Given a data &lt;span class="math"&gt;\(\mathbf{x}_0\)&lt;/span&gt; sampled from the data distribution &lt;span class="math"&gt;\(\mathbf{x}_0 \sim q(\mathbf{x})\)&lt;/span&gt;, forward diffusion process is to add a small amount of white noise to it in &lt;span class="math"&gt;\(1, 2, \cdots, T\)&lt;/span&gt; steps to produce the sequence &lt;span class="math"&gt;\(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T\)&lt;/span&gt;. From step &lt;span class="math"&gt;\(t-1\)&lt;/span&gt; to step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, the input is degraded by &lt;span class="math"&gt;\(\sqrt{1-\beta_t}\)&lt;/span&gt; and the new added noise is &lt;span class="math"&gt;\(\sqrt{\beta_t} \boldsymbol{\epsilon}_{t-1}\)&lt;/span&gt; where &lt;span class="math"&gt;\(\beta_t \in (0, 1)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})\)&lt;/span&gt;, that is &lt;span class="math"&gt;\(\mathbf{x}_t = \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \boldsymbol{\epsilon}_{t-1}\)&lt;/span&gt;. In the experiment, the varience in each step is set to linearly increasing from &lt;span class="math"&gt;\(\beta_1 = 10^{-5}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\beta_T = 0.02\)&lt;/span&gt;. Based on the relation from &lt;span class="math"&gt;\(\mathbf{x}_t\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{x}_{t-1}\)&lt;/span&gt;, we can deduce the relation of &lt;span class="math"&gt;\(\mathbf{x}_{t-1}\)&lt;/span&gt; to &lt;span class="math"&gt;\(\mathbf{x}_{t-2}\)&lt;/span&gt; and replace &lt;span class="math"&gt;\(\mathbf{x}_{t-1}\)&lt;/span&gt; with &lt;span class="math"&gt;\(\mathbf{x}_{t-2}\)&lt;/span&gt;, and so on. After the iteration until &lt;span class="math"&gt;\(\mathbf{x}_{0}\)&lt;/span&gt;, we have the relation of &lt;span class="math"&gt;\(\mathbf{x}_{t}\)&lt;/span&gt; represented as &lt;span class="math"&gt;\(\mathbf{x}_{0}\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;$$\mathbf{x}_{t} = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}$$&lt;/div&gt;
&lt;p&gt; 
where &lt;span class="math"&gt;\(\alpha_t = 1 - \beta_t\)&lt;/span&gt; and &lt;span class="math"&gt;\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In python:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;    alpha = 1. - beta
    alpha_hat = torch.cumprod(alpha, dim=0)
    sqrt_alpha_hat = torch.sqrt(alpha_hat[t])[:, None, None, None]
    sqrt_one_minus_alpha_hat = torch.sqrt(1 - alpha_hat[t])[:, None, None, None]
    z = torch.randn_like(x)
    sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * z
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Reverse diffusion process&lt;/h3&gt;
&lt;p&gt;This step is also called denoise as it is from the noise &lt;span class="math"&gt;\(\mathbf{x}_{T}\)&lt;/span&gt; back to the image &lt;span class="math"&gt;\(\mathbf{x}_{0}\)&lt;/span&gt;, like from &lt;span class="math"&gt;\(\mathbf{x}_{T} \rightarrow \mathbf{x}_{t-1} \rightarrow \cdots \rightarrow \mathbf{x}_{1} \rightarrow \mathbf{x}_{0}\)&lt;/span&gt;. That is, given &lt;span class="math"&gt;\(\mathbf{x}_{t}\)&lt;/span&gt;, this step is to find a noise term &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_t\)&lt;/span&gt; so that &lt;span class="math"&gt;\(\mathbf{x}_{t-1} = \mathbf{x}_{t} - \boldsymbol{\epsilon}_t\)&lt;/span&gt;, Theoretially this is not doable as you need to start from the white noise &lt;span class="math"&gt;\(\mathbf{x}_{T}\)&lt;/span&gt; to get an image &lt;span class="math"&gt;\(\mathbf{x}_{0}\)&lt;/span&gt; by denoising. But is there any way to get &lt;span class="math"&gt;\(q(\mathbf{x}_{t-1} | \mathbf{x}_{t})\)&lt;/span&gt;? We can rewrite it based on Bayesian formula&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t) 
&amp;amp;= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) \frac{ q(\mathbf{x}_{t-1}  ) }{ q(\mathbf{x}_t  ) }
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;In this way we convert &lt;span class="math"&gt;\(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t)\)&lt;/span&gt; to  &lt;span class="math"&gt;\(q(\mathbf{x}_t \vert \mathbf{x}_{t-1}),  q(\mathbf{x}_{t-1}), \text{ and } q(\mathbf{x}_t)\)&lt;/span&gt;. The thing is, we still don't know &lt;span class="math"&gt;\(q(\mathbf{x}_{t-1})  \text{ and } q(\mathbf{x}_t)\)&lt;/span&gt;. But we know &lt;span class="math"&gt;\(q(\mathbf{x}_{t-1} \vert  \mathbf{x}_0)\)&lt;/span&gt; from fowward process. So now we will rewrite the formula to conditional on &lt;span class="math"&gt;\(\mathbf{x}_0\)&lt;/span&gt;, and denote it as &lt;span class="math"&gt;\(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)\)&lt;/span&gt;. That is, we want to calculate &lt;span class="math"&gt;\(\textcolor{blue} {q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;From the forward process, we know that:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
&amp;amp; \textcolor{blue}{q(\mathbf{x}_{t} \vert \mathbf{x}_{t-1})} \mbox{ with mean } \sqrt{{\alpha}_{t}}\mathbf{x}_{t-1} + \sqrt{1 - {\alpha}_{t}}\boldsymbol{\epsilon}   \text{    },  \sim \mathcal{N}\left(\sqrt{{\alpha}_{t}}\mathbf{x}_{t-1}, (1-{\alpha}_{t})\mathbf{I}\right) \\
&amp;amp; \textcolor{blue}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} \mbox{ with mean } \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1}}\boldsymbol{\epsilon}  \text{    }, \sim \mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 , (1-\bar{\alpha}_{t-1})\mathbf{I}\right) \\
&amp;amp; \textcolor{blue}{q(\mathbf{x}_{t} \vert \mathbf{x}_0)} \mbox{ with mean } \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}  \text{    },  \sim \mathcal{N}\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I}\right) 
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;From the Bayesian formula, we have:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) 
&amp;amp;= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
&amp;amp;\propto \exp \Big(-\frac{1}{2} \big(\frac{\left(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1}\right)^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&amp;amp;= \exp\Big( -\frac{1}{2} \big({\left(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right)} \mathbf{x}_{t-1}^2 -  {\left(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0\right)} \mathbf{x}_{t-1}  { + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;This is the kernel funciton for normal distribution with the mean equals to:&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t
&amp;amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0,  \text{    and   } \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&amp;amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&amp;amp;= {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}  \boldsymbol{\epsilon}_t \Big)}
\end{aligned}
$$&lt;/div&gt;
&lt;p&gt;Here &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_t\)&lt;/span&gt; is not known, it will be learned from the forward process. In forward process, from &lt;span class="math"&gt;\(\mathbf{x}_t\)&lt;/span&gt; to  &lt;span class="math"&gt;\(\mathbf{x}_{t+1}\)&lt;/span&gt; by adding a noise &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_t\)&lt;/span&gt;. The true value is from the difference of the input, and A NN model (in the paper, it is UNet) will be built to learn this added noice. &lt;/p&gt;
&lt;p&gt;In python:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;    predicted_noise = model(x, t)
    x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="06 forward reverse process" src="/figures/20230628_06_forward_reverse_process.png"&gt;&lt;/p&gt;
&lt;h3&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;&lt;img alt="Diffusion Model DDPM" src="/figures/20230628_07_Diffusion_DDPM_algorithm.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Training&lt;/strong&gt;: Combine the forward and reverse process together, in the training process, step &lt;span class="math"&gt;\(t\)&lt;/span&gt; in the forward process will generate the true value of the noise for that given step, and the reverse process will try to build a neural network model to predict the denoised term, which should be close to the noise term in the froward process. Because for any step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, it needs to learn the noise so the step &lt;span class="math"&gt;\(t\)&lt;/span&gt; will also be an input to the model.&lt;/p&gt;
&lt;p&gt;In all, there are two inputs to the initial step: a given image &lt;span class="math"&gt;\(\mathbf{x}_{0}\)&lt;/span&gt; and the step &lt;span class="math"&gt;\(t\)&lt;/span&gt;. It will learn a neural network (U-net in the original paper) &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_{\theta}\)&lt;/span&gt; to approxiate the added Gaussian noise where &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_{\theta} = \boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t  \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sampling&lt;/strong&gt;: starting from the Gaussian noise &lt;span class="math"&gt;\(\mathbf{x}_{T} \sim \mathcal{N}(0, \mathbf{I})\)&lt;/span&gt;, for each step &lt;span class="math"&gt;\(t\)&lt;/span&gt;, calculate the denoised term from the trained neural network &lt;span class="math"&gt;\(\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\)&lt;/span&gt; and then denoise from step &lt;span class="math"&gt;\(t\)&lt;/span&gt; to step &lt;span class="math"&gt;\(t-1\)&lt;/span&gt; through &lt;span class="math"&gt;\(\mathbf{x}_{t-1} = {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}  \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t) \Big)} + \sigma_t \mathbf{z}\)&lt;/span&gt; where &lt;span class="math"&gt;\(\mathbf{z}\)&lt;/span&gt; is Gaussian noise for &lt;span class="math"&gt;\(t&amp;gt;1\)&lt;/span&gt; else 0.&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://arxiv.org/abs/2006.11239"&gt;Denoising Diffusion Probabilistic Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://vsehwag.github.io/blog/2023/2/all_papers_on_diffusion.html"&gt;All papers on diffusion&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category><category term="AIG"></category></entry><entry><title>GPT-1, GPT-2, GPT-3, InstructGPT / ChatGPT and GPT-4 summary</title><link href="/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/" rel="alternate"></link><published>2023-05-28T00:00:00-05:00</published><updated>2023-05-28T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2023-05-28:/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/</id><summary type="html">&lt;h2&gt;1. GPT-1&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"&gt;Improving Language Understandingby Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;What the problem GPT-1 solve?&lt;/h3&gt;
&lt;p&gt;Before GPT-1, NLP was usually a supervised model. For each task, there are some labeled data, and then develop a suoervised model based on these labeled data. There are several problems with this approach: First, labeled data is …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. GPT-1&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf"&gt;Improving Language Understandingby Generative Pre-Training&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;What the problem GPT-1 solve?&lt;/h3&gt;
&lt;p&gt;Before GPT-1, NLP was usually a supervised model. For each task, there are some labeled data, and then develop a suoervised model based on these labeled data. There are several problems with this approach: First, labeled data is required. But NLP does not like CV which has a well-labeled data imagenet. Second, the models trained by these different tasks are not very general. For example, it is difficult to directly use the model trained by translation on classification. &lt;/p&gt;
&lt;p&gt;GPT-1 puts forward some ideas: First, there are a lot of texts without labels in reality, can we use these large amount of unlabeled data? Second, is it possible to pre-train a general model that can be transfered to handle different tasks? However, leveraging the unlabeled data is challenging for 2 main reasons. First, what objective function should be used to optimize to learn the text representation that can be transferable? Second, there is no consensus on the model to transfer these learned representations to the target task. &lt;/p&gt;
&lt;p&gt;GPT-1 used the semi-supervised approach to achieve this objective: it used a self-supervised (which is called unsupervised in the paper) pre-trained model to learn the text representation from the large amount of ublabeled data, and it used supervised models to fine-tune for each sub-task with annotated training data. I'd call it self-supervised rather than unsupervised as in ML, unsupervised model usually means the data has no labels and the task are more like clustering. Here the semi-supervised only means it does not need manual labelled data but just predict the next token (word) from the tokens (words) up to current time. More details will be introduced in the models details below.&lt;/p&gt;
&lt;p&gt;For model structure, GPT-1 uses &lt;code&gt;Transformer decoder&lt;/code&gt; because they think it provides a more structured memory for handling the long-term dependencies in text which results in robust transfer performance across different tasks. Compared to BERT which is also based on Transformer but it uses &lt;code&gt;Transformer encoder&lt;/code&gt;. This means GPT-1 chooses a more difficult way than BERT because decode only uses informaiton until &lt;span class="math"&gt;\(t\)&lt;/span&gt; to predict the next token at &lt;span class="math"&gt;\(t+1\)&lt;/span&gt;, while encoder will leverage the information before and after the masked token to predict, which is easier and performance might be better because the tokens afterwards have already been seen to make the prediciton. That is why GPT is called unidirectional (from left to right) while BERT is called Bi-directionsl (use words before and after the masked token to predict).&lt;/p&gt;
&lt;h3&gt;How does GPT-1 work (model framework)?&lt;/h3&gt;
&lt;p&gt;As introduced above, GPT-1 includes two stages. The first stage is learning a high-capacity language
model on a large corpus of text. This is followed by a fine-tuning stage, where we adapt the model to
a discriminative task with labeled data.&lt;/p&gt;
&lt;h4&gt;1. Unsupervisded (self-supervised) pre-trained model&lt;/h4&gt;
&lt;p&gt;This task is to predict the token &lt;span class="math"&gt;\(u_i\)&lt;/span&gt; based on the previous &lt;span class="math"&gt;\(k\)&lt;/span&gt; tokens &lt;span class="math"&gt;\((u_{i-k}, \cdots, u_{i-1})\)&lt;/span&gt;. Given the tokens &lt;span class="math"&gt;\(\mathcal{U} = \{u_1, \cdots, u_n \}\)&lt;/span&gt;, the objective is to maximize the probability likelihood to predict the next word
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_1(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(k\)&lt;/span&gt; is the size of the context window and the conditional probability &lt;span class="math"&gt;\(P\)&lt;/span&gt; is a NN model with parameters &lt;span class="math"&gt;\(\Theta\)&lt;/span&gt;. The bigger the &lt;span class="math"&gt;\(k\)&lt;/span&gt;, the longer of the previous text that the model will see. So a bigger &lt;span class="math"&gt;\(k\)&lt;/span&gt; usually enable the model to learn and remember better.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(L_1(\mathcal{U}) = \sum_i \log P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta) = \log \prod_i P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\prod_i P(u_i | u_{i-k}, \cdots, u_{i-1}; \Theta)\)&lt;/span&gt; is the joint probability of the prediciton for each word. So maximizing this joint probability is to find the &lt;span class="math"&gt;\(\Theta\)&lt;/span&gt; so that the predicted words are the same as the input text. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;As in the formula above, here GPT-1 choose to predict the next word based on the previous &lt;span class="math"&gt;\(k\)&lt;/span&gt; words, while BERT will use the words before and after the target word. This makes the task in GPT a little more difficult than BERT, and thus its performance may not be as good as BERT in some tasks (which is verified in BERT paper pubished several months after GPT-1). &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the experiment, the model is a multi-layer Transfromer decoder. It applies a multi-head self-attention operation over the input tokens followed by position-wise feedforward layers. Through this transformation, the author cleverly found an objective to optimize.&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
 &amp;amp; h_0 = U W_c + W_p \\
 &amp;amp; h_l = \mbox{transformer_block} (h_{l-1}), \forall i \in [1, n]  \\
 &amp;amp; P(u) = \mbox{softmax} (h_n W_e^{T})
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(U = (u_{-k}, \cdots, u_{-1})\)&lt;/span&gt; is the context token vector, &lt;span class="math"&gt;\(h_0\)&lt;/span&gt; is the mapping projection of &lt;span class="math"&gt;\(U\)&lt;/span&gt;, &lt;span class="math"&gt;\(W_c\)&lt;/span&gt; is the token embedding and &lt;span class="math"&gt;\(W_p\)&lt;/span&gt; is the position embedding. &lt;/p&gt;
&lt;h4&gt;2. Supervised fine-tuning&lt;/h4&gt;
&lt;p&gt;After the generative model is trained from the unlabelled data, GPT-1 fine tuned the pre-trained model on the subtasks by adding a task specific layer after the last layer of the pre-trained model. The parameters from the pre-trained model are adapted to the supervised subtasks by fine-tuning on the labeled data &lt;span class="math"&gt;\(\mathcal{C}\)&lt;/span&gt;, in which each instance is a sequence of input tokens &lt;span class="math"&gt;\(x^1, \cdots, x^m\)&lt;/span&gt; along  a label &lt;span class="math"&gt;\(y\)&lt;/span&gt;. The inputs are passed throught the pre-trained transformer block to get the activation &lt;span class="math"&gt;\(h_l^m\)&lt;/span&gt;, which is fed into a linear layer with softmax to predict the probability for &lt;span class="math"&gt;\(y\)&lt;/span&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y|x^1, \cdots, x^m) = \mbox{softmax}(h_l^m W_y)
$$&lt;/div&gt;
&lt;p&gt;
The objective is to maximize the likelihood
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_2(\mathcal{C}) = \sum_{(x, y)} \log (P(y|x^1, \cdots, x^m))
$$&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Auxiliary training objective&lt;/strong&gt;: rather than maximizing the likelihood funciton separately, the authors find that including language modeling as an auxiliary objective to the fine-tuning helped improving the generalizaiton of the supervised model and accelerating the convergance. So the final objective is to maximize the added two objective funciton with parameter &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt;
&lt;/p&gt;
&lt;div class="math"&gt;$$
L_3(\mathcal{C}) = L_2(\mathcal{C})+ \lambda * L_1(\mathcal{C})
$$&lt;/div&gt;
&lt;h4&gt;3. Supervised model tasks and input transformation&lt;/h4&gt;
&lt;p&gt;The input to the transformer decode is a sequence of tokens. For the subtasks like classification, it can be directly input to the transformer decoder. For other tasks like Entailment, Similarity and Multiple choices, the data was re-structured by adding some special tokens to indicate the &lt;code&gt;Start, Delim and Extract&lt;/code&gt; (&lt;code&gt;&amp;lt;BOS&amp;gt;, &amp;lt;EOS&amp;gt;, &amp;lt;PAD&amp;gt;&lt;/code&gt; etc) as is shown in the Figure 1 below. After the data being re-structured to the uniformly format, it will be split to tokens as the input to transformer decoder as is shown in Figure 2. &lt;/p&gt;
&lt;p&gt;As the previous section described, in the fine-tuning step, the objective is the auxiliary objective which is the added two objective from the next word prediction in the pre-training model and the classification task in the fine-tuning tasks. So the output from the transformer in the fine-tuning task include two parts: text prediction and the output for classifier (See Figure 2).&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 1" src="/figures/20230508_GPT_02.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 2" src="/figures/20230508_GPT_01.png"&gt;&lt;/p&gt;
&lt;h3&gt;Analysis and discussion&lt;/h3&gt;
&lt;p&gt;The experiment results will be ignored here. But in the Analysis section the authors provides some useful informaion why they choose transformer rather than the other languages models like LSTM.&lt;/p&gt;
&lt;p&gt;The first observation is that the more number of layers transferred from the pre-trained model to the fine-tuning tasks, the better the performance (accuracy) is. That is, one way to improve the model performance is to increase the number of model layers (and the layer size = d_model). That is what GPT-2/3/4 will do.&lt;/p&gt;
&lt;p&gt;The second observation is that transformer can learn to improve its language model capability with the more structured attentional memory (My understanding is that transformer has much more parameters and the attention mechanism can help to selectively remember the useful information). The author verifies this by zero-shot learning. That is, use the pre-trained model directly without fine-tuning on the subtasks. The performance of transformer is more stable and better than LSTM. That is also what GPT-2 will focus on.&lt;/p&gt;
&lt;h2&gt;2. GPT-2&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf"&gt;Language Models are Unsupervised Multitask Learners&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;What the problem GPT-2 solve?&lt;/h3&gt;
&lt;p&gt;As discussed at the end of GPT-1 (and also stimulated by BERT?), the pre-trained model performance can be improved with more complex model (more layer - deeper, higher layer size - wider). It also can be used in the target tasks directly (zero-shot learning) and beat the performance of LSTM. That is GPT-2. Similar to GPT-1, GPT-2 is also the self-supervised model with transformer decoder but much more parameters. And GPT-2 mainly focus on zero-shot learning.&lt;/p&gt;
&lt;p&gt;Why zero-shot? BERT which was introduced after GPT-1 outperformed GPT-1 in many tasks. In the last section of GPT-1, it shows the pre-trained model performance could be improved with bigger model. But if just purely increasing the parameters, the value of the GPT-2 paper may not mean too much. So GPT-2 wants to discuss from the other aspect that the model can do zero-shot: without additional training, the model can performs good in some tasks. This also shows that the gpt model has a strong generalization ability, which in fact is lacked in BERT.&lt;/p&gt;
&lt;h3&gt;How does GPT-2 work (model framework)?&lt;/h3&gt;
&lt;p&gt;Because it doesn't have fine-tuning tasks, GPT-2 does not need the special tokens like (&lt;code&gt;&amp;lt;BOS&amp;gt;, &amp;lt;EOS&amp;gt;&lt;/code&gt; as GPT-1 did. Instead, GPT-2 uses a &lt;code&gt;promot&lt;/code&gt; to control the input to the model. A prompt is a small piece of text provided to the model, and the model will generate the additional text based on this input. The prompt is task specific and depends on the specific input sequence and task.&lt;/p&gt;
&lt;p&gt;In contrast, GPT-2 uses a prompt to control the input to the model. A prompt is a small piece of text that is provided to the model as an initial input, and the model generates additional text based on this input. The prompt is task-specific and depends on the specific input sequence and task. For example, to translate to French, the prompt is like "translate to french", followed by Engligh sentences, and then French sentences. So the example data is a sequence like &lt;code&gt;(translate to french, english text, french text)&lt;/code&gt;. &lt;/p&gt;
&lt;h4&gt;1. Data&lt;/h4&gt;
&lt;p&gt;Because GPT-2 has much more parameters than GPT-1, it requires more training data. To build a large and diverse dateset to collect the natual language demonstrations of tasks, the authors used the Common Crawl data. But the quality of this data set is low, so they picked the relatively higher quality subset from this data: the posts from reddit with at least 3 karma. Another dataset is WenText which is extracted from Dragnet and Newspaper content. The data is cleaned and split by Byte Pair Encoding (BPE). &lt;/p&gt;
&lt;h4&gt;2. Model&lt;/h4&gt;
&lt;p&gt;The model is almost the same as GPT-1 which is transformer decoder, with layer norm was moved to each sub-block and an additional layer norm was added after the final self-attention block. &lt;/p&gt;
&lt;p&gt;There are different setups of the number of transformer layers and the d_model. &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Parameters&lt;/th&gt;
&lt;th&gt;Layers&lt;/th&gt;
&lt;th&gt;d_model&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;117M&lt;/td&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;768&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;345M&lt;/td&gt;
&lt;td&gt;24&lt;/td&gt;
&lt;td&gt;1024&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;762M&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;1280&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1542M&lt;/td&gt;
&lt;td&gt;48&lt;/td&gt;
&lt;td&gt;1600&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;3. Zero-shot learning performance on the NLP tasks&lt;/h4&gt;
&lt;p&gt;&lt;img alt="Table 3" src="/figures/20230508_GPT_03.png"&gt;&lt;/p&gt;
&lt;p&gt;From the experiment it shows that: 1) GPT-2 with zero-shot can outperform most of the other SOTA zero-shot models on the NLP tasks (with the corresponding datasets). 2) As the number of parameters increased, GPT-2 performance also increased. The largest model almost beat all the SOTA models on these tasks.&lt;/p&gt;
&lt;h3&gt;Analysis and discussion&lt;/h3&gt;
&lt;p&gt;GPT-2 has similar performance as the supervised model on some tasks like reading comprehension and its performance is not far away from humen performance, but on the other tasks like QA it still does not work well and is no better than random, especially it's far away from human performance. When compared with the other self-supervised pre-trained models like BERT with similar number of parameters, GPT-2 does not beat their performance on the sub-tasks. That means although zero-shot learning can provide a strong pre-trained model, it may still need some fine-tuning on the specific tasks to boost its performance on the specific tasks. The quesiton is, does it need as much labelled data as the fine-tuning in each subtask or it just needs a small amount of labeled data? The following paper GPT-3 shows only a small data already (few-shot learning or even one-shot learning) helps a lot to improve the performane.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 4" src="/figures/20230508_GPT_04.png"&gt;&lt;/p&gt;
&lt;p&gt;The second observaiton is that model performance still increases as the number of parameters increases (more layers and larger layer size). As the plot shown above, with the number of parameters increased in LM from 117M to 1542M, the perplexity decreased in both the training data and the test data. That also what GPT-3 will do: increase the number of model parameters and increase the training data size at the same time. &lt;/p&gt;
&lt;p&gt;The last finding is that GPT-2 can write news articles. Although the author did not spend too much space discussing this feature, this feature is one of the main reasons why the GPT series will shine in the future - it can generate contents. We will discuss this below. &lt;/p&gt;
&lt;h2&gt;3. GPT-3&lt;/h2&gt;
&lt;h3&gt;What the problem GPT-3 solve?&lt;/h3&gt;
&lt;p&gt;Models trained on the specific target task usually need task-specific dataset and task-specific fine-tuning, and thus need some labeled data. There are some limitations for this: 1) specific data for each task limits the generalizaiton of the trained model. And for many tasks it is hard to collect a large supervised training dataset; 2) the model performance may depends on the data: the potential of the trained model has a relation with the expressiveness of the model and the narrowness of the training distribution. When large model is fine-tuned on the parrow task distribution, large models do not necessarily generalize better. In other words, the fine-tuned model performing better does not mean the large pre-trained model is better. It may be just because the fine-tuning training data has overlaps witht the evaluaiton data; 3) humans can learn more efficiently without large amount of data.&lt;/p&gt;
&lt;p&gt;To solve these concerns, GPT-3 proposed these ideas: &lt;/p&gt;
&lt;p&gt;First: It trained a model with broad set of skills and pattern recognizaiton abiities at training time, and relies on this ability in inference time for the new tasks. Specifically, GPT-3 trains a good model with great generalizaiton capability (meta-learning) and it does not update the parameters in the large pre-trained model for the subtasks (in-context learning), but only update the output weight for that task (usually it is the last last layer) with a few data samples (few-shot learning). &lt;/p&gt;
&lt;p&gt;It is like adding a layer for the specific task. In few-shot learning, the big model parameters will not be updated so that the generalization capability and the pattern recognizaiton capability from the pre-trained model will be retained. It only updates the paramters of the last layer for that specific task with a few samples for that specific task. For example, if it is a classificaiton task, it will have a layer for calssificaion and the paramters for that layer will be upated by the calssificaiton task data. The other paramters in the pre-trained models are frozen.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 5" src="/figures/20230508_GPT_05.png"&gt;&lt;/p&gt;
&lt;p&gt;For examle in the Figure above, the big training data may have already many cases like the math sum, text work correction or translate from one language to another. In the pre-trained model, it will learn from these data to understand the task and develop a broad set of skills and pattern recognizaiton capabilities (self-supervised here with SGD). After it is learned, the big pre-trained model parameters will not be updated. It learns this capability and rapidly adapt to or recognize the task in infreence time. &lt;/p&gt;
&lt;p&gt;Second: it trained a big model with 100 times more data and 10 times more parameters to learn. GPT-3 has 175 billion parameters and is evaluated under 3 conditions: 1) few-shot learning, which uses 10 to 100 data points to tune for the specific task. 2) one-shot learning, where only one data is used; and 3) zero-shot learning, where no additional data is used. &lt;/p&gt;
&lt;h3&gt;How does GPT-3 work (model framework)?&lt;/h3&gt;
&lt;h4&gt;1. Data&lt;/h4&gt;
&lt;p&gt;GPT-2 uses reddit data from Common Crawl (CW) datasets. In GPT-3, as the model increased 10x time, the whole Common Crawl dataset was used. They also remove the duplicates of the data by comparing the two sets of the dataset. As the quality of this data is low, the authors used a model to filter out the low quality data. &lt;/p&gt;
&lt;p&gt;The final data includes filtered Common Crawl, WebText2, Books1, Books2 and Wikipedia. As the CW data quality is not high, the sampling weights for CW is set low compared to its big size. &lt;/p&gt;
&lt;h4&gt;2. Model and Approach&lt;/h4&gt;
&lt;p&gt;The model that GPT-3 used is still transformer decoder which predicts the next token based on the previous tokens. It is similar to GPT-2 structure but using alternating dense and locally banded sparse attention patterns in the layers of transformer.&lt;/p&gt;
&lt;p&gt;The authors explains the details of the approach they used, which includes few-shot, one-shot and zero shot, together with fine-tuning which they did not use, as shown in the Figure below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Figure 6" src="/figures/20230508_GPT_06.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Fine-tuning&lt;/strong&gt;: fine-tuning is to collect thousands of data points for each task to refresh the parameters in the model. It usually has strong performance on the benchmark data, but it needs a new large dataset for each task. GPT-3 can be fine-tuned and it might be a promosing direction in the future. In fact, in the following work of InstructGPT, the authors begin to tune the parameters based on the feedback from human annotations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Few-shot&lt;/strong&gt;: the model will be given a few samples data of the task at the inference time, but no weights of the pre-trained model will be updated. For example, in the inference time of translation from English to French, first it will be given the task description, then some examples (10 to 100) from English to French will be given. And then a prompt will be given, the model will automatically generate the French translation based on the input in prompt. The advantage here is that only a limited number of input data is needed. The disadvantage is that the performance of few-shot learning usually is not as good as the SOTA fine-tuned model. &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;One-shot&lt;/strong&gt;: it's similar to few-shot setup, but just one sample data is given after the task description. This is close to how the tasks are communicated to human. When asking human to do something, it is common to give an example of the task.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zero-shot&lt;/strong&gt;: there is no data after the task description. From the model using aspect, this is the easiest way to go. But it is also the most challenging setup for model to understand the task as no example is given. &lt;/p&gt;
&lt;p&gt;One potential cave for few-shot or one-shot is: how can it remember or understand the previous input data informaion when there is new input data? For example, if 10 data points are used in few-shot for the first time, if there are another 15 data points for the second time, it seems the setup of few-shot learning cannot link the input inforation from the first 10 and the second 15 data points.&lt;/p&gt;
&lt;p&gt;The experiment result part will be ignored. Generally it discussed GPT-3 performance on more than 10 tasks.&lt;/p&gt;
&lt;h3&gt;Limitations&lt;/h3&gt;
&lt;p&gt;Some limitations in GPT-3 are discussed and the potential improvements are also discussed for future work.&lt;/p&gt;
&lt;p&gt;First, it is hard and expensive to train considering the large amount of parameters and the training dataset. It requires huge computing resources and time to train the model.&lt;/p&gt;
&lt;p&gt;Second, GPT-3 has weakness in some NLP tasks like text synthesis and some comparison tasks like if two words are used in the same way in the sentence. GPT-3 has difficulty for some common sense physics quesitons to human. For example, it is hard for GPR-3 to answer "If I put cheese into the fridge, will it melt"? &lt;/p&gt;
&lt;p&gt;GPT-3 using unidirectional approaach may have structure and algorithmic limitaion coampred to the bidirection approach. As introduced in GPT-1, this may affect GPT performance in some tasks, like fill-in-the-blank tasks, tasks that involves looking back and comparing two pieces of content, or tasks that require re-reading. Alls these tasks will be easier when the context information before and after the token is known. So expanding the bidirection function in GPT model will be a promising direction for future research (learn from BERT?). &lt;/p&gt;
&lt;p&gt;A more fundmental limiation for GPT-3 is that it is a language model based on text data, so the pre-trained model has no idea about the other domains of experience. Like it cannot handle video data or real-world physical interactions. One promising direction is to include learning objective from humans and fine-tuned with reinforcement learning (that is what &lt;code&gt;InstructGPT/ChatGPT&lt;/code&gt; will do), and to add additional modalities such as images and videos (that is what &lt;code&gt;GPT-4&lt;/code&gt; will do, it will accepct images or pdf files to descirbe or summarize the input file).&lt;/p&gt;
&lt;h2&gt;4. InstructGPT / ChatGPT&lt;/h2&gt;
&lt;h3&gt;What the problem InstructGPT solve?&lt;/h3&gt;
&lt;p&gt;GPT3 limitation:
First, it does not remember the previous few-shot inputs. ChatGPT can continuous rememnber what you chated with it before and based on that context information to continue the multi-round chat with you.&lt;/p&gt;
&lt;p&gt;Second, as the model becomes bigger and bigger, the output from GPT-3 sometimes is hard to control. For example, if you ask about What is Gaussian Process, it may generate some low quality text to you. Or sometimes model may generate some harmful informaiton like biased, racist or gender discriminative text. &lt;/p&gt;
&lt;p&gt;Third, language model use self-supervised method to pre-train a big generalization model. As there is no labeled data in self-supervised model, if you want the model to answer some question like "what is Gaussian Process", the training data should have included this information so that the model can learn it. So, in order to let the model have enough generalizaiton capability, the training data text should be big enough.&lt;/p&gt;
&lt;p&gt;The reasons for these unintended behaviors are mainly because the language models's objective is trying to predict the next token (that is that GPT-1/2/3 did, using transformer decoder to predict the next word given the previous tokens), which is different from the objective to follow the user's instruction helpfully and safely. To overcome these limitations, InstructGPT used the labeled data to fine-tune GPT-3 model to align the language model with reinforcement learning from human feedback (RLHF) to fine-tune GPT-3 to follow the human written instructions and use human preferences as the reward signal to fine-tune the model. &lt;/p&gt;
&lt;h3&gt;How does InstructGPT work (model setup)?&lt;/h3&gt;
&lt;p&gt;InstructGPT include fine-tune GPT-3 with labeled data to train a supervised fine-tune(SFT model) to generate outputs, a reward model (RM) to compare the diffent answers generated from SFT, and  reinforcement learning on the reward model to guide the SFT generation. &lt;/p&gt;
&lt;p&gt;&lt;img alt="InstructGPT Figure2.png" src="/figures/20230508_InstructGPT_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Step 1. Sample a prompt from the prompt dataset, and then human (or the model) write the answer for that sampled promot. After the promot and the answer is ready, it will be send to fine-tune GPT-3 with supervised learning (SFT). The challenge here is that it is expensive to get human prepare the answers for the prompts. With this model, it can learn how to generate the answers based from the SFT.&lt;/p&gt;
&lt;p&gt;Step 2. When a promot is give, it first use GPT-3 to generagte several answers for the given promot. For example, if 4 answers are generated, they will be marked as A, B, C, D. Then human will just need to rank the answers (e.g., A&amp;gt;B&amp;gt;C=D). In this way, human does not need to write the answers but only just rank the model generated answers. With this comparison data, InstructGPT trains a reward model to compare the 4 &lt;code&gt;&amp;lt;prompt, answer&amp;gt;&lt;/code&gt; pairs so that the ranking relation can be larned in the reward model (RM). With this model, it can be used to compare the generated answers and calcualte the reward. &lt;/p&gt;
&lt;p&gt;Step 3. Now they can use the RM to guide the SFT model to generate better results to achieve higher RM reward. InstructGPT use the output of the RM as a scalar reward and fine-tune the supervised policy to optimize this reward. The trained model is called InstructGPT.&lt;/p&gt;
&lt;p&gt;Accordingly, there are 3 datasets: 1) SFT dataset (13k prompts) to train the SFT model; 2) RM dataset (33k prompts) with labeler rankings of model output to train the RM model; and 3) PPO dataset (31k prompts) without human labels as input for RLHF fine-tuning. &lt;/p&gt;
&lt;h4&gt;More details about the models&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;SFT&lt;/strong&gt;: it is a supervised model by fine-tuning GPT-3 on the SFT data in which eash data point is a promot and the answers by the labelers. AS there are only 13k data, the model overfits after 1 epoch. However, training for more epochs will help in the RM score model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RM&lt;/strong&gt;: first, it replaces the last layer of SFT model (which is softmax layer) by a linear layer to output a scalar reward. So for each input &lt;code&gt;&amp;lt;prompt, answer&amp;gt;&lt;/code&gt; to the modified SFT model, it will output a scalar value (reward). If one prompt has &lt;span class="math"&gt;\(K\)&lt;/span&gt; answers, to compare them pairwisely, there will be &lt;span class="math"&gt;\(C(K,2)\)&lt;/span&gt; combinations. For two answers &lt;span class="math"&gt;\(y_w\)&lt;/span&gt; and &lt;span class="math"&gt;\(y_l\)&lt;/span&gt;, if &lt;span class="math"&gt;\(y_w\)&lt;/span&gt; is better than &lt;span class="math"&gt;\(y_l\)&lt;/span&gt;,  the purpose is to discriminate the two answers as much as possible, so the loss function is a logit loss of the delta of the two reward &lt;span class="math"&gt;\((r(x, y_w) - r(x, y_l))\)&lt;/span&gt;. The authors mentions that they used the pairwise answers comparision (where there are &lt;span class="math"&gt;\(C(K, 2)\)&lt;/span&gt; pairs of data) rather than to select the best answer will help to avoid overfiting.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RL&lt;/strong&gt;: The purpose of RL is to learn a better GPT-3 model &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt; to generate the answers. The objective function is designed to guide the model towards generating responses that adhere to the given instructions or constraints provided by users. &lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
 \mbox{objective}(\phi) = &amp;amp; E_{(x, y) \sim D_{\pi_{\phi}^{RL}}}[r_{\theta}(x, y) - \beta \log (\pi_{\phi}^{RL}(y|x) / \pi^{SFT}(y|x))] \\
 &amp;amp; + \gamma E_{x\sim D_{pretrain}}[\log(\pi_{\phi}^{RL}(x))]
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;It has 3 terms. The meaning of each term is:&lt;/p&gt;
&lt;p&gt;The first term &lt;span class="math"&gt;\(E_{(x, y) \sim D_{\pi_{\phi}^{RL}}}[r_{\theta}(x, y)]\)&lt;/span&gt; in the objective funciton says that for the new model generated answer &lt;span class="math"&gt;\(y\)&lt;/span&gt;, it tries to maximize the expectation of the reward model. &lt;span class="math"&gt;\(\pi_{\phi}^{SFT}\)&lt;/span&gt; is the SFT model from the labeler written data. &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt; is the model learned from RL with the generated data, where &lt;span class="math"&gt;\(\phi\)&lt;/span&gt; is the parameter. It is initialized as the SFT model. &lt;span class="math"&gt;\((x, y) \sim \pi_{\phi}^{RL}\)&lt;/span&gt; means putting prompt &lt;span class="math"&gt;\(x\)&lt;/span&gt; into the model &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt; to generate answer &lt;span class="math"&gt;\(y\)&lt;/span&gt;, and then put the generated pair &lt;span class="math"&gt;\((x,y)\)&lt;/span&gt; into the trained RM to calculate reward, and the final target is to maximize this reward. In this way the new model &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt; will be close to the best model from the labeler.&lt;/p&gt;
&lt;p&gt;The second term in the objective is the expection of &lt;span class="math"&gt;\(\beta \log (\pi_{\phi}^{RL}(y|x) / \pi_{\phi}^{SFT}(y|x))\)&lt;/span&gt;, which is KL divergence. The purpose of adding this term is that: after many interations, &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\pi_{\phi}^{SFT}\)&lt;/span&gt; could be more and more different. However, the reward model &lt;span class="math"&gt;\(r_{\theta}\)&lt;/span&gt; is trained from the first SFT and the corresponding &lt;span class="math"&gt;\(y\)&lt;/span&gt;. Now if the new model and the original SFT model are different, then the reward model will not work any more to evaluate. Adding this penalized term is to make sure the RM model can still be effective to evaluate the new model &lt;span class="math"&gt;\(\pi_{\phi}^{RL}\)&lt;/span&gt;. This is what "PPO" means.&lt;/p&gt;
&lt;p&gt;The third term &lt;span class="math"&gt;\(\gamma E_{x\sim D_{pretrain}}[\log(\pi_{\phi}^{RL}(x))]\)&lt;/span&gt;. That is, for the pretrained data of GPT-3, it calcualtes the loss and adds it to the objective. If &lt;span class="math"&gt;\(\gamma = 0\)&lt;/span&gt;, it is called PPO-ptx model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What ChatGPT did?&lt;/strong&gt; ChatGPT use the same method as InstructGPT but with slight difference in the data collection setup. ChatGPT use multi-round dialogue data and fine-tuned from the model in GPT-3.5. &lt;/p&gt;
&lt;h2&gt;5. GPT-4&lt;/h2&gt;
&lt;p&gt;OpenAI &lt;a href="https://arxiv.org/pdf/2303.08774.pdf"&gt;GPT-4 Technical Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GPT-4 is a transformer-style model pretrained to predict next token in a document. GPT-4 is a large multimodela model that can accept both image and text input prompts and output text. It was fine-tuned with RLHF (See above in InstructGPT) to improve the model's broader general knowledge and advanced reasoning capabilities&lt;/p&gt;
&lt;p&gt;GPT-4 training was stable. As the model is very big, it may fail during training process because of many reasons: loss does not converge, gradients exploding, hardware issues like machine failed, network connection broken. All these will bring lots of engineering challenge for model training.  &lt;/p&gt;
&lt;p&gt;Training performance can be predicted ahead of time. OpenAI developed the infrastructure and optimization that have very predictable behavior across multiple scales such that they can predict the final loss of the very large model GPT-4 by extrapolating from models trained with the same methodology but with &lt;span class="math"&gt;\(10^{-4}\)&lt;/span&gt;x less compute.&lt;/p&gt;
&lt;p&gt;&lt;img alt="GPT-4-1-training-loss-prediciton" src="/figures/20230528-GPT-4-1-training-loss-prediciton.png"&gt;&lt;/p&gt;
&lt;p&gt;Capability. GPT-4 improves significantly compared to GPT-3.5. It exhibits human level performance on the majority the professional and academic exams. The model’s capabilities on exams appear to stem primarily from the pre-training process and are not significantly affected by RLHF.&lt;/p&gt;
&lt;p&gt;&lt;img alt="GPT-4-2-exam-results.png" src="/figures/20230528-GPT-4-2-exam-results.png"&gt;&lt;/p&gt;
&lt;p&gt;Steerability. GPT-4 allow users to use "system" message to customize the experience within bounds. Generally "system" message tells GPT-4 to follow the users's instruction to provide additional information, instructions, or context during the conversation.&lt;/p&gt;
&lt;p&gt;Overall, the tech report did not touch too much details about the model, the training data and how OpenAI trained the complicated model.  &lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="AI"></category><category term="LLM"></category></entry><entry><title>Recommendation System 05 - Bayesian Optimization</title><link href="/pages/2021/12/31/recommendation-system-05-bayesian-optimization/" rel="alternate"></link><published>2021-12-31T19:08:00-06:00</published><updated>2021-12-31T19:08:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-12-31:/pages/2021/12/31/recommendation-system-05-bayesian-optimization/</id><summary type="html"></summary><content type="html">&lt;p&gt;If we have a function &lt;span class="math"&gt;\(f\)&lt;/span&gt;, and the objective is to find the point value that can maximize the value of the function. For example, when training DNN, what learning rate or other hyperparameters should we choose to make the loss function as small as possible. Another very practical example: in the allocation of advertisements, when the shopper input a query, the system will propose the corresponding possible advertisements for the allocation system to find the best one to show. Which advertisement will be displayed to the shopper in the end? This is related to many factors, such as the relevance of the advertisement to the query, the click-through rate of the advertisement, the placement of the advertisement, the bid price of the advertisement, and so on. According to these, we will get a comprehensive function, the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; is the return of all shopper query's potential advertisements. Our topic is to maximize this return, while ensuring that advertising does not affect the shopper's shopping experience.&lt;/p&gt;
&lt;p&gt;Because &lt;span class="math"&gt;\(f\)&lt;/span&gt; is very complicated, this problem is usually difficult to solve directly (for example, it has many factors) or very expensive (for example, it takes a lot of time to try all learning rates, considering it is continuous) or because of many constraints, leading to &lt;span class="math"&gt;\(f\)&lt;/span&gt; There is no explicit expression.&lt;/p&gt;
&lt;p&gt;What should we do at this time? In &lt;a href="https://songhuiming.github.io/pages/2021/12/26/recommendation-system-04-gaussian-process-regression/"&gt;the previous blog&lt;/a&gt;, we introduced the construction of a Gaussian process regression based on the limited observation points to surrogate the real &lt;span class="math"&gt;\(f\)&lt;/span&gt; and the uncertainty. &lt;/p&gt;
&lt;h3&gt;Acquisition function&lt;/h3&gt;
&lt;p&gt;Now we need to think about how to get more observation points to tune the Gaussian process and fit the data. The acuqisition function takes the mean and the variance at each point of the GPR function and computes a value that indicates how desirable it is to sample next at this position. A good acquisition function should trade off the exploration and exploitation. With acquisiton function, we don't need the real observations but the point based on the acquisition function (usaally the point that max the acquisition funciton, see details below). With this, the main steps for Bayesian Optimization are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Choose a surrogate function (here we use Gaussian process regression) to model the true function &lt;span class="math"&gt;\(f\)&lt;/span&gt; and define its prior distribution (the kernal function of Gaussian process)&lt;/li&gt;
&lt;li&gt;Based on the given observations, calcualte the posterior distribution through Bayesian formula&lt;/li&gt;
&lt;li&gt;Calculate the acuqisition function &lt;span class="math"&gt;\(\alpha(x)\)&lt;/span&gt;, and determine the next point that maximize the acquisition funciton &lt;span class="math"&gt;\(x^* = \mbox{arg max}_x \alpha(x)\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;Add the new point to the observations and update the posterior&lt;/li&gt;
&lt;li&gt;Repeat the process&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="img1" src="/figures/20211230_bayesian_optimization_01.png"&gt;&lt;/p&gt;
&lt;h2&gt;1. Objective function&lt;/h2&gt;
&lt;p&gt;Function &lt;span class="math"&gt;\(f\)&lt;/span&gt; like a black box usually is not unknown and expensive to evaluate. That is, the explicit expression of &lt;span class="math"&gt;\(f\)&lt;/span&gt; is unknown and it can only be evaluated based on the test points. Most of the time, only the noisy observations are available.&lt;/p&gt;
&lt;p&gt;Usually Gaussian Process (GP) with  mean = &lt;span class="math"&gt;\(\mu(x)\)&lt;/span&gt; and a covariance kernel &lt;span class="math"&gt;\(k(x, x')\)&lt;/span&gt; is used to surrogate it. That is, for any &lt;span class="math"&gt;\(k\)&lt;/span&gt; observed points, they have multivariate normal distribution wtih mean = &lt;span class="math"&gt;\((\mu(x_0), \ldots, \mu(x_k))\)&lt;/span&gt; and covariance matrix &lt;span class="math"&gt;\(\Sigma $ with $\Sigma_{ij} = k(x_i, x_j)\)&lt;/span&gt;. GP surrogate model for &lt;span class="math"&gt;\(f\)&lt;/span&gt; means &lt;span class="math"&gt;\((f(x_1), \ldots, f(x_k))\)&lt;/span&gt; is multivariate normal with mean and variance determined by  &lt;span class="math"&gt;\(\mu(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(k(x, x')\)&lt;/span&gt;.  &lt;/p&gt;
&lt;h2&gt;2. Acquisition Function&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Syntax&lt;/th&gt;
&lt;th&gt;Description&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(X_n\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;n observed data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(Y_n\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;n observed value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(\hat{y_n^*}\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;max of the n observed value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;span class="math"&gt;\(y^*, x^*\)&lt;/span&gt;&lt;/td&gt;
&lt;td&gt;final global max and global optimial point&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Some of the popular acuqisition functions are like:&lt;/p&gt;
&lt;h3&gt;2.1. Probability Improvement (PI)&lt;/h3&gt;
&lt;p&gt;PI is to find next &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt; s.t. &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; will be greater than the maximum from the expirical (denoted with &lt;span class="math"&gt;\(y_n^*\)&lt;/span&gt;) with a small increse of &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. Here &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt; is like a random variable from the posterior distribution &lt;span class="math"&gt;\(f(x) \sim N(\mu_n(x), \sigma_n(x))\)&lt;/span&gt; &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the maximum &lt;span class="math"&gt;\(\hat{y_n^*} = \mbox{max}_{i \le n}f(x_i)\)&lt;/span&gt; based on &lt;span class="math"&gt;\(Y_n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For any query point &lt;span class="math"&gt;\(x\)&lt;/span&gt;, put &lt;span class="math"&gt;\(x, X_n, Y_n\)&lt;/span&gt; into GP to get the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; at &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;'s distribution with mean &lt;span class="math"&gt;\(\mu_n(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma_n(x)\)&lt;/span&gt;, which is normal &lt;span class="math"&gt;\(f(x) \sim \mathcal{N}(\mu_n(x), \sigma_n(x))\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;Get &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt; which maximize the PI. &lt;span class="math"&gt;\(x_{next} = \mbox{argmax PI}_n(x) = \mbox{argmax P} (f(x) \ge \hat{y_n^*} + \epsilon)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Calc &lt;span class="math"&gt;\(f(x_{next})\)&lt;/span&gt;, update &lt;span class="math"&gt;\(X_n, Y_n\)&lt;/span&gt; by &lt;span class="math"&gt;\(x_{next}, f(x_{next})\)&lt;/span&gt; and repeat from step 1. &lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$\mbox{argmax PI}_n(x) = \mbox{argmax P} (f(x) \ge \hat{y_n^*} + \epsilon) \\
= \mbox{argmax P} \left( \frac{f(x) - \mu_n(x)}{\sigma_n(x)} \ge \frac{\hat{y_n^*} + \epsilon - \mu_n(x)}{\sigma_n(x)} \right) \\
= \mbox{argmax P} \left( \frac{f(x) - \mu_n(x)}{\sigma_n(x)} \le \frac{\mu_n(x) - \hat{y_n^*} - \epsilon}{\sigma_n(x)} \right) \\
= \mbox{argmax} \Phi\left( \frac{\mu_n(x) - \hat{y_n^*} - \epsilon}{\sigma_n(x)} \right)
$$&lt;/div&gt;
&lt;p&gt;The parameter &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; is to control exploration and exploitation. &lt;/p&gt;
&lt;h3&gt;2.2. Expected Improvement (EI)&lt;/h3&gt;
&lt;p&gt;When use PI to find the next evaluation point &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt;, it cares about the probability of function value &lt;span class="math"&gt;\(f(x) \ge \hat{y_n^*} + \epsilon\)&lt;/span&gt;. In fact, we should also cares about the distance of function value at evaluation point to the global optimization &lt;span class="math"&gt;\(y^8 =f(x^*)\)&lt;/span&gt;. That is
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mbox{arg min}_x \mbox{EI}(x) = \mbox{arg min}_x \mathbb{E}_{f(x) \sim \mathcal{N}(\mu_n(x), \sigma_n(x))}(f(x) - y^*)
$$&lt;/div&gt;
&lt;p&gt;Since the global optimization point &lt;span class="math"&gt;\(y^*\)&lt;/span&gt; is unknown, so we use this surrogate to replace the above function
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mbox{arg max}_x \mbox{EI}(x) = \mbox{arg max}_x \mathbb{E}_{f(x) \sim \mathcal{N}(\mu_n(x), \sigma_n(x))}\left(\mbox{max}\left(f(x) - \hat{y_n^*}, 0 \right)\right)
$$&lt;/div&gt;
&lt;p&gt;PI cares about the probability while EI cares about how much it is. The detailed steps will be:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select the maximum &lt;span class="math"&gt;\(\hat{y_n^*} = \mbox{max}_{i \le n}f(x_i)\)&lt;/span&gt; based on &lt;span class="math"&gt;\(Y_n\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;For any query point &lt;span class="math"&gt;\(x\)&lt;/span&gt;, put &lt;span class="math"&gt;\(x, X_n, Y_n\)&lt;/span&gt; into GP to get the function &lt;span class="math"&gt;\(f\)&lt;/span&gt; at &lt;span class="math"&gt;\(f(x)\)&lt;/span&gt;'s distribution with mean &lt;span class="math"&gt;\(\mu_n(x)\)&lt;/span&gt; and &lt;span class="math"&gt;\(\sigma_n(x)\)&lt;/span&gt;, which is normal &lt;span class="math"&gt;\(f(x) \sim \mathcal{N}(\mu_n(x), \sigma_n(x))\)&lt;/span&gt; &lt;/li&gt;
&lt;li&gt;define &lt;span class="math"&gt;\(a^+=\mbox{max}(a, 0)\)&lt;/span&gt; and expected distance &lt;span class="math"&gt;\(\Delta_n(x) = \mu_n(x) - \hat{y_n^*}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;The max expectation improvement point &lt;span class="math"&gt;\(x_{next} = \mbox{arg max}_x \mathbb{E}_{f(x) \sim \mathcal{N}(\mu_n(x), \sigma_n(x))}\left(\left(f(x) - \hat{y_n^*} \right)^+\right)\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;After getting &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt;, put the new data point &lt;span class="math"&gt;\((x_{next}, f(x_{next}))\)&lt;/span&gt; into the observations and repeat the processes above.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;2.3. Thompson Sampling&lt;/h3&gt;
&lt;p&gt;In PI and EI, the next point &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt; is a single point to maximize some acquisition function. Rather than using one point, it is also possible to draw some points from the posterior distribution such that these points could represent the posterior function. Thompson sampling explots this by drawing such a sample from the posterior distribution and thenchoose the next point &lt;span class="math"&gt;\(x_{next}\)&lt;/span&gt;. Thompson Sampling balances expolration and exploitation in this way:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Locations with large variance have higher uncertainty. Optimizing such samples will help exploration. &lt;/li&gt;
&lt;li&gt;Samples from current data points have no uncertainty. So if sampling from the surrogate posterior will help exploitation. &lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;2.4. Upper Confidence Bound (UCB)&lt;/h3&gt;
&lt;p&gt;UCB is the most straightforward method. It is defined as
&lt;/p&gt;
&lt;div class="math"&gt;$$
\mbox{UCB}(x^*) = \mu(x^*) + \beta * \sigma(x^*)
$$&lt;/div&gt;
&lt;p&gt;
This setup will favor the region that &lt;span class="math"&gt;\(\mu(x^*)\)&lt;/span&gt; is large (for exploitation) or regions that &lt;span class="math"&gt;\(\sigma(x^*)\)&lt;/span&gt; (for exploration) is large. The parameter &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; is to trade off these two tendencies. &lt;/p&gt;
&lt;h3&gt;2.5. Others&lt;/h3&gt;
&lt;p&gt;Some other acquisition funciton are like Knowledge Gradient and Entropy Search. &lt;/p&gt;
&lt;h2&gt;3. Other surrogate functions&lt;/h2&gt;
&lt;p&gt;In addition to the Gaussian process, there are some other functions that can work as the surrogate function. For example, &lt;a href="https://www.cs.ubc.ca/~hutter/papers/10-TR-SMAC.pdf"&gt;random forest&lt;/a&gt; is used for sequential model based algorithm; &lt;a href="https://proceedings.neurips.cc/paper/2011/file/86e8f7ab32cfd12577bc2619bc635690-Paper.pdf"&gt;Tree-Parzen estimator&lt;/a&gt; works to model the linklihood of the data rather than the posterior.&lt;/p&gt;
&lt;h2&gt;4. tools&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://scikit-optimize.github.io/stable/"&gt;scikit-loptimize&lt;/a&gt; provides some examples how to do &lt;a href="https://scikit-optimize.github.io/stable/auto_examples/bayesian-optimization.html"&gt;Bayesian optimization&lt;/a&gt;. &lt;a href="https://github.com/fmfn/BayesianOptimization"&gt;This Bayesian optimization tool&lt;/a&gt; also provides the function to implement the global optimization with Gaussian process. &lt;a href="https://botorch.org/"&gt;BoTorch&lt;/a&gt; provides a scalable tool to implement Bayesian optimization with pyTorch which can leverage GPU to boost the training speed.&lt;/p&gt;
&lt;h2&gt;5. An simple example&lt;/h2&gt;
&lt;p&gt;To be continued with BOTorch example.&lt;/p&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://distill.pub/2020/bayesian-optimization/"&gt;Exploring Bayesian Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.aidanscannell.com/post/gaussian-process-regression/"&gt;Gaussian Process Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/"&gt;Bayesian optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/139478368"&gt;PR Ⅲ：从高斯分布到高斯过程、高斯过程回归、贝叶斯优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/pytorch/botorch"&gt;pytorch/botorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/GPflow/GPflowOpt"&gt;GPflow/GPflowOpt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/scikit-optimize/scikit-optimize"&gt;scikit-optimize/scikit-optimize&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category><category term="pytorch"></category></entry><entry><title>Recommendation System 04 - Gaussian process regression</title><link href="/pages/2021/12/26/recommendation-system-04-gaussian-process-regression/" rel="alternate"></link><published>2021-12-26T19:08:00-06:00</published><updated>2021-12-26T19:08:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-12-26:/pages/2021/12/26/recommendation-system-04-gaussian-process-regression/</id><summary type="html"></summary><content type="html">&lt;p&gt;Before we start Bayesian, we will give a brief introduction of Gaussian Process Regression. &lt;/p&gt;
&lt;p&gt;Suppose there are data &lt;span class="math"&gt;\((X_i, y_i), i = 1 \cdots n\)&lt;/span&gt;. If we want to learn the relation between &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(y\)&lt;/span&gt;, we have learned to build a linear regression or a decision tree based model to fit the data. The fitted funciton is like &lt;span class="math"&gt;\(y = f(x | \theta)\)&lt;/span&gt; where &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; is the parameter.&lt;/p&gt;
&lt;p&gt;Gaussian process regression is infinite number of regression functions. Each function is like one regresson function. All these fitted functions together will not only give the fitted value, but also the variance.&lt;/p&gt;
&lt;h2&gt;Gaussian process&lt;/h2&gt;
&lt;p&gt;A Gaussian process is a collection of random variables, where any finite number of these are jointly normal distribution which is defined by: 1) the mean function &lt;span class="math"&gt;\(m(x)\)&lt;/span&gt; and 2) the covariance function &lt;span class="math"&gt;\(k(x, x')\)&lt;/span&gt;. The function &lt;span class="math"&gt;\(\mbox{f}(x) \sim \mbox{GP}(m(x), k(x, x'))\)&lt;/span&gt; means
&lt;/p&gt;
&lt;div class="math"&gt;$$ 
\begin{eqnarray} 
\mathbb{E}(\mbox{f}(\mathbf{x})) &amp;amp;=&amp;amp; \mbox{m}(\mathbf{x})  \tag{1} 
\end{eqnarray}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray} 
\mathbb{E}\left(\left(\mbox{f}(\mathbf{x})-\mbox{m}(\mathbf{x})\right)(f(\mathbf{x}')-\mbox{m}(\mathbf{x}'))\right) &amp;amp;=&amp;amp; k(\mathbf{x}, \mathbf{x}')  \tag{2} 
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;So the Gaussian process can be expressed as
&lt;/p&gt;
&lt;div class="math"&gt;$$
f \left(x\right) \sim \mathcal{GP} \left( \mbox{m}(\mathbf{x}),k(\mathbf{x}, \mathbf{x}') \right)
$$&lt;/div&gt;
&lt;p&gt;高斯过程的任意有限个样本的联合分布为联合正太分布。&lt;/p&gt;
&lt;p&gt;Here for simplify and without loss of generality, we assume the mean is 0. That is,&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray} 
\mbox{m}(\mathbf{x}) &amp;amp;=&amp;amp; 0 \tag{3} 
\end{eqnarray}
$$&lt;/div&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray} 
k(\mathbf{x}, \mathbf{x}')  
&amp;amp;=&amp;amp;\mbox{exp}\left[-\frac{1}{2}\left(\mathbf{x}-\mathbf{x}'\right)^{T}\left(\mathbf{x}-\mathbf{x}'\right)\right] \tag{4} 
\end{eqnarray}
$$&lt;/div&gt;
&lt;h2&gt;Gaussian process regression&lt;/h2&gt;
&lt;p&gt;Suppose we have &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations  &lt;span class="math"&gt;\(\left(X_i, y_i=f(X_i)\right), i = 1 \cdots n\)&lt;/span&gt;, we want to get the prediction given the new data point &lt;span class="math"&gt;\(X^*\)&lt;/span&gt;. Let's denote the &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations as &lt;span class="math"&gt;\(\mathbf{X}=[X_1, X_2, \ldots, X_n]\)&lt;/span&gt; and &lt;span class="math"&gt;\(\mathbf{f} = [f(X_{1}), f(X_{2}),\ldots, f(X_{n})]\)&lt;/span&gt;. Based on the property of Gaussian process, the new function value &lt;span class="math"&gt;\(f^* = f(X^*)\)&lt;/span&gt; is jointly normally distributed with the &lt;span class="math"&gt;\(n\)&lt;/span&gt; observations
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation} 
Pr\left(\begin{bmatrix}\label{eq:GP_Joint} 
\mathbf{f}\\f^{*}\end{bmatrix}\right) = \mbox{Norm}\left[\mathbf{0}, \begin{bmatrix}\mathbf{K}[\mathbf{X},\mathbf{X}] &amp;amp; \mathbf{K}[\mathbf{X},X^{*}]\\ \mathbf{K}[X^{*},\mathbf{X}]&amp;amp; \mathbf{K}[X^{*},X^{*}]\end{bmatrix}\right] \tag{5} 
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(\mathbf{K}[\mathbf{X},\mathbf{X}]\)&lt;/span&gt; is a &lt;span class="math"&gt;\(n \times n\)&lt;/span&gt; matric where the element &lt;span class="math"&gt;\((i, j)\)&lt;/span&gt; is given by the variance function &lt;span class="math"&gt;\(k(X_i, X_j)\)&lt;/span&gt;, &lt;span class="math"&gt;\(\mathbf{K}(\mathbf{X},X^{*})\)&lt;/span&gt; is a &lt;span class="math"&gt;\(t \times 1\)&lt;/span&gt; vector.&lt;/p&gt;
&lt;p&gt;Based on the property of joint normal distribution, the conditonal distribution of one element given the rest is also normally distributed with the univariate normal distribution funciton
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{equation}\label{eq:gp_posterior} 
Pr(f^*|\mathbf{f}) = \mbox{Norm}(\mu(X^{*}),\sigma^{2}(X^{*})), \tag{6} 
\end{equation}
$$&lt;/div&gt;
&lt;p&gt;
where 
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{eqnarray}\label{eq:GP_Conditional} 
\mu(X^{*})&amp;amp;=&amp;amp; \mathbf{K}[X^{*},\mathbf{X}]\mathbf{K}[\mathbf{X},\mathbf{X}]^{-1}\mathbf{f}\nonumber \\ 
\sigma^{2}(X^{*})&amp;amp;=&amp;amp;k(X^{*},X^{*})\!-\!\mathbf{K}[X^{*}, \mathbf{X}]\mathbf{K}[\mathbf{X},\mathbf{X}]^{-1}\mathbf{K}[\mathbf{X},X^{*}]. \tag{7} 
\end{eqnarray}
$$&lt;/div&gt;
&lt;p&gt;The formula gives the distribution of the prediction &lt;span class="math"&gt;\(f(X^*)\)&lt;/span&gt; of the new point &lt;span class="math"&gt;\(X^*\)&lt;/span&gt;. As you can see, now &lt;span class="math"&gt;\(f(X^*)\)&lt;/span&gt; is not a point prediction but a random variable with mean=&lt;span class="math"&gt;\(\mu(X^{*})\)&lt;/span&gt; and variance=&lt;span class="math"&gt;\(\sigma^{2}(X^{*})\)&lt;/span&gt;. If we want to make a point prediction, then the best prediction will be =&lt;span class="math"&gt;\(\mu(X^{*})\)&lt;/span&gt;. Below is an example of plot of GPR fitted by 4 observations. As the new observations added (for example, &lt;span class="math"&gt;\(x_5\)&lt;/span&gt;), the uncertainty will go down so the variance will decrease. As more observations onboard, the uncertainty/variance will continue to go down. However, in reality, these observaroins may be expensive to observe or may be hard to get. So the question is, how can we get the new observations to fit the regression? Bayesian Optimization (BO) will provide some ideas about how to get these new data points by maximizing the acquisition function. We will introduce that in next blog. &lt;/p&gt;
&lt;p&gt;&lt;img alt="img1" src="/figures/20211230_gaussian_process_01.png"&gt;&lt;/p&gt;
&lt;p&gt;Next we will give an example of how Gaussian process regression is developed and what it looks like. The code is from &lt;a href="https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#example-with-noise-free-target"&gt;this sklearn example&lt;/a&gt;. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
plt.rcParams[&amp;quot;figure.figsize&amp;quot;] = (32,20)

np.random.seed = 8
X = np.linspace(start = 0, stop=10, num=1000).reshape(-1,1)
y = np.squeeze(X * np.sin(X))

rng = np.random.RandomState(1)
training_indices = rng.choice(np.arange(y.size), size = 6, replace = False)
X_train, y_train = X[training_indices], y[training_indices]

from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import RBF

kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2))
gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer = 8)
gaussian_process.fit(X_train, y_train)

gaussian_process.kernel_

mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True)

plt.plot(X, y, label=r&amp;quot;$f(x) = x \sin(x)$&amp;quot;, linestyle=&amp;quot;dotted&amp;quot;)
plt.scatter(X_train, y_train, label=&amp;quot;Observations&amp;quot;)
plt.plot(X, mean_prediction, label=&amp;quot;Mean prediction = $\mu(X)$&amp;quot;)
plt.fill_between(
    X.ravel(),
    mean_prediction - 1.96 * std_prediction,
    mean_prediction + 1.96 * std_prediction,
    alpha=0.2,
    label=r&amp;quot;95% confidence interval=$\mu(X) \pm 1.96 \cdot \sigma(X)$&amp;quot;,
)

plt.legend()
plt.xlabel(&amp;quot;$x$&amp;quot;)
plt.ylabel(&amp;quot;$f(x)$&amp;quot;)
_ = plt.title(&amp;quot;Gaussian process regression&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="img2" src="/figures/20211230_gaussian_process_02.png"&gt;
The mean of the Gaussian process regression (red line) is close to the tune function &lt;span class="math"&gt;\(y =f(x)= x\cdot \mbox{sin}(x)\)&lt;/span&gt;. At the points where there is an observation (blue solid dot), the predicted value is exactly the same as the observation. So there is no variance on these points. For the area there is no observatons (the left side), the variance is higher than the area where there are some observations. &lt;/p&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.aidanscannell.com/post/gaussian-process-regression/"&gt;Gaussian Process Regression&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://distill.pub/2019/visual-exploration-gaussian-processes/"&gt;A Visual Exploration of Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/139478368"&gt;PR Ⅲ：从高斯分布到高斯过程、高斯过程回归、贝叶斯优化&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.borealisai.com/en/blog/tutorial-8-bayesian-optimization/"&gt;Bayesian optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-noisy-targets-py"&gt;Gaussian Processes regression: basic introductory example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_prior_posterior.html#sphx-glr-auto-examples-gaussian-process-plot-gpr-prior-posterior-py"&gt;Illustration of prior and posterior Gaussian process for different kernels&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category></entry><entry><title>职场话题——关于职场communications skill的一点感想 (转载)</title><link href="/pages/2021/12/22/zhi-chang-hua-ti-guan-yu-zhi-chang-communications-skillde-yi-dian-gan-xiang-zhuan-zai/" rel="alternate"></link><published>2021-12-22T19:08:00-06:00</published><updated>2021-12-26T19:08:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-12-22:/pages/2021/12/22/zhi-chang-hua-ti-guan-yu-zhi-chang-communications-skillde-yi-dian-gan-xiang-zhuan-zai/</id><summary type="html"></summary><content type="html">&lt;blockquote&gt;
&lt;p&gt;初级&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;多说，多练是最重要的。不开口，就一直开不了口。&lt;/li&gt;
&lt;li&gt;不要害怕有口音，在美国，从欧洲人到印度人到日韩国家，各种口音都有，我们的口音不是最差的，一般也不影响理解。&lt;/li&gt;
&lt;li&gt;心态上要自信，而对我来说自信来源于准备。如果一开始害怕即兴发挥说不好，可以多争取presentation的机会，因为可以事先写稿。但我一般会先自己练习几遍，然后争取脱稿，这样在讲的过程中其实会有一些即兴发挥的部分，但是是有准备的发挥。包括日常的小组会议，如果事先知道议题，我也会先准备一些发言内容，到时说不定有一部分可以用上。这样日久天长，你会发现不用写稿，也会组织语言了。&lt;/li&gt;
&lt;li&gt;语气要坚定，不要讲话黏黏糊糊，或者说着说着尾音就低下去了，这些在我的感官上都显得不太自信。&lt;/li&gt;
&lt;li&gt;如果真的愿意花时间练习，可以对着手机录下来，再回头来看自己的问题在哪儿，以及对比一些你觉得表达能力强的同事，看有什么可以借鉴的地方。&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;中级&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;好的发言结构可以弥补一些语言的不足：一般“论点 -- 论据（1，2...）-- 结论”的结构可以让你的发言听上去比较有条理，听发言的人也比较容易理解。&lt;/li&gt;
&lt;li&gt;我后来发现自己有时会有些用力过猛，或者语速过快的情况，所以会刻意地放慢语速，以及表现得轻松一些。大家都知道如果在发言以及演讲的时候可以开个玩笑，效果会比较好，但这就可遇而不可求了。&lt;/li&gt;
&lt;li&gt;从发言内容上来说可以考虑如何bring in unique perspective，根据自己之前的职业，技术甚至文化背景，是不是有些特殊的角度，当然不同工作情况不同。&lt;/li&gt;
&lt;li&gt;对于职场新人的一个小tip，在参加各种部门大会、讲座的时候，事先准备好一些还不错的问题提问，其实是收益挺高的，因为可以不用花太多精力就较快地给领导留下一些印象。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总结起来，communications skill在职场上是很重要的技能，相当于一个杠杆，可以让自己在专业上的努力事半功倍，有时也算是一种生存技能吧。&lt;/p&gt;
&lt;p&gt;讲演的话，多想想你的听众是谁，他们在意什么，你的讲演会对他们有什么影响，这样讲演会更吸引听众。&lt;/p&gt;
&lt;p&gt;讲演的时候，说慢点有好处。听众听得更清楚。&lt;/p&gt;
&lt;p&gt;电子邮件，一定要言简意赅。大家都忙得不行，没时间看长篇累牍。&lt;/p&gt;
&lt;p&gt;和同事讨论的时候一定要就事论事，按别人字面上的话回复，不要联想，不要假设，可以澄清。这点非常重要。关键的关键还是脑子聪明，言之有物且言简意赅，能有效填补info gap才是好的communication.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;和观众的互动，除了在准备的时候要想听众是谁，你在讲给谁听以外，在讲话的同时要sense your audience,看看他们的表情，甚至pick up on 一些小的verbal or non verbal的feedback然后在事先准备的不同方案里面选出最好的，甚至即兴讲新的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;面对提问和质疑，不要对立态度。如果是有提问环节，一定不要把任何提问包括质疑的提问当作的自己的对立面，而是要借力展开。态度一定要是真诚感谢。如果有现成的答案，自然侃侃而谈，不完全确定的，在感谢问题之后表示我试着解释这个问题，还请提问者再给feedback. 真遇到challenge的问题，可以表示你完全同意这个可能是一个cInferno, 目前的解决方案只能是...&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;要了解自己的讲话内容里的段落的重要程度的不同，随时准备可以跳过。常常会发现讲话超时，这个时候一定要很轻松的跳过一些段落，告知大意并表示这些跳过内容可以有时间再谈或者欢迎私下交流。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对自己专业内容非常熟悉，别人都不会讲，只能听你讲，你讲着讲着就越讲越自信了。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;会议上尽量先发言，以防你的点被人抢走了。&lt;/li&gt;
&lt;li&gt;没组织好讲什么时也可以先抢下floor, 简单扯几句接话头就可以，比如说很理解这个concern, 我们也遇到过这些问题之类的。边扯边组织语言。&lt;/li&gt;
&lt;li&gt;把自己要讲的内容归纳出几个点来讲。每个点找个关键字。讲完前把自己的几个点重复一下。这样听众容易记住你讲的内容。&lt;/li&gt;
&lt;li&gt;讲完后不要纠结自己讲得好不好，把注意力投入到别人的讲话中去。&lt;/li&gt;
&lt;li&gt;没人总结next steps/action items 的话你来总结。visibility 最高。但是也要注意别一不小心自己揽太多活了。&lt;/li&gt;
&lt;li&gt;还有如果完全想提高visibility，哪怕别人说了你一样的观点，你可以继续second/echo他啊，然后加一点点新的comment就好了，不一定要全新的东西才可以出声。&lt;/li&gt;
&lt;li&gt;还有就是可以从问问题开始锻炼。哪怕自己不是真的有critical的问题就是简单问一个，或者sumarize一下确定是这个意思，大家都同意，得到confirm了就过了。不要占大家太多时间。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;a href="https://forums.huaren.us/showtopic.html?topicid=2639393"&gt;先谈一些大的教训 （道理浅显，但只有经历了才明白）&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;工作能力和努力上进在升入中层管理时还是被看中的，越往上走就越不被看中了。往上走被看中是因为人在他们的小圈子里，是他们的嫡系亲属, 或他们能培养出来的拥护者。所以一个不比你有经验，有成绩，和有好的合作能力的人，分分钟都能代替你，就用他们理所当然的理由，甚至都不要啥理由。当然还是可以认真工作的，我们要显现自己专业的一面，但不要老觉得 “I will prove it one more time so that they will recognize me.” They won’t！&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;明白了上面这个道理，期望值就要放低，不要老患得患失的。你努力工作的目的是一要这份工资，二要尽量能心情愉快地工作。你累坏了累死了没有人会真正可怜你，你家人特别是孩子会最可怜。所以你得把心态放对了，看很多工作的问题不过就是在要走流程，按班就步就好。我曾经为了老板的业绩也为了自我表现，带着我的团队，累到我的月经紊乱得看医生，同时几个队员生病失眠。换上今天的我，我会 still do a good job, but it is not worthwhile to sacrifice my or anyone’s health. 老板为升官能逼人到这个份上，很多过后也不会记得你的好，只会给你更多活，反正你又一次证明了你真是太能干了嘛。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;和你的直接老板搞好关系是最重要的，即使这个老板不懂技术（西方职场很多时候是管理和技术分开的，不奇怪），即使这个老板只是个芝麻官，即使你的大老板似乎更欣赏你。。。还是好好和直接老板搞好关系，如果实在是碰上了一个恶魔，还是离开吧，不值得把自己拖垮。和老板搞好关系不是一定要拍马屁，要职业化，要讲技巧 say no in a yes way, 见下一条。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;刚开始做管理者的女性，1）往往爱一下就意气风发，和老板承诺很多事情 (over promise deliverables and deadlines)。这时候应该和老板说，谢谢他/她和你谈目标 （马屁），你回去理一下拿出具体方案和你的团队商量一下（缓兵之策），再回来和老板请教如何排序prioritize（马屁请教）。把你想做能做的放前面说，不能做的就说团队有顾虑如时间紧人手不够技术不支持等，请老板决定做什么先而不是什么都一起做。即使最后还是都得拼命做，至少老板知道你在拼命了。2）大包大揽，和手下交流烦，不如自己做得快时干脆就自己做了。见下条。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;有的手下总丢三落四的，让他/她和你对话是记笔记，走前口头复述要点，下次带着同样的笔记本，来和你一一对接工作。有的手下是等着你的明确指令才会做活的，那就直接下指令定好交货期和质量。有的手下不配合还可能要和你对着干，那就多用e-mail 留下证据。如果对方有什么不专业的行为，一开始就要开个文件，记录下当时发生了什么有没有证人在场。同时要和你老板时不时的沟通，啥时下PIP，到时你们一块开这个人就水到渠成了。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总之，要学会分配任务，检查任务，支持任务，和结束任务。不同的手下性格不同，要用不同手段，即使再生气，表面也平静甚至是微笑的（这得练没办法）。哎，中层关人真是累，体验过的人都知道无官一身轻。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;找个有经验的能信得过的mentor，平时主动聚一聚聊天，不要等到有事才找人。不一定是高层女性，你以为他们不是经历过磨难了才会帮助别的女性吗？坏人是不分性别的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;越忙越要organized 越要高效, 每天写To-do-list，不管大事小事，包括生活的事，然后一条一条地化掉，真的很有成就感，也大大减少焦虑。你一副无头苍蝇的忙样，没有人会认真地对待你，包括你的手下。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;学习一下工作中说话的技巧，有很多关于人的心理学。比如你都气得要死了，与其说：We have talked about this several times. It won''''''''t work! ...可以“皮笑肉不笑”地说：I think we are 90% on the same page. I really like what you said about... （挑个能引申到你想做的）。I remember Catie suggested that WE add... (也是你想做的).That is great, too. We can scale it up if we can integrate her idea into yours. How about…（加入你想做的）? &lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这一招就是先表扬，再恭维对方往你要去的路上引导，然后多用问句提出你的想法，让对方只在你的框架里提建议时感觉良好。&lt;/p&gt;
&lt;p&gt;﻿﻿人在感觉被尊重（feel respected），被融入（ engaged）, 和被参与（involved）的时候，比较容易听你的。这一招不一定每次都有用，但比我当年的直来直去效果好多了。这不是母语的问题，我的美国老板经常派我去处理一堆人事牵扯的难事情，就是因为他知道自己说话太直了。所以说话的艺术是可以学习的。&lt;/p&gt;
&lt;p&gt;下面是我精读过的很多职场书的其中一部分，有的是经典，可选1-2本好好精读，我敢保证一定有领悟：&lt;/p&gt;
&lt;p&gt;﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿https://selfcarestacking.com/blog/f/women-leadership&lt;/p&gt;</content><category term="career growth"></category><category term="career growth"></category></entry><entry><title>职场话题——职场常见8大场景沟通(转载)</title><link href="/pages/2021/12/22/zhi-chang-hua-ti-zhi-chang-chang-jian-8da-chang-jing-gou-tong-zhuan-zai/" rel="alternate"></link><published>2021-12-22T19:08:00-06:00</published><updated>2021-12-22T19:08:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-12-22:/pages/2021/12/22/zhi-chang-hua-ti-zhi-chang-chang-jian-8da-chang-jing-gou-tong-zhuan-zai/</id><summary type="html"></summary><content type="html">&lt;h2&gt;1. Ken：初次见面该怎样沟通？&lt;/h2&gt;
&lt;p&gt;Wendy：无论是职场新人还是刚换组/公司的职场老人，你给同事们的第一印象非常重要。作为新人，应该主动出击去沟通，安排一系列1对1会议，包括和老板的、组里同事、以及相关合作组的会议。和老板沟通时，可以请老板介绍组内概况、年度计划，分享一些资源、文件、工作中常用工具等，还可向老板寻求一个需要沟通的同事名单，帮助你积极学习融入。和组内同事沟通时，既要介绍自己的背景，也要展示你能给组带来些什么，并表达自己乐于合作，希望和同事学习，更好地为组做贡献的意愿。注意，聊的时候不要过于聚焦在自己身上给对方造成自大的印象，可以多了解和关注对方。如果所在的组有秘书，也建议跟秘书做一次1对1。跟秘书建立好的关系，会对今后工作行政方面有好的帮助。作为新人不要只预约一次会议，可以多约一些follow up的会议。此外，新人可以给自己制定一个On board plan，例如自己30天、60天、90天的时间节点想达到什么目标，并和老板进行沟通探讨。另一方面，如果你作为一个团队的老同事，能为新人提供信息或帮助，可以更好地和新同事建立信任，为今后有效沟通合作建立好的基础。&lt;/p&gt;
&lt;h2&gt;2. Ken：1对1时该怎样沟通？&lt;/h2&gt;
&lt;p&gt;Wendy：1对1以沟通工作进展、获取信息资源、得到建议指导为目的。1对1沟通主要包括和老板、和同事两个方面。对领导：沟通前要提前做准备， 列好议程并提供一些信息给领导，包括你项目进展、感兴趣的方面、你好的想法、职业规划等，让老板提供反馈、给与支持或者去帮你找资源。沟通时也可以谈你近期的成果、顾虑、风险等去请教领导。注意，不能一味听从领导觉得他说什么都是对，而要有自己的判断和规划，并保持正面积极态度，尽量和领导达成共识。和同事1对1 沟通时，注意不要Point fingers。如果遇到分歧，可以提出问题或者建设性的反馈，沟通中就事论事，不要抱怨或者指责他人。无论和老板还是和同事1对1沟通时，如果对方曾给你提供过帮助或者好的建议，要给对方一个update并且表示感谢。&lt;/p&gt;
&lt;h2&gt;3. Ken：疫情阶段Remote办公该怎样沟通？&lt;/h2&gt;
&lt;p&gt;Wendy:Remote的沟通比面对面沟通难度增加了很多，但只要积极主动、放松心态，也能达到很好的沟通效果。例如，在Online会议，开会时不一定要直奔主题，可以用Small Talk开始，聊一点和生活、家庭相关的内容，以此拉近和同事们的距离感。平时留意同事身上发生的一些事，例如生病、结婚、毕业、项目结题等等，并给与及时的关心或祝贺。可以组织一些线上的Happy hour或者Coffee time。如果自己手里有活动、团建经费，组织个Lunch together活动等。记住，如果这个特别时期能寻找一起和同事Offline见面的机会，约一起喝咖啡或者散步，也会对今后和同事间的合作有很多帮助。如果可以，尽量开会时答开摄像头，让对方看到你的职业形象。定期或不定期跟你想要建立关系的人做一下Touch base，讲讲你最近的一些成果，Review 你的项目或者拉对方跟你Brain storm。疫情期间的沟通也要采取多样性，除了线上开会，也可以通过邮件、IM、Group slack, 共享文件等形式。&lt;/p&gt;
&lt;h2&gt;4. Ken：会议沟通的重点是什么？如何扩大自己的影响力？&lt;/h2&gt;
&lt;p&gt;Wendy：大组会议是为了建立关系、展示进展、达成共识或者寻求解决方法。好的沟通方式是你要主导这个对话。会议前做好准备工作，会上展示议程。在会议沟通时可以将你遇到的问题或者需要出谋划策的地方和团队成员分享，从团队成员那里获得建议和帮助。例如，在某项目上你可能有ABC等好几种方案，如果你举棋不定该采取哪种方法时，可以将这几个方案以及每种方案的利弊都分享给同事，让团队成员帮你一起决策。大组会上不要一味地关注自己，要参与讨论，分享信息、资源，提出问题和给与建议。此外，在自己主导的会上要养成记Meeting Note的好习惯，包括会议梗概、会议目标、会议产出、接下来的To do list、各个任务的负责人，截止时间等，将这样的Meeting Note发给大家，可以很好地追踪会议，有依可考。&lt;/p&gt;
&lt;h2&gt;4. Ken :如何推进一个项目？&lt;/h2&gt;
&lt;p&gt;Wendy: 很多时候，项目沟通不是一次会议就能解决的。首先启动的第一个会议是动员会议，Wave Your hand，先给大家项目信息，你的预期结果、KPI目标，带给相关组的利益等。项目沟通需要做努力，采用不同形式和内容，多沟通几次。要影响不同的人，不要轻易放弃。如果对方对你的项目不感兴趣，可以用上级、合作方来帮你“站台”。你需要有这个态度：“我要影响我的老板，让老板再影响他的领导，让每一个角色都能参与合作” 。如果一条路走不通，可以多换几条路试试。例如， 我之前在一家公司时做了一个非常有用的算法，但在给客户推广时遇到阻力，很多客户不想更新系统。于是我先和Skip level的领导以及大组的PM沟通，通过他们来影响客户。另外，如果相关组需要试运行的话，自己可以先设计收集反馈的平台，列好需要反馈的内容和时间，尽量减轻对方的工作量，给他们好的合作体验。此外，项目沟通也是交换的过程，不光需要别人做什么，而是要设身处地站在对方的角度上想到别人能得到什么利益。在项目推进的过程中，要做好及时沟通，明确下一步的内容，采取do X by Y at Z的方式给对方合理的预期。&lt;/p&gt;
&lt;h2&gt;6. Ken: 当你代表团队去演讲时该怎么展示？&lt;/h2&gt;
&lt;p&gt;Wendy：演讲在于展示影响力和曝光度，演讲目的是通过展示和讲述来增加可见度，获取认可、支持、资源等。演讲前需要做充分准备工作，自己可多演练几遍熟悉内容，对可能会被问到问题的地方进行模拟回答。注意，不要自己做好幻灯片到了汇报时候才展示给大家，而是应该提前让老板和你的队友一起帮你过一遍幻灯片内容，让他们给出反馈并帮你改进。演讲时，要“演”和“讲”相结合，“演”是通过你的面部表情、肢体语言来调动大家的情绪， “讲”的关键是陈述过程中能将自己的观点表述清楚，从而更好地达到沟通目的。演讲方式可多种，如“Show final result”、“Show intermediate result”等，让听众知道你或者你的组的价值，为寻求到下一步的资源或支持做好铺垫。平时工作中要积极争取演讲的机会。一方面可以让自己被更多人看到、获取资源和建立合作关系，另外一方面也可以锻炼自己的表达沟通能力，讲好故事的能力，把演讲当作自己职业发展的一个机会。&lt;/p&gt;
&lt;h2&gt;7. Ken：和领导或者同事意见不一致时该怎么沟通？&lt;/h2&gt;
&lt;p&gt;Wendy：当会议上和别人有不同意见时，建议不要立即反驳。要先表达认可对方解决问题的态度，然后通过分析利弊，评估风险等角度有理有据地提供备选方案或指出问题。有时候提出问题，让对方自己发现破绽比直接反驳更有效。在讨论激烈插不上话时可以采用对话框以文字形式礼貌提出。当跟自己利益冲突时，不要急于去回应，可以先听对方讲述，然后提出问题，或者说自己需要时间做一下了解、研究，然后再跟这个人私底沟通，为自己争取准备时间。如果是团队与团队之间的冲突，沟通前要获得队友支持，先和他们打好招呼，制定好方案，然后协同作战。如果遇到老板否定你的方案，并且要按照他的方案执行时，可以约一个时间请老板分享一下他是怎么做决策的，都权衡了哪些因素，把这个当作一个了解老板做事风格的一次机会，不要一味的被动接受，帮助你在今后工作中更好地和老板沟通或者保持一致。如果你经过思考了解后觉得对方是对的，也要跟对方更新自己采用他们的建议或者方案，并及时表达感谢。&lt;/p&gt;
&lt;h2&gt;8. Ken: 非正式的日常沟通重要吗？&lt;/h2&gt;
&lt;p&gt;Wendy：非常重要。非正式的沟通有助于和别人建立关系，获取信任，当有项目合作的机会，别人也许会首先想到你。大家都喜欢和自己信任、熟悉、兴趣相同的人合作，会让人觉得舒服。非正式沟通可以找一个共同话题去打开他的内心。例如，如果你的合作方养狗你刚好也养狗，Small talk或闲聊时可以聊一些关于狗的话题就很容易建立关系。在公司要扩大自己的Network，多刷脸机会就更多。非正式沟通可以约一起吃午饭或者喝咖啡，也可以是给对方发一些安慰、鼓励、认可的邮件，还可以在公司内部或外部做志愿者、积极组织参加公司活动。做志愿者是一个很好的途径，通过它可以展示你的领导力、沟通能力、团队合作精神，让同事们看到你工作外的另一面，给别人留一个好印象。非正式沟通还可以通过撰写工作Documentation,把自己作为某些资源的联系人，在给对方提供解答或者资源时获得建立关系的机会。日常沟通要尝试给予，而不仅是索取，不要总是想着从别人那里得到，你也要有一颗时刻帮助别人的心。&lt;/p&gt;
&lt;p&gt;最后，Wendy总结， 沟通是双向的，你要知道你沟通的目的，自己心理有个预期，根据预期展开对话。&lt;/p&gt;</content><category term="career growth"></category><category term="career growth"></category></entry><entry><title>Recommendation System 03</title><link href="/pages/2021/11/06/recommendation-system-03/" rel="alternate"></link><published>2021-11-06T18:58:00-05:00</published><updated>2021-11-06T18:58:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-11-06:/pages/2021/11/06/recommendation-system-03/</id><summary type="html">&lt;p&gt;multi-armed bandit and how to use it in CTR optimization&lt;/p&gt;</summary><content type="html">&lt;p&gt;Purpose: achieve the return as much as possible (reduce the regret)&lt;/p&gt;
&lt;p&gt;Suppose we want to show the advertisements to the customer and we want to maximize the CTR (click through rate). Let's simplify the situation as there are 3 ads: ad1, ad2 and ad3, the average CTR for them is &lt;span class="math"&gt;\(\mu_1 = 0.1, \mu_2 = 0.09, \mu_3=0.0\)&lt;/span&gt;. If we can show the ads 300 times, then which ad shall we show? In this case, surely we will display ad1 300 times since it is most likely to be clicked by the customer.&lt;/p&gt;
&lt;p&gt;But if we not know the value of &lt;span class="math"&gt;\(\mu_1, \mu_2, \mu_3\)&lt;/span&gt;, what shall we do?&lt;/p&gt;
&lt;h2&gt;Exploration only&lt;/h2&gt;
&lt;p&gt;Since we don't know which ad will higher ctr, so we may just randomly select an ad to display. In this case, ad1, ad2 and ad3 will be displayed 100 times each. This is called exploration only which means doing it randomly. But it is not good since there is better way: after some random trials (let's say 50 times), we may calcuate the average ctr for each ad, and we may find ad1 has higher ctr than the rest. Then we should display ad1 in the rest 250 times.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Ad():
    def __init__(self, mu, sigma):
        self.mu = mu
        self.sigma = sigma

    def ctr_sample(self):
        return np.random.normal(self.mu, self.sigma)


def explore(ads, ntotal=300):
    &amp;quot;&amp;quot;&amp;quot;
    ads is a list of random variables
        each represents the distribution of the corresponding ad
    ntotal: is the total number of trials we can test
    &amp;quot;&amp;quot;&amp;quot;
    rewards = []
    for _ in range(ntotal):
        rewards.append(random.choice(ads).ctr_sample())
    return rewards

ad1, ad2, ad3 = Ads(0.1, 0.03), Ads(0.09, 0.03), Ads(0.08, 0.03) 

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Epsilon-greedy bandit&lt;/h2&gt;
&lt;p&gt;Rather than choosing the ads randomly, we will try some random choice to explore. After we collect some information, we will exploit to the most highly returned ad. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Begin by randomly choosing an ad for each visitor and observing whether it receives a click. &lt;/li&gt;
&lt;li&gt;As each ad is chosen and the click observed, keep track of the CTR (empirical CTR). &lt;/li&gt;
&lt;li&gt;Start assigning “most” visitors to your top performing ad by empirical CTR at that point in time, otherwise randomly choose among your other ads. &lt;/li&gt;
&lt;li&gt;As the empirical CTR changes with more trials, continue updating which ad you assign to most visitors. &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The key parameter we decide upon for an epsilon-greedy bandit is &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;. &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt; manages the exploration / exploitation trade-off by determining what proportion of ads we assign to exploration &lt;span class="math"&gt;\(\epsilon\)&lt;/span&gt;, and exploitation, &lt;span class="math"&gt;\(1 - \epsilon\)&lt;/span&gt;. For example, if &lt;span class="math"&gt;\(\epsilon = 0.1\)&lt;/span&gt;, means 10% of the 300 total trials will be used for exploration (30 times).&lt;/p&gt;
&lt;p&gt;Let's denote &lt;span class="math"&gt;\(P_t(i)\)&lt;/span&gt; as the probability to choose ad &lt;span class="math"&gt;\(i\)&lt;/span&gt; in the &lt;span class="math"&gt;\(t_{th}\)&lt;/span&gt; trial, &lt;span class="math"&gt;\(K\)&lt;/span&gt; as the number of ads, and &lt;span class="math"&gt;\(N_t(i)\)&lt;/span&gt; is the number of trials for ad &lt;span class="math"&gt;\(i\)&lt;/span&gt; in the &lt;span class="math"&gt;\(t\)&lt;/span&gt; trials. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random

class EpsilonGreedyBandit():
    def __init__(self, true_ctr, epsilon = 0.1):
        self.epsilon = epsilon
        self.num_ads = len(true_ctr)
        self.ad_ids = np.arange(self.num_ads)
        self.weights = np.full(self.num_ads, 1/ self.num_ads)
        self.clicks = np.zeros(self.num_ads)
        self.trials = np.zeros(self.num_ads)
        self.true_ctr = true_ctr
        self.empirical_ctr = np.zeros(self.num_ads)

    def choose_ad(self):
        &amp;quot;&amp;quot;&amp;quot;
        Choose ad to display based on the updated weights
        1. Choose the current best ad with prob 1-epislon
        2. Or randomly explore with prob epislon/(num_ads - 1)
        &amp;quot;&amp;quot;&amp;quot;
        ad = np.random.choice(self.ad_ids, size = 1, p = self.weights)[0]
        return ad

    def click(self, ad):
        &amp;quot;&amp;quot;&amp;quot;
        Customer click with prob = true_ctr
        &amp;quot;&amp;quot;&amp;quot;
        click = np.random.binomial(size = 1, n = 1, p=self.true_ctr[ad])[0]
        return click


    def update(self, ad, click):
        &amp;quot;&amp;quot;&amp;quot;
        For given ad, update the values
        &amp;quot;&amp;quot;&amp;quot;
        self.clicks[ad] += click
        self.trials[ad] += 1
        self.empirical_ctr[ad] = self.clicks[ad] / self.trials[ad]
        if self.clicks.any():
            self.weights[:] = self.epsilon / (self.num_ads - 1)
            best_ad = np.argmax(self.empirical_ctr)
            self.weights[best_ad] = 1 - self.epsilon

    def run_trial(self):
        ad = self.choose_ad()
        click = self.click(ad)
        self.update(ad, click)


# Run an example

emp_ctr = []
ctr = [0.1, 0.09, 0.08]

for i in range(6):
    egb = EpsilonGreedyBandit(ctr)
    for j in range(10**i):
        egb.run_trial()

    emp_ctr.append(egb.empirical_ctr)

emp_ctr = np.array(emp_ctr)

fig, ax = plt.subplots(figsize=(16, 8))

for i in range(len(ctr)):
    plt.plot(emp_ctr[:, i], label = &amp;quot;CTR for ad&amp;quot; + str(i))
plt.legend()
plt.title('Epsilon Greedy Bandit CTR Simulation')
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="multi armd bandit 01" src="/figures/20211106_multi_armd_bandit_01.png"&gt;
As we can see, the empirical CTR will concerge to the true CTR and the best ad will be chosen.&lt;/p&gt;
&lt;h2&gt;Upper confidence bound (UCB)&lt;/h2&gt;
&lt;p&gt;In Epsilon-greedy bandit, we only choose the ad based on the empirical_ctr. This might be a little risky since you may lose some information. Let's say there are two ads, after some trials, ad1 has empirical_ctr=0.1, and ad2 has empirical_ctr=0.11. In this case, our algorithm will choose ad2 to exploit since it has higher empirical_ctr. But if ad1 is only shown once, and ad2 has already been shown 20 times. Under this scenario, shall we give ad1 a little more change since it is only being shown 1 time?&lt;/p&gt;
&lt;p&gt;UCB is the complement method. In UCB, the best ad is chosen based on who has the highest &lt;span class="math"&gt;\(\mbox{empirical_ctr} + \sqrt{{2 ln(t)}/{N_t(i)}}\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(N_t(i)\)&lt;/span&gt; is the number of trials for ad &lt;span class="math"&gt;\(i\)&lt;/span&gt; in the &lt;span class="math"&gt;\(t\)&lt;/span&gt; trials. In the scenario above, ad1 was only shown 1 time, so &lt;span class="math"&gt;\(N_t(1) = 1\)&lt;/span&gt; and &lt;span class="math"&gt;\(N_t(2)=20\)&lt;/span&gt; since ad2 was shown 20 times. Because &lt;span class="math"&gt;\(N_t(i)\)&lt;/span&gt; is in the denominator, as it higher (already shown more time), the corresponding has less likelihood to be selected again unless it has very high empirical_ctr.&lt;/p&gt;
&lt;h2&gt;Thompson sampling bandits&lt;/h2&gt;
&lt;p&gt;Thompson sampling bandits (Bayesian bandits) is to chose the action that maximize the expected reward. It aims to tackle the exploration-exploitation trade off in a Bayesian way.&lt;/p&gt;
&lt;p&gt;Let denote the action of click or not as a random variable &lt;span class="math"&gt;\(X, X=1\)&lt;/span&gt; means click and &lt;span class="math"&gt;\(0\)&lt;/span&gt; means not. If we consider CTR as a parameter &lt;span class="math"&gt;\(p\)&lt;/span&gt; which is unknown, that is &lt;span class="math"&gt;\(P(X=1) = p\)&lt;/span&gt;. In fact, &lt;span class="math"&gt;\(X\)&lt;/span&gt; has Bernoulli distribution &lt;span class="math"&gt;\(P(X=x|p) = p^x (1-p)^{1-x}\)&lt;/span&gt;. In this scenario, the conjugate prior for &lt;span class="math"&gt;\(p\)&lt;/span&gt; is beta distribution &lt;span class="math"&gt;\(p \sim Beta(a, b) = p^{a-1} (1-p)^{b-1}\)&lt;/span&gt;. The posterior distribution is also Beta distribution &lt;span class="math"&gt;\(\sim Beta(a+x, b+1-x) = p^{a-1+x} (1-p)^{b-1 + 1-x }\)&lt;/span&gt;. That is, if the ad is clicked &lt;span class="math"&gt;\(x = 1\)&lt;/span&gt;, so &lt;span class="math"&gt;\(a\)&lt;/span&gt; will add 1. Otherwise &lt;span class="math"&gt;\(b\)&lt;/span&gt; will add 1. If we have 10 ads, then we will have 10 posterior beta distributions with their own &lt;span class="math"&gt;\(a\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt;. To update the simulation, for each step, we will sample a random value (which is &lt;span class="math"&gt;\(p\)&lt;/span&gt;) from each of the posterior distribution and pick the ad with the max sampled &lt;span class="math"&gt;\(p\)&lt;/span&gt; to show.&lt;/p&gt;
&lt;p&gt;The general idea is: suppose at time &lt;span class="math"&gt;\(t\)&lt;/span&gt; (already run &lt;span class="math"&gt;\(t\)&lt;/span&gt; trials in total),  &lt;span class="math"&gt;\(N_t(i)\)&lt;/span&gt; is the number of trials for ad &lt;span class="math"&gt;\(i\)&lt;/span&gt; in the &lt;span class="math"&gt;\(t\)&lt;/span&gt; trials. The posterior probability for ad i is given by the formula above. To do the simulation, we only need these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly draw a CTR from each ad’s posterior beta distribution.&lt;/li&gt;
&lt;li&gt;Choose the ad with the highest drawn CTR.&lt;/li&gt;
&lt;li&gt;If a click is observed increment &lt;span class="math"&gt;\(a\)&lt;/span&gt; by 1, otherwise increment &lt;span class="math"&gt;\(b\)&lt;/span&gt; by 1. (&lt;span class="math"&gt;\(\alpha\)&lt;/span&gt; and &lt;span class="math"&gt;\(\beta\)&lt;/span&gt; in the Beta distribution.)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class ThompsonSamplingBandit():
    def __init__(self, true_ctr):
        self.num_ads = len(true_ctr)
        self.ad_ids = np.arange(self.num_ads)
        self.clicks = np.zeros(self.num_ads)
        self.alphas = np.ones(self.num_ads)
        self.betas = np.ones(self.num_ads)
        self.trials = np.zeros(self.num_ads)
        self.true_ctr = true_ctr
        self.empirical_ctr = np.zeros(self.num_ads)

    def choose_ad(self):
        beta_draws = np.zeros(self.num_ads)
        for ad in self.ad_ids:
            draw = np.random.beta(self.alphas[ad], self.betas[ad], size = 1)[0]
            beta_draws[ad] = draw
        best_ad = np.argmax(beta_draws)
        return best_ad

    def click(self, ad):
        click = np.random.binomial(size = 1, n = 1, p = self.true_ctr[ad])[0]
        return click

    def update(self, ad, click):
        self.trials[ad] += 1
        self.clicks[ad] += click
        self.empirical_ctr[ad] = self.clicks[ad] / self.trials[ad]
        if click:
            self.alphas[ad] += 1
        else:
            self.betas[ad] += 1

    def run_trial(self):
        ad = self.choose_ad()
        click = self.click(ad)
        self.update(ad, click)

tsb = ThompsonSamplingBandit(true_ctr = [.2, .15, .1])
for _ in range(200000):
    tsb.run_trial()

print(tsb.empirical_ctr)

# array([0.20028314, 0.14479638, 0.09352518])

emp_ctr = []
ctr = [0.2, 0.15, 0.08]

for i in range(6):
    egb = ThompsonSamplingBandit(ctr)
    for j in range(10**i):
        egb.run_trial()

    emp_ctr.append(egb.empirical_ctr)

emp_ctr = np.array(emp_ctr)

fig, ax = plt.subplots(figsize=(16, 8))

for i in range(len(ctr)):
    plt.plot(emp_ctr[:, i], label = &amp;quot;CTR for ad&amp;quot; + str(i))
plt.legend()
plt.title('Thompson Sampling Bandit CTR simulation')
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="multi armd bandit 01" src="/figures/20211106_multi_armd_bandit_02.png"&gt;&lt;/p&gt;
&lt;p&gt;However, in reality, &lt;span class="math"&gt;\(p\)&lt;/span&gt; is not simply a random value, itself maybe a function of different variables / setups. Let’s assume we use logistic regression f to model &lt;span class="math"&gt;\(p\)&lt;/span&gt;. That is, &lt;span class="math"&gt;\(p = f(variables, weights)\)&lt;/span&gt; and we denote weights as &lt;span class="math"&gt;\(w\)&lt;/span&gt; which is a vector with &lt;span class="math"&gt;\(dim=d\)&lt;/span&gt;. Now we need to find &lt;span class="math"&gt;\(w\)&lt;/span&gt; to optimize CTR. The conjugate prior for &lt;span class="math"&gt;\(w\)&lt;/span&gt; is normal distribution. And the corresponding posterior of &lt;span class="math"&gt;\(w\)&lt;/span&gt; is also normal distribution. After we observed (x, y), w and the posterior will be updated.&lt;/p&gt;
&lt;p&gt;In a more general case, the number of variables and &lt;span class="math"&gt;\(w\)&lt;/span&gt; of logistic regression may also be random. In this case, we cannot use fixed number of &lt;span class="math"&gt;\(dim=d\)&lt;/span&gt; priors of normal distribution. We need to extend it to Gaussian processes. &lt;/p&gt;
&lt;p&gt;Now we will define the &lt;strong&gt;Bayesian Optimization&lt;/strong&gt; problem.&lt;/p&gt;
&lt;p&gt;Objection: &lt;/p&gt;
&lt;div class="math"&gt;$$\mbox{max}_{x \in A}  f(x), \mbox{where } x \in \mathbb{R}^d$$&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;output: Objection function &lt;span class="math"&gt;\(f\)&lt;/span&gt; is continuous&lt;/li&gt;
&lt;li&gt;Input: complexity depends on dim &lt;span class="math"&gt;\(d\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Restriction of &lt;span class="math"&gt;\(A\)&lt;/span&gt;: hyper-rectangle &lt;span class="math"&gt;\(\{ x \in \mathbb{R}^d: a_i \le x_i \le b_i \}\)&lt;/span&gt; or simplex &lt;span class="math"&gt;\(\{ x \in \mathbb{R}^d: \sum_{i} x_i = 1 \}\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(f\)&lt;/span&gt;: usually &lt;span class="math"&gt;\(f\)&lt;/span&gt; is very complicated, it is not linear or convex. It is called black-box.&lt;/li&gt;
&lt;li&gt;Evaluation: usually there is restriction on the number of trials to evaluate &lt;span class="math"&gt;\(f\)&lt;/span&gt;. derivative-free&lt;/li&gt;
&lt;li&gt;Target: the target is to find global optimizaion &lt;span class="math"&gt;\(x^*\)&lt;/span&gt; to max &lt;span class="math"&gt;\(f\)&lt;/span&gt;. It is called global optimization.&lt;/li&gt;
&lt;li&gt;With these, Bayesian optimization is also called black-box derivative-free global optimization.  &lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It mainly includes two parts: Gaussian Process Regression and Acquisition Function&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;GPR: based on prior and observations to calculate the posterior distribution&lt;/li&gt;
&lt;li&gt;AF: from the posterior of GPR to get surrogate to find next point to be evaluated&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To be continued.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>Recommendation System 02</title><link href="/pages/2021/11/06/recommendation-system-02/" rel="alternate"></link><published>2021-11-06T11:08:00-05:00</published><updated>2021-11-06T11:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-11-06:/pages/2021/11/06/recommendation-system-02/</id><summary type="html">&lt;p&gt;paper reading and summary&lt;/p&gt;</summary><content type="html">&lt;p&gt;List some papers to read for better understanding and practical use.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RecSys'16, YouTube | Deep Neural Networks for YouTube Recommendations&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="fig1" src="/figures/20211106_recommendation_system_01.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Retrival&lt;/em&gt;: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use mean pooling (average) for sequential rfeatures embedding&lt;/li&gt;
&lt;li&gt;Example age is the the time between user click and training. It's used to describe the bias that user are more likely to click the new video. When applying the model online, the value for this feature is 0.&lt;/li&gt;
&lt;li&gt;When training offline, the user embedding and view embedding will be saved offline. When running online, it will be called by hash. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="fig2" src="/figures/20211106_recommendation_system_02.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Ranking&lt;/em&gt;: &lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The model structure is similar to retrival &lt;/li&gt;
&lt;li&gt;Features: use more features to describe the users, videos and behaviors. &lt;/li&gt;
&lt;li&gt;Target: if only use CTR, then some videos with sensational title will be recommended. In the training, it uses weighted loss function and the weight is related to video length. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is the very early paper to apply deep learning in recommendation system rather than using GBDT or logistic regression. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CIKM'20, jd.com | Deep Multifaceted Transformers for Multi-objective Ranking in Large-Scale E-commerce Recommender Systems&lt;/li&gt;
&lt;li&gt;AAAI'20, Alibaba | Deep Match to Rank Model for Personalized Click-Through Rate Prediction&lt;/li&gt;
&lt;li&gt;AAAI'21, Tencent | U-BERT: Pre-training User Representations for Improved Recommendation&lt;/li&gt;
&lt;li&gt;IJCAI'18, Microsoft | Sequential Recommender System based on Hierarchical Attention Network&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To be continued.&lt;/p&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>Recommendation System 01</title><link href="/pages/2021/10/30/recommendation-system-01/" rel="alternate"></link><published>2021-10-30T00:00:00-05:00</published><updated>2021-10-30T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-10-30:/pages/2021/10/30/recommendation-system-01/</id><summary type="html">&lt;p&gt;Recommendation system introduction - collaborative filtering, DNN for query and item embedding&lt;/p&gt;</summary><content type="html">&lt;p&gt;The purpose of recommendation system is to recommend the related products (item) to the users based on the user's query.&lt;/p&gt;
&lt;h1&gt;1. Terminology&lt;/h1&gt;
&lt;h2&gt;1.1. Items (also known as documents)&lt;/h2&gt;
&lt;p&gt;The entities a system recommends. For the Google Play store, the items are apps to install. For YouTube, the items are videos.&lt;/p&gt;
&lt;h2&gt;1.2. Query (also known as context)&lt;/h2&gt;
&lt;p&gt;The information a system uses to make recommendations. Queries can be a combination of the following:&lt;/p&gt;
&lt;h3&gt;user information&lt;/h3&gt;
&lt;p&gt;the id of the user
  items that users previously interacted with&lt;/p&gt;
&lt;h3&gt;additional context&lt;/h3&gt;
&lt;p&gt;time of day
  the user's device&lt;/p&gt;
&lt;p&gt;目的就是把item推荐给user based on user information.&lt;/p&gt;
&lt;h2&gt;1.3. Embedding&lt;/h2&gt;
&lt;p&gt;A mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space. Many recommendation systems rely on learning an appropriate embedding representation of the queries and items.&lt;/p&gt;
&lt;h1&gt;2. Recommendation Systems Overview&lt;/h1&gt;
&lt;p&gt;&lt;img alt="image" src="https://developers.google.cn/machine-learning/recommendation/images/Process.svg"&gt;&lt;/p&gt;
&lt;h2&gt;2.1. Candidate Generation / Retrival&lt;/h2&gt;
&lt;p&gt;In this first stage, the system starts from a potentially huge corpus and generates a much smaller subset of candidates. For example, the candidate generator in YouTube reduces billions of videos down to hundreds or thousands. The model needs to evaluate queries quickly given the enormous size of the corpus. A given model may provide multiple candidate generators, each nominating a different subset of candidates.&lt;/p&gt;
&lt;h2&gt;2.2. Scoring&lt;/h2&gt;
&lt;p&gt;Next, another model scores and ranks the candidates in order to select the set of items (on the order of 10) to display to the user. Since this model evaluates a relatively small subset of items, the system can use a more precise model relying on additional queries.&lt;/p&gt;
&lt;h2&gt;2.3. Re-ranking&lt;/h2&gt;
&lt;p&gt;Finally, the system must take into account additional constraints for the final ranking. For example, the system removes items that the user explicitly disliked or boosts the score of fresher content. Re-ranking can also help ensure diversity, freshness, and fairness.&lt;/p&gt;
&lt;p&gt;We will discuss each of these stages over the course of the class and give examples from different recommendation systems, such as YouTube.&lt;/p&gt;
&lt;h1&gt;3. Candidate Generation Overview&lt;/h1&gt;
&lt;pre&gt;&lt;code&gt;1. content-based filtering: recommendate based on the similarity of items
2. collaborative filtering： if user A and user B are similar, then recommend user A's items to user B.
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3.1. Embedding Space: map item and query to a &lt;span class="math"&gt;\(d\)&lt;/span&gt; dimension space&lt;/h2&gt;
&lt;p&gt;Map each item and each query (or context) to an embedding vector in a common embedding space . Typically, the embedding space is low-dimensional &lt;span class="math"&gt;\(E = \mathbb{R}^{d}\)&lt;/span&gt; (that is,  is much smaller than the size of the corpus), and captures some latent structure of the item or query set.&lt;/p&gt;
&lt;h2&gt;3.2. Similarity Measures：based on embedding to find item, or find the related items and querys&lt;/h2&gt;
&lt;p&gt;A similarity measure is a function &lt;span class="math"&gt;\(s: E X E -&amp;gt; \mathbb{R}\)&lt;/span&gt; that takes a pair of embeddings and returns a scalar measuring their similarity. The embeddings can be used for candidate generation as follows: given a query embedding &lt;span class="math"&gt;\(q \in E\)&lt;/span&gt;, the system looks for item embeddings &lt;span class="math"&gt;\(x \in E\)&lt;/span&gt; that are close to &lt;span class="math"&gt;\(q\)&lt;/span&gt;, that is, embeddings with high similarity &lt;span class="math"&gt;\(s(q, x)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(cos(x,y)\)&lt;/span&gt;: the anger between two vectors. Below C and item has the least anger degree.&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(dot(x, y)\)&lt;/span&gt;：dot product, the length of vector 1 times the length of vector 2, then times cos(anger). A is the longest.&lt;/p&gt;
&lt;p&gt;Eculidian distance ||x - y||: which is the distance of two vectors. B and query has the shortest dist.&lt;/p&gt;
&lt;p&gt;&lt;img alt="看看那个距离是对的" src="https://developers.google.cn/machine-learning/recommendation/images/Similarity.svg"&gt;&lt;/p&gt;
&lt;h2&gt;3.3. Which Similarity Measure to Choose?&lt;/h2&gt;
&lt;p&gt;If use dot prod, the bigger the norm of the vector, the more likely it will be selected. The item has more frequency are usually have embedding with bigger norm, which will result in the common items (or most viewed videos) are more likely to be recommended. &lt;/p&gt;
&lt;h1&gt;4. Content-based Filtering&lt;/h1&gt;
&lt;p&gt;Which recommend based on similar items. Ignore here sicne it is easy. &lt;/p&gt;
&lt;h1&gt;5. &lt;a href="https://developers.google.cn/machine-learning/recommendation/collaborative/basics"&gt;Collaborative Filtering&lt;/a&gt;&lt;/h1&gt;
&lt;h2&gt;5.1. Basics&lt;/h2&gt;
&lt;p&gt;Take an example to recommend movies to the customer. One row is one customer, and one column is one movie. The target is: for a given customer, recommend movies to him based on the customers that are clsoe to him.&lt;/p&gt;
&lt;p&gt;Suppose movies has these features: name, rating, introduction&lt;/p&gt;
&lt;h3&gt;5.1.1. 1-d embedding&lt;/h3&gt;
&lt;p&gt;Suppose we can score the movie from -1 to 1 based on if it is for kids or for adults. -1 means for kids and 1 means only for adults. With this each movie will have a score from -1 to 1. In the same way, we can score the customers based on they are kids or adults. &lt;/p&gt;
&lt;p&gt;&lt;img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1D.svg"&gt;&lt;/p&gt;
&lt;p&gt;Now in the (user，item) feedback matrix we have the "like" from the user to the movie.
&lt;img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1Dmatrix.svg"&gt;&lt;/p&gt;
&lt;h3&gt;5.1.2. 2-d embedding&lt;/h3&gt;
&lt;p&gt;Most of the time, 1-d cannot describe the complexity of the movie. We may need 2-d or high dimension to describe. Suppose we have another dimenison to describe the movie is Arthose or Blockbuster.&lt;/p&gt;
&lt;p&gt;&lt;img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2D.svg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2Dmatrix.svg"&gt;&lt;/p&gt;
&lt;h2&gt;5.2. &lt;a href="https://developers.google.cn/machine-learning/recommendation/collaborative/matrix"&gt;Matrix Factorization&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Assume feedback matrix &lt;span class="math"&gt;\(A \in \mathbb{R}^{m \times n}\)&lt;/span&gt;, m x n matrix. m means m user, and n means n items. &lt;/p&gt;
&lt;p&gt;Matrix Factorization is to get a matrix U and matrix V, such that&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. U is matrix m x d, each row is an user's embedding
2. V is matrix n x d, each row is an item's embedding
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our goal is to factorize the ratings matrix &lt;span class="math"&gt;\(A\)&lt;/span&gt; into the product of a user embedding matrix &lt;span class="math"&gt;\(U\)&lt;/span&gt; and movie embedding matrix &lt;span class="math"&gt;\(V\)&lt;/span&gt;, such that &lt;span class="math"&gt;\(A \approx UV^\top\)&lt;/span&gt; with&lt;/p&gt;
&lt;div class="math"&gt;$$U = \begin{bmatrix} u_{1} \\ \hline \vdots \\ \hline u_{N} \end{bmatrix}$$&lt;/div&gt;
&lt;div class="math"&gt;$$V = \begin{bmatrix} v_{1} \\ \hline \vdots \\ \hline v_{M} \end{bmatrix}$$&lt;/div&gt;
&lt;p&gt;Here&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- $N$ is the number of users,
- $M$ is the number of movies,
- $A_{ij}$ is the rating of the $j$th movies by the $i$th user,
- each row $U_i$ is a $d$-dimensional vector (embedding) representing user $i$,
- each row $V_j$ is a $d$-dimensional vector (embedding) representing movie $j$,
- the prediction of the model for the $(i, j)$ pair is the dot product $\langle U_i, V_j \rangle$.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;embedding matrix U and V has &lt;span class="math"&gt;\(UV^T\)&lt;/span&gt; as approx of feedback matrix A. A has dim &lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;, the new U V has dim = &lt;span class="math"&gt;\(（m+n）\times d\)&lt;/span&gt;. Usually d is small, which means we will use low dim matrix to approx the original A. Singular value decomposion (SVD) is not a good method here because A is sparse most of the time.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Stochastic gradient descent (SGD) is a generic method to minimize loss functions.
2. Weighted Alternating Least Squares (WALS) is specialized to this particular objective.

1. pros: no special pre-knowledge, Serendipity (model help to get likes), easy to start
2. cons: cannot handle new item, cannot use side features (age, address)- high dim feedback matrix A
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=recommendation-systems"&gt;使用colab来练习怎么做推荐系统&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv"&gt;kaggle data link&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;6. &lt;a href="https://developers.google.cn/machine-learning/recommendation/dnn/softmax"&gt;Recommendation with Deep Neural Network (DNN) Models&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;DNN can put item feature and user feature into a network modle to learn embeddings.&lt;/p&gt;
&lt;h2&gt;Softmax DNN for Recommendation&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;input: user query  
output: prob of n items
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;input X can be dense feature (watch time length, time since last watch), it can also be sparse feature (country, watch history).&lt;/p&gt;
&lt;p&gt;&lt;img alt="DNN for recommendaiton system" src="https://developers.google.cn/machine-learning/recommendation/images/LossFunction.svg"&gt;&lt;/p&gt;
&lt;p&gt;Last hidden layer of DNN output is &lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;, which is a vector with dim = d. It will times a matrix V &lt;span class="math"&gt;\(V \in n \times d\)&lt;/span&gt; to get a vector with dim = n. Each value is the probability for item.&lt;/p&gt;
&lt;p&gt;Loss funciton is the function of  model output &lt;span class="math"&gt;\(\hat{p}\)&lt;/span&gt; and the true item y. We can use cross entropy.&lt;/p&gt;
&lt;p&gt;The purpose of the model is to learn embedding, s.t. for each item j, there is an vector &lt;span class="math"&gt;\(v_j \in \mathbb{R}^d\)&lt;/span&gt; (n items, then &lt;span class="math"&gt;\(V \in n \times d\)&lt;/span&gt;). With softmax model, V are the parameters of the model.&lt;/p&gt;
&lt;p&gt;For query embedding, for query i, model is not to get embedding &lt;span class="math"&gt;\(U_{i}\)&lt;/span&gt;, but to learn the function &lt;span class="math"&gt;\(\phi(\cdot)\)&lt;/span&gt;, which will map input x  to a d-dim vector &lt;span class="math"&gt;\(\phi(x)\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Furthermore, can we build two DNN:
    1. 1st DNN is to get function &lt;span class="math"&gt;\(\phi(x_{item}) \in \mathbb{R}^{d}\)&lt;/span&gt;, map item to d-dim space
    2. 2nd DNN is to get funciton &lt;span class="math"&gt;\(\psi(x_{query}) \in \mathbb{R}^{d}\)&lt;/span&gt;, map query  to d-dim space&lt;/p&gt;
&lt;p&gt;Finally we can use dot product &amp;lt; &lt;span class="math"&gt;\(\phi(x_{item})\)&lt;/span&gt;, &lt;span class="math"&gt;\(\psi(x_{query})\)&lt;/span&gt; &amp;gt; to get the correlation between query i and item j.&lt;/p&gt;
&lt;h2&gt;Train the model&lt;/h2&gt;
&lt;p&gt;The training data includes the user's query and the user's side features, and the items that the user has been interested before for each user. The forward and backward propagation is based on the design of the DNN. The minium of the loss funciton can be achieved by stochastic gradient descending algorithm.  &lt;/p&gt;
&lt;h2&gt;Negative sampling&lt;/h2&gt;
&lt;p&gt;When there are n items, the output prob from DNN is &lt;span class="math"&gt;\(\hat{p} \in \mathbb{R}^{n}\)&lt;/span&gt;. Since n is very big, which will result in lots of calcualation in backpropagation.(In NLP, when clac the prob of neighbors words given the central word, it also uses negative smapling because of word size is also very big). &lt;/p&gt;
&lt;p&gt;Negative sampling will do: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. All positive items (the ones that appear in the target label)
2. A sample of negative items (j in 1, 2, 3, ... n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;An example from Youtube recommendation:
&lt;img alt="youtube" src="/figures/20211030_recommendation_01_youtube.png"&gt;&lt;/p&gt;
&lt;h1&gt;Reference&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://developers.google.cn/machine-learning/recommendation"&gt;google: 推荐系统介绍&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s?__biz=MzIwNDA5NDYzNA==&amp;amp;mid=2247484256&amp;amp;idx=1&amp;amp;sn=a92fc08b974339e1143c4f07b6591b72&amp;amp;chksm=96c42ea5a1b3a7b39c996f91471d47478fedde1b3c7a3f6538cdcb2d0b95407199381e6b7c80&amp;amp;cur_album_id=1555890573570523140&amp;amp;scene=189#rd"&gt;深入理解推荐系统：召回&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/114703091"&gt;深入理解YouTube推荐系统算法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/58160982"&gt;推荐系统召回四模型之：全能的FM模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;amp;utm_campaign=colab-external&amp;amp;utm_medium=referral&amp;amp;utm_content=recommendation-systems"&gt;使用colab来练习怎么做推荐系统&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv"&gt;kaggle上的data链接&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://mp.weixin.qq.com/s/yv27xqGDLTYG6mxguLIL0Q"&gt;情感分析技术在美团的探索与应用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/420995638"&gt;浅谈行为序列建模&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ax.dev"&gt;ax.dev&lt;/a&gt;  ---  &lt;a href="https://gpytorch.ai"&gt;GPyTorch&lt;/a&gt;  --- &lt;a href="https://botorch.org"&gt;BoTorch&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>职场话题——北美华人高管总结的职场上升方法</title><link href="/pages/2021/10/22/zhi-chang-hua-ti-bei-mei-hua-ren-gao-guan-zong-jie-de-zhi-chang-shang-sheng-fang-fa/" rel="alternate"></link><published>2021-10-22T00:00:00-05:00</published><updated>2021-10-22T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2021-10-22:/pages/2021/10/22/zhi-chang-hua-ti-bei-mei-hua-ren-gao-guan-zong-jie-de-zhi-chang-shang-sheng-fang-fa/</id><summary type="html">&lt;ol&gt;
&lt;li&gt;people skill: 组织，交流，setup vision -&amp;gt; roadmap -&amp;gt; project -&amp;gt; milestone -&amp;gt; metrics, 怎么improve people，怎么evaluate people&lt;/li&gt;
&lt;li&gt;工作努力，但是nobody understand you。会不会too techinical，要understand your audience，更要understand你的audience想要什么，让他们明白80%，在留给 …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;people skill: 组织，交流，setup vision -&amp;gt; roadmap -&amp;gt; project -&amp;gt; milestone -&amp;gt; metrics, 怎么improve people，怎么evaluate people&lt;/li&gt;
&lt;li&gt;工作努力，但是nobody understand you。会不会too techinical，要understand your audience，更要understand你的audience想要什么，让他们明白80%，在留给他们20%不懂得让他们感兴趣来follow&lt;/li&gt;
&lt;li&gt;有什么事情大胆的讲出来，要升职，还是要加薪。Meagan升职的例子&lt;/li&gt;
&lt;li&gt;大公司还是小公司&lt;/li&gt;
&lt;li&gt;怎么提高自信。&lt;ol&gt;
&lt;li&gt;相互share endorcement, share context, 找伙伴&lt;/li&gt;
&lt;li&gt;找competitor，来跟他们合作&lt;/li&gt;
&lt;li&gt;但是不要自信过头，容易会被排斥&lt;/li&gt;
&lt;li&gt;要自省。不断反省，日积月累&lt;/li&gt;
&lt;li&gt;我们（特别是从中国文化氛围下长大的我们）都是从别人的肯定来获取自信，要尝试改变。怔怔地自信来自于自己的成就感。要意识到自己的长处，passion，然后发挥特长，这样才会像滚雪球一样，越来越好，越来越自信。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;build relationship&lt;ol&gt;
&lt;li&gt;（crediabiltiy + reliability + honesty）/ self-orientation&lt;/li&gt;
&lt;li&gt;需要做的：增加自己的credibility，让自己在别人眼里更reliable，做事情transpartent and honest&lt;/li&gt;
&lt;li&gt;不要光想着自己，要想着别人&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;一个例子，一个亚麻跳槽出去的兄弟（或者姐妹）做的很好，在新的地方很快建立自己的威信&lt;ol&gt;
&lt;li&gt;把话讲明白，想做什么，不想做什么，希望自己的老板帮忙做什么，不要老板做什么都讲清楚 （namage up?）&lt;/li&gt;
&lt;li&gt;不管是不是份内的事情，都主动去做，哪怕技术难度很大&lt;/li&gt;
&lt;li&gt;让别人觉得放心&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;怎么找mentor，mentor对mentee的期望&lt;ol&gt;
&lt;li&gt;不是ask for sponser，而是共事的时候达到默契&lt;/li&gt;
&lt;li&gt;sponser会主动肯定你的贡献，能力，建议你去发扬光大。否则就是有问题，不是你做的不好，就是sponser做的不对。要找出问题在哪里&lt;/li&gt;
&lt;li&gt;mentor跟sponser不一样，mentor不牵扯到工作利益的事情。从mentor那儿能得到让自己提高的地方&lt;/li&gt;
&lt;li&gt;如果你是一个公司，你想招什么样的人？&lt;/li&gt;
&lt;li&gt;多总结，提高自己&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;分享书&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Never-Split-Difference-Negotiating-Depended/dp/0062407805"&gt;Never Split the Difference&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.amazon.com/Zero-to-One-audiobook/dp/B00M284NY2/ref=sr_1_1?dchild=1&amp;amp;keywords=zero+to+one&amp;amp;qid=1634964286&amp;amp;qsid=139-7844979-3259030&amp;amp;s=books&amp;amp;sr=1-1&amp;amp;sres=0804139296%2CB00W0TK2R0%2CB00QEUZ690%2CB08DBW1428%2CB00OHY87WS%2C1941259715%2CB09BY5WBV3%2C0645095877%2C3969260094%2CB089CSW3Y6%2CB09JRGQ3H9%2CB09JJJ73B5%2C0306924811%2CB09JR7ZL8W%2CB09JV9NFD4%2CB08F6J1CNK&amp;amp;srpt=ABIS_BOOK"&gt;Zero to One&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;还有一本，back howard？书名没有听清楚&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=UF8uR6Z6KLc"&gt;Steve Jobs - Stanford Commencement Address&lt;/a&gt;, &lt;a href="https://www.youtube.com/watch?v=vpW2sGlCtaE"&gt;Penn's 2011 Commencement Address by Denzel Washington&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;运气很重要 - 比你成功的不一定比你更懂，没有你成功的不一定没有你懂，有时候运气也很重要。理解了这个，你也会更humble，更能对自己有个清晰的认识&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Youtube: https://www.youtube.com/watch?v=jABowKQc3-g&lt;/p&gt;</content><category term="career growth"></category><category term="career growth"></category></entry><entry><title>职场话题——也随便讲讲职业发展 (转载)</title><link href="/pages/2020/12/22/zhi-chang-hua-ti-ye-sui-bian-jiang-jiang-zhi-ye-fa-zhan-zhuan-zai/" rel="alternate"></link><published>2020-12-22T19:08:00-06:00</published><updated>2020-12-26T19:08:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2020-12-22:/pages/2020/12/22/zhi-chang-hua-ti-ye-sui-bian-jiang-jiang-zhi-ye-fa-zhan-zhuan-zai/</id><summary type="html"></summary><content type="html">&lt;p&gt;前几天看了那个工资和职业发展的帖子，也有些感触想来讲讲。&lt;/p&gt;
&lt;p&gt;之前先说明几点：&lt;/p&gt;
&lt;p&gt;1）在闲话发，是因为这儿人多，牛人也多，想看看大家的想法和反馈。&lt;/p&gt;
&lt;p&gt;2）我知道职业发展不是大家都有的追求。比起钱来说，没有自己创业或者理财赚得多。比起自由度来说，没有太多弹性。而且人生那么多面，工作只是其中很小的一部分，其它有很多重要的方面，比如家庭，比如兴趣爱好。那这样的话，我为什么还要聊呢？是因为我觉得对大多数打工的人来说，既然我铁定要在公司里clock in 8-9个小时，另外路上还要花1-2个小时不等，这样9-11个小时投入下去，为什么在不扩大时间产出的情况下，尽量增加产出（升职加薪）呢？请看好我的前提是不做额外的加班。这就是我对这个话题感兴趣的初衷。&lt;/p&gt;
&lt;p&gt;3）再次申明发帖子不是为了宣扬自己做得好。相反，我做得远远没有之前几个发帖子的mm好。我年龄大（37），工龄长，之前因为私事走了无数弯路，也不在金融或软件这些热钱行业工作，所以没有震慑人的数字可以炫耀。我的目的很简单，最近两年觉得自己在这方面开始有点上轨道了。想一抛砖引玉，学习其他人经验；二，如果有20出头刚在职业成长期的mm，看了能有点启发的，我也觉得是能帮助人的。&lt;/p&gt;
&lt;p&gt;4）为了大家有个直观想法，我公开我的一些大背景细节：37岁，MBA，MBA之前6年工作经验，之后9年，都是在commercial这一块。传统型企业，我的level是director，带的组有6个人，两个国外，四个在美国，level都是associate director之类的。下个季度已经确定有进一步扩张scope，带的人会增加一些。工资方面，base 19万，现金bonus 7万，股票有，但我不算进我的年收入，因为要分4年vest。家庭方面有两个小孩，一个襁褓，一个幼儿园，都是要花大量时间的年龄段，老公平时很忙，但周末很给力。就这些，非常简单。我的目标也很简单，在不增加工作时间的前提下，如何升职到executive director。&lt;/p&gt;
&lt;p&gt;5）我不觉得我追求职业发展是因为内心贪婪。引用Bon Jovi的歌词：it doesn't make a difference if we make it or not。只是对工薪层来说，掌握升职的方法是个技巧，就象肌肉训练一样，任何人都可以从无到有练习。我真心希望看到更多的华人做到executive，这样我们的社会形象和话语权才会更高。为我们自己，也为了我们下一代。&lt;/p&gt;
&lt;p&gt;好了，废话结束，下面进入正题。（另外，我要带小孩，写完这些大概要一两天，请大家多包涵）&lt;/p&gt;
&lt;h3&gt;提纲&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;时间投入。Structured Productivity&lt;/li&gt;
&lt;li&gt;公司晋升文化&lt;/li&gt;
&lt;li&gt;工作中create WOW factor; Positioning: rare and valuable skill&lt;/li&gt;
&lt;li&gt;Build your team&lt;/li&gt;
&lt;li&gt;Key stakeholders: U movie&lt;/li&gt;
&lt;li&gt;Your style&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一、时间投入。structured productivity我不喜欢加班，平时家里小孩一大堆事，所以以为自己不是工作狂就不能晋升。直到我看到了一个人的博客Cal Newport。他每天8点半上班，5点半准时下班，异常高产发论文、出书、做演讲，和做自己的本职工作（大学副教授）。我并不是很推荐他的书，他的blog也写得零零碎碎的，但主旨意思很inspire我。他让我从2012年第开始反思，我原来自认为高效率的工作原来抓错重点了。&lt;/p&gt;
&lt;p&gt;话讲回来，我的时间更零碎，所以我要保证的就是工作时间只花在有意义的工作上。那些对我没帮助的工作就尽量推或最少时间内完成就好。这需要以下几点：&lt;/p&gt;
&lt;p&gt;1）你的工作允许你这么做。找工作的时候要睁大眼睛，不能光看钱，要看这份工作会不会给你pack on有用的经验。&lt;/p&gt;
&lt;p&gt;2）你的老板支持你。一样要在招工的时候看好。我有一份工作，老板很不给力，但老板的老板很make sense，所以这样也行。&lt;/p&gt;
&lt;p&gt;我的原则就是自己给自己保证每天工作时间不超过9小时。这样给了自己limit，就不敢浪费一分钟。我同时也告诉我的老板、下属和家人，我几点之后不会工作，只有紧急email才会处理。这样大家都有一致，家人也喜欢我的安排。&lt;/p&gt;
&lt;p&gt;技巧方面Cal Newport在他的书里有很多阐述，对我来说就是处理对自己重要的事，而不是紧急的事。每天这样做，你会发现你照自己的agenda走（你实现你心目中工作的蓝图），而不是被别人的agenda驱赶（你成了workhorse)&lt;/p&gt;
&lt;h3&gt;二，公司晋升文化&lt;/h3&gt;
&lt;p&gt;在开始为自己谋划下一份好工作的时候，千万不要轻举妄动，先观察了解公司的晋升文化：&lt;/p&gt;
&lt;p&gt;1）看你敬仰的公司内的好领导人，他们是怎么晋升的：他们有没有跨职能调动，年龄层如何，晋升的速度如何，谁有决定权。部门和部门之间有没有差别。&lt;/p&gt;
&lt;p&gt;2）看你公司最近晋升的人。他们有什么口碑，晋升的依据是什么？&lt;/p&gt;
&lt;p&gt;3）经过前面两步你应该对在本公司内晋升有个大概想法了，下面就是根据你的理解，猜测下面能晋升的是谁。当你猜测八九不离十了，你就对公司晋升文化有了把握了。&lt;/p&gt;
&lt;p&gt;有把握不代表你会赢。打个比方，如果你公司晋升需要的就仅仅是对大老板投其所好（就是打个简单极端的比方），但我本人知道自己和大老板的喜好（比如足球）没有半毛钱关系，那我就肯定out了。&lt;/p&gt;
&lt;p&gt;所以有把握是第一步，衡量自己的赢面是第二步。赢面多就努力，赢面少就换公司，这就是我朴素的想法。反正薪水差不多的工作很多，能支持你进步的公司撞对了就好，没有的话就是拖延你晋升时间。&lt;/p&gt;
&lt;p&gt;我自己的例子, MBA之后：&lt;/p&gt;
&lt;p&gt;第一份工作，成绩好，一年多manager升senior manager，但再晋升的criteria及其苛刻，就是你要get in the right circle, old boy club，并不是用成绩说话。那不是我的强处。继续逗留了3年一无所获。&lt;/p&gt;
&lt;p&gt;第二份工作，换了公司，给了associate director title，进去后才发现，晋升文化和第一家公司一模一样。后悔都来不及。stuck了一年多，突然听说美国部有完全不一样的culture（我当时在全球总部），赶紧在美国部找工作，就是我的第三份工作。&lt;/p&gt;
&lt;p&gt;第三份工作，平级调动。老板不咋样，老板的老板是公司红人，性格和我合，对未来远景和我合，自己是快速提升出来的，对自己人忠诚。这还不够，我通过面试又认识了一个senior VP，也是非常投缘。我就脸皮厚厚的和他们说，在美国部人生地部熟，transfer过来唯一的要求就是需要他们两位做我的career mentor。他们两个一口答应，说一个季度保证见一次面，还说在美国部晋升就是90%看成绩，我如果能作出别人作不出的成绩，就能升director。我当时手上还有本公司中国区的offer，一样level，薪水比美国多40%，但工作性质（pack on useful experience)和前景比不上美国。权衡再三，到了美国总部。他们知道也很appreciate我的权衡，找了另外一个vp给我打电话做说客。我答应offer的那天三个人早上7点给我congrat email。这份工作难做，但是是我擅长的，几个月内就出效益，而且效益惊人，一年半后升了director，带3个人，直接report给我原来老板的老板。&lt;/p&gt;
&lt;p&gt;第四份工作，升了director之后半年，我的那个svp mentor要我去他的部门，平级调动，scope做回全球，带6个人。我的老板无法拒绝。所以我没面试就换工作了。&lt;/p&gt;
&lt;p&gt;第五份工作，调动后半年，我的svp升职了，下面的组跟着调动。我的scope也相应增加，带更多人。这次也是口头通知，没面试就换工作。&lt;/p&gt;
&lt;p&gt;我之所以留在现在的工作，就是因为现在的晋升文化对我有益：&lt;/p&gt;
&lt;p&gt;1）90%看出色成绩，这个我在行&lt;/p&gt;
&lt;p&gt;2）10%看大家对我的印象，这个我在下面第五板块会阐述&lt;/p&gt;
&lt;p&gt;3）领导人都重视内部提拔，都对自己团队的人忠诚。也就是说我对他们loyal，他们就会照顾我。我们公司提升快的人都是没有任何面试的。
4）我对晋升文化把握很好，基本所有我猜测会升职的人都升了。所以说我对自己规划的formula 也会有效。&lt;/p&gt;
&lt;p&gt;这个章节希望大家多观察，多分析，把自己身处positive的环境，对那些鸡肋环境不要太留恋。&lt;/p&gt;
&lt;h3&gt;三 工作中create WOW factor; Positioning: rare and valuable skill&lt;/h3&gt;
&lt;p&gt;这里面有两点：&lt;/p&gt;
&lt;p&gt;1）你的工作强，so good people cannot ignore you&lt;/p&gt;
&lt;p&gt;2）你的技能不仅有用而且稀有&lt;/p&gt;
&lt;p&gt;这一个板块是最重要的！！！三个惊叹号表示其重要性。我看到的无数聪明人在晋升途中是在这儿倒下的。&lt;/p&gt;
&lt;p&gt;一个一个来说。&lt;/p&gt;
&lt;p&gt;1）工作强到people cannot ignore you。我在商，用营业额和效率说话。我升director就是一份创新在两个月内实施，实施后10个月内创造1700万美元营业额。那是2014年。我们来看看原因是什么。&lt;/p&gt;
&lt;p&gt;创新的角度。唯一能在竞争中出头就是创新。创新不是天分，也是技能（象练肌肉一样能学的），请阅读一下管理学上关于little bets的想法。大体就是每个礼拜坚持阅读、分析、总结跨领域学识，来攻破你的行业中的核心问题。我就是借鉴consumer marketing里的crm把它用到b2b上。有想法就用90天尝试，不行就马上move on，积累一定error rate才会赢。我从2013年7月到12月，失败了很多想法，然后突然就有一个stick了。&lt;/p&gt;
&lt;p&gt;我有专一的技能，而且磨练已久。在这之前我有很多年从业经验，而且都是实实在在积累的。大家看一下10000小时定律，有道理的。&lt;/p&gt;
&lt;p&gt;2）你的技能不仅有用，而且稀有&lt;/p&gt;
&lt;p&gt;如果象上面所说，一直坚持积累和创新，这点是顺理成章的。如果你和别人的技能有重合，千万要重新position。要让别人一想到某个重要点就想起你。（比如在我们组，想到“创新”，“营业额增加”，经常想到的就是我带的组）&lt;/p&gt;
&lt;p&gt;我说我看到无数人倒下，因为没有1和2。上班就是完成被布置的任务，而没有自己追逐的远景和蓝图。&lt;/p&gt;
&lt;p&gt;那我来说说我觉得应该如何在工作中position yourself。后面有mm说做2x2 grid，那个也好，但太tactic了，我要你做得更strategic&lt;/p&gt;
&lt;p&gt;看清楚自己的工作，不管你是什么level，工作中有多少自主权，这些别管，分析清楚为什么公司要你这个位置。&lt;/p&gt;
&lt;p&gt;我的位置：“不管怎样把竞争产品打败，让客户需要此类产品的时候首先想到我们产品”&lt;/p&gt;
&lt;p&gt;我要想清楚的是：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;竞争者会怎么卖
我要怎么营销才会比他们强
怎么让我想出的办法竞争者用不了——哪个角度他们没有也不可能想出办法
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;好，那看来制胜关键点就是这个创新角度。那我如何规划到我的工作中呢？&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;每天上班前半小时快速阅读xxx几个跨领域角度
每周给多少vendor打电话，争取合作机会
每周给一个solid idea给老板，ask for opinion 和尝试机会
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;慢慢这些idea多了，我就有了自己的远景。这些都不是任何人给我安排的工作，但对我来说我重要，时间上有绝对优先权。老板看到的我给他的每周的东西（作完他布置的，还会每周有具体项目proposal)，而其他同事没有这些，印象分就不错。更何况我的点子抓准了，有效益出来，我和别人的差距马上就拉开了。&lt;/p&gt;
&lt;p&gt;请大家务必这章节努力钻研一下，也可以阅读下career capital有关的理论。这才是你要升职的content。&lt;/p&gt;
&lt;h3&gt;四 Build your team&lt;/h3&gt;
&lt;p&gt;当别人已经看到了你不能被ignore的能力了，当你的positioning已经深入人心了，下面该做什么，该如何突破？&lt;/p&gt;
&lt;p&gt;在传统企业，继续望下走(executive director)，就是带越来越大的team。如何才能让上级愿意给我越来越多的人和规模？&lt;/p&gt;
&lt;p&gt;你的团队的复杂程度和你的能力成正比&lt;/p&gt;
&lt;p&gt;我只在美国商业组带3个人，大部分是product manager的时候大家都觉得还好&lt;/p&gt;
&lt;p&gt;转到全球商业组带6个人，都是associate director，而且在不同国家的时候，老板就会帮我敲敲警钟&lt;/p&gt;
&lt;p&gt;现在除了商业组，要带其他组了，我的svp跑过来说，我对你唯一的要求就是build a strong team&lt;/p&gt;
&lt;p&gt;build team一样要carefully strategize and position&lt;/p&gt;
&lt;p&gt;你要大家对你的团队什么印象&lt;/p&gt;
&lt;p&gt;你的团队需要做什么，需要是什么样的人&lt;/p&gt;
&lt;p&gt;我的例子，我对自己团队，因为长大了，定位从commercial innovation 转到 corporate innovation。我需要大家想到我们，就想到“创新”，“实打实的效益增长”，“公司活力”，“脚踏实地”，“不假大空”，“是公司高层的战略伙伴”。所以我要的人必须smart, organized, team player, on the edge, well informed，可以 young, 可以 green，但必须hard working and stay hungry。有潜力的多引导多培养，所有人都独当一面（都和自己负责的vp直接打交道），不合适的转给需要的组。换句话说，我可以要一个“虎狼之师”，我需要他们尽心尽力为我工作两年，然后either我把他们平级转到其他好的组增加经验，或者留下来等升职机会。但我需要他们两年的时间很出色，很独立。&lt;/p&gt;
&lt;p&gt;换另外一个组的话，如果稳定性是最重要的，就不能按一样的条件选人用人了。&lt;/p&gt;
&lt;p&gt;build team还要team有团队意识。&lt;/p&gt;
&lt;p&gt;如果一个人搞砸了，对公司来说他们是对整个组产生不好的印象&lt;/p&gt;
&lt;p&gt;反之，如果整个组的口碑不好，组里的人以后的placement会有很大问题&lt;/p&gt;
&lt;p&gt;所以，一荣俱荣，我就是要不停灌输这一点（我也这么相信）&lt;/p&gt;
&lt;p&gt;这个章节是我的薄弱之处，没有太多心得体会，希望有经验的姐妹多多指正&lt;/p&gt;
&lt;h3&gt;五 Key stakeholders: U movie&lt;/h3&gt;
&lt;p&gt;好了，说完前面四章，你工作也出色了，带的team也不错，是不是机会就向你招手呢？&lt;/p&gt;
&lt;p&gt;当然不是！用confuse mm的话讲，“老黄牛”实干的人多了去了，为什么找你？&lt;/p&gt;
&lt;p&gt;我举个例子：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;你要的下一个位子，甚至下两个位子是什么？
谁是这个位子的决定者？又有谁是参与讨论的人？
你知道为了升level，minimum的performance rating是多少？谁来决定，谁来参与？
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这些都是你要影响的人，都是你要create u movie 的人&lt;/p&gt;
&lt;p&gt;我们一个一个来讲&lt;/p&gt;
&lt;p&gt;1）我在这个公司目前要的下一个（或下两个）位子是corporate innovation officer。这个位子现在没有的，我希望ceo看到这个必要，能设个executive director level的。我和我的SVP讲了，和人事VP讲了。大家都觉得决定者是ceo，参与讨论的人就是ceo的leadership team。这是一组人A。&lt;/p&gt;
&lt;p&gt;2）为了升level，performance rating 要好看，在这个公司要到8（很难拿）。这个rating是我老板打分，他的老板(VP)和同事同意，这是另外一组人B。&lt;/p&gt;
&lt;p&gt;B很简单，他们得知道我的工作如何和他们自己的团队配合，in other words, how do I help them look good。&lt;/p&gt;
&lt;p&gt;A复杂得多。A里面我要做到几点：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;多少人能够复述我的主要成绩(talking points)
多少人同意我要的位子对公司是有利的
多少人愿意在ceo team面前advocate我的能力和品质
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;好，A里面的人都是SVP level，我有些只认识脸，有些对我不错，有些根本不知道我是谁，想做什么。要manage这些stakeholder必须用非常规渠道接近（日常工作之外的special projects），而我的SVP和hr vp能帮我的就是建立这些special projects。但不能多，每年改变两个stakeholders。&lt;/p&gt;
&lt;p&gt;这还没完，A组的人手下的，有些重要的vp和market president我工作中经常打交道的，我还要编第三张网（c )，让这些人能够在a面前提醒、复述以上一些要点。&lt;/p&gt;
&lt;p&gt;这abc三组人才是我完整的stakeholder lists。我需要精心的manage，才能在有我感兴趣的职位open前，大家会把我当作valid人选考虑。&lt;/p&gt;
&lt;p&gt;这里面还要学会看那些是up and comer，尽管位子还没有很高，但长期很管用，这一点只要第二板块练习好，很容易识别。
最后，什么是U movie?&lt;/p&gt;
&lt;p&gt;U movie就是关于你的movie！（这儿我知道鸡蛋已经砸过来了）我没有开玩笑。其实到最后，谁都记不了那么多关于你的东西。（每个人真是太微小了）除了你自己，别人关于你的记忆都是碎片。你什么时候说了句什么话，哪一次没handle好一个什么局面，甚至你平时出现的形象是什么，这些才是最a终别人保存对你的记忆。&lt;/p&gt;
&lt;p&gt;你要保证的是你的stakholder对你有正确的碎片。那怎么办？consistent image。穿的，动作，神态，语言，consistency is key。我记得我mba后第一份工作老板的面试问题：什么是quality。我说quality就是投入产出做到比配最好。他说恰恰相反，quality是你过一条高速，每次都是30分钟才是quality，如果一次50分钟一次10分钟，别人就会产生那条路很不可靠的印象。对我印象深刻的回答。&lt;/p&gt;
&lt;h3&gt;六 Your style&lt;/h3&gt;
&lt;p&gt;讲到这儿，我不希望自己给大家一个印象就是攻于心计。不是这样的。有些人认为作到一定level就要有特定样子：比如圆滑啦，虚伪啦，不说实话啊什么的。&lt;/p&gt;
&lt;p&gt;我认为恰恰相反，一个领导人在选一个接班的，或者一个初入职的在选一个可靠的领导去跟随的，首先大家考虑的是这个人是什么样的人。真实才让人觉得可信。&lt;/p&gt;
&lt;p&gt;真实也是最好的武器。因为这样我永远不会fail我的形象。&lt;/p&gt;
&lt;p&gt;而且，每个人显示他的弱点时才有亲和力。&lt;/p&gt;
&lt;p&gt;我喜欢的领导里面：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;有讲话超快的
有及其单刀直入的
有爱讲笑话的
有有时候silly的
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;只要在合适的地方显示出来，不会扣分反而会加分。&lt;/p&gt;
&lt;p&gt;请做象自己，只要我们的干劲是真实的，成绩是真实的，被人误解的时候坦率平静的解释，和团队在一起的时候露点可爱本色，一切都应该是真诚有意思的。如果我必须伪装来达到某个目的，那这个目的不值得去达到。&lt;/p&gt;
&lt;p&gt;昨天睡得早，今天早上一看，深受感动，大家谬赞了。华人上有很多很多厉害的姐妹。我只不过是现在有时间（在家里坐月子）把这些零散的感受写出来而已。什么时候大家都来分享了，各行各业，那才能看到我们华人真正的本领呢。&lt;/p&gt;
&lt;p&gt;收尾&lt;/p&gt;
&lt;p&gt;职业发展本来就是各式各样的路。我说的适合愿意做点成绩的人。好处是你的简历会有硬货，只要你的skill stay current，我就不愁公司内部外部没有机会（现在有好几个和我打交道的vendor邀请我加入他们的公司，而且是高层，因为他们看到了我做的东西卖给行业内其他企业的商机）。坏处是一开始需要投入智力体力。我选择这条路本来就是因为我enjoy deep thinking，而且更愿意做个有实货的生意人，但是因为现阶段家庭关系，不能投入太多时间，所以我就帮自己create了这个formula。&lt;/p&gt;
&lt;p&gt;每个人在选择时都要清楚认识自己，扬长避短，才能锋利。如果你权利手腕高超，我说的这条路不适合你。如果你愿意投入无数时间，这条路对你就太过conservative。
我见过&lt;/p&gt;
&lt;p&gt;白天上班，下班后打篮球吃饭，晚饭后去地下室创业到早上，直接再去上班的人。人家现在是的公司是一块niche里的领头羊了。&lt;/p&gt;
&lt;p&gt;或者，能说会道，1-2年就跳槽，越跳越好，都是高层title，可是和他谈你清楚的知道他不懂自己的行业。但这不影响他出入各种conference，做top speaker。在业界算得上一块牌子。&lt;/p&gt;
&lt;p&gt;只要对自己合适打造，都能成为让人佩服的人。&lt;/p&gt;
&lt;p&gt;所以说认识自己才是做任何事前的第一步&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;什么是你的长处
什么是你要追求的
为了这个追求你愿意付出多少，你愿意trade off什么
你无论如何不愿意trade off的又是什么
什么样的人/环境会喜欢/接受你（什么样的人不会）
你心态如何（没有达成目标，受到挫折你会如何？你会panic攻击他人吗？你会心灰意懒惩罚自己吗？你能保持荣辱不惊步调一致走下去吗？）
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;没有这些认识的话，那做事有点象赌运气了。也没什么不好，就是不同的途径而已。&lt;/p&gt;
&lt;p&gt;那么，有人问，怎样才能认识自己呢？好问题，我不知道答案。我只知道我如何从一个心浮气燥的小姑娘走到今天的。&lt;/p&gt;
&lt;p&gt;一个，我年龄摆在这儿了，40不惑嘛&lt;/p&gt;
&lt;p&gt;第二个，人生中的逆境完全改变了我。老天肯定是看不下去以前的我，让我在20多岁的时候学到了很多人经历不到的。当然，不是象小说里，什么what didn't kill you make you stronger。我的trauma恢复非常漫长，到现在还在。可是那段经历确实改变了我的perspective，知道自己的力量，也知道什么是对自己重要的，什么是我压根不care的。心态摆正了，你自然就知道自己是谁。&lt;/p&gt;
&lt;p&gt;最后一个，也是最简单的，你问自己，骨子你相信你会做成什么，做不成什么。不需要任何证据，相信你的感觉。我turn down中国的多40% package的时候，有朋友劝我，万一美国这个不work怎么办，因为job description写的太空，太risky。我告诉他，我就是觉得这是我的机会，我肯定能做个什么出来，这是在我还没有任何实际想法之前。旧象现在我就知道我现在的路走下去再加一个subtle spin off，三五年内我会有个大的格局出来，至于是什么格局我也说不仔细。&lt;/p&gt;
&lt;p&gt;最后祝大家永远幸福。究根揭底，工作是不重要的，超出生活所需的金钱是不重要的，名誉这个事I couldn't care less。重要的是既随心所欲（让自己爽）又没有不满足感（不会觉得自己永远在追求一个未来的东西）。当下已经是完美的了。风和日丽，还有什么他求呢？&lt;/p&gt;
&lt;p&gt;&lt;a href="https://forums.huaren.us/showtopic.html?topicid=1982242"&gt;转载 （完结了）也随便讲讲职业发展&lt;/a&gt;&lt;/p&gt;</content><category term="career growth"></category><category term="career growth"></category></entry><entry><title>tmux quick notes</title><link href="/pages/2019/08/18/tmux-quick-notes/" rel="alternate"></link><published>2019-08-18T00:00:00-05:00</published><updated>2019-08-18T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-08-18:/pages/2019/08/18/tmux-quick-notes/</id><summary type="html">&lt;h3&gt;极简命令&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;建立session
   &lt;code&gt;tmux new -s my_session_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;建立新窗口
   &lt;code&gt;prefix + c     # c -&amp;gt; create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;list所有窗口，然后选择某个窗口
    &lt;code&gt;prefix + w     # W -&amp;gt; window，然后0，1，2，3，4选择需要的窗口；&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;窗口命名
   &lt;code&gt;prefix + ,     # 逗号&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;建立竖 …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h3&gt;极简命令&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;建立session
   &lt;code&gt;tmux new -s my_session_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;建立新窗口
   &lt;code&gt;prefix + c     # c -&amp;gt; create&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;list所有窗口，然后选择某个窗口
    &lt;code&gt;prefix + w     # W -&amp;gt; window，然后0，1，2，3，4选择需要的窗口；&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;窗口命名
   &lt;code&gt;prefix + ,     # 逗号&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;建立竖直切分，生成左右两个屏幕
   &lt;code&gt;Prefix %&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;建立水平切分，生成上下两个屏幕
    &lt;code&gt;Prefix “&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;修改配置&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;Tmux&lt;/code&gt;默认不支持鼠标滚动查看窗口中的前后内容。需要修改配置来支持。   &lt;code&gt;vim ~/.tmux.conf&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;复制代码# 开启鼠标模式
set-option -g mouse on

# 允许鼠标选择窗格
# set -g mouse-select-pane on
​
# 如果喜欢给窗口自定义命名，那么需要关闭窗口的自动命名
set-option -g allow-rename off
​
# 如果对 vim 比较熟悉，可以将 copy mode 的快捷键换成 vi 模式
set-window-option -g mode-keys vi

# 默认快捷键是 `Ctrl + b` (`C-b`), 记为 prefix. 自定义快捷键 prefix 为 cmd + a
# unbind ^b
# set -g prefix 'M-a'
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;tmux的主要元素分为三层：&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Session&lt;/strong&gt; 一组窗口的集合，通常用来概括同一个任务。session可以有自己的名字便于任务之间的切换。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Window&lt;/strong&gt; 单个可见窗口。Windows有自己的编号，也可以认为和ITerm2中的Tab类似。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pane&lt;/strong&gt; 窗格，被划分成小块的窗口，类似于Vim中 C-w +v 后的效果。一图以蔽之：&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="tmux 说明" src="https://harttle.land/assets/img/blog/tmux-concept.png"&gt;&lt;/p&gt;
&lt;h4&gt;Session&lt;/h4&gt;
&lt;p&gt;Tmux为了防止与全局快捷键冲突，大部分快捷键需要先需要输入前缀Ctrl + b，下文用Prefix代替。
Session主要相关命令如下：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;复制代码# 创建
tmux new # 不指定session name
tmux new -s [session-name]

# 删除Session
tmux kill-session -t [session-name]
tmux kill-server

# 列出当前Session
tmux ls # 
Prefix s # tmux 内

# 恢复Session
tmux a -t [session-name]
tmux a

# 断开Session
tmux detach
Prefix d

# 重命名Session
Prefix $
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Window&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;
# 创建
Prefix c

# 选择窗口
Prefix + [number] # 选择第n个窗口
Prefix + p/n    # 前/后一个窗口

# 关闭窗口
Prefix &amp;amp;
exit

# 列出所有window（包含其他Session）
Prefix w 
j/k # 前后选择

# 搜索窗口
Prefix f

# 重命名当前窗口
Prefix ,
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;Pane&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;
# 创建
Prefix %    # 水平窗格
Prefix '&amp;quot;'  # 垂直窗格

# 关闭
Prefix x

# 切换
Prefix o # 在窗格间切换
Prefix q    # 显示窗格编号，输入编号切换

# 将当前窗格切换到新窗口
Prefix ！

# 窗格交换位置
Prefix + {/}
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-07-27 Week 30 工具合集</title><link href="/pages/2019/07/27/2019-07-27-week-30-gong-ju-he-ji/" rel="alternate"></link><published>2019-07-27T00:00:00-05:00</published><updated>2019-07-27T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-07-27:/pages/2019/07/27/2019-07-27-week-30-gong-ju-he-ji/</id><content type="html">&lt;ol&gt;
&lt;li&gt;loop in bash; &lt;a href="https://unix.stackexchange.com/questions/10646/repeat-a-unix-command-every-x-seconds-forever"&gt;Repeat a Unix command every x seconds forever&lt;/a&gt;;  &lt;a href="https://askubuntu.com/questions/385528/how-to-increment-a-variable-in-bash"&gt;How to increment a variable in bash?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;var=0; 
while true; 
do
    echo &amp;quot;Hi&amp;quot;;
    var=$((var + 1));
    echo $var;
    sleep 1;
done
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-07-20 Week 29, nvidia-smi error</title><link href="/pages/2019/07/20/2019-07-20-week-29-nvidia-smi-error/" rel="alternate"></link><published>2019-07-20T00:00:00-05:00</published><updated>2019-07-20T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-07-20:/pages/2019/07/20/2019-07-20-week-29-nvidia-smi-error/</id><summary type="html">&lt;h1&gt;nvidia-smi error&lt;/h1&gt;
&lt;p&gt;The error is
    (tensorflow_p36) ubuntu@ip-172-31-36-1:~$ nvidia-smi
    NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; This problem can be resolved by installing the (currently) latest version of the Nvidia driver. &lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;create an install file …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;nvidia-smi error&lt;/h1&gt;
&lt;p&gt;The error is
    (tensorflow_p36) ubuntu@ip-172-31-36-1:~$ nvidia-smi
    NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt; This problem can be resolved by installing the (currently) latest version of the Nvidia driver. &lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;create an install file &lt;code&gt;vim install.sh&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;#!/bin/bash

set -x

version=$1
#version=410.79
#version=410.104

wget http://us.download.nvidia.com/tesla/${version}/NVIDIA-Linux-x86_64-${version}.run 
sudo sh ./NVIDIA-Linux-x86_64-${version}.run --no-drm --disable-nouveau --dkms --silent --install-libglvnd 

&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sudo ./install.sh 410.104&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;modinfo nvidia | head -7&lt;/code&gt;
    filename: /lib/modules/4.4.0-1077-aws/updates/dkms/nvidia.ko
    alias: char-major-195-&lt;em&gt;
    version: 410.104
    supported: external
    license: NVIDIA
    srcversion: 3B812B02678A6B43A294F17
    alias: pci:v000010DEd00000E00sv&lt;/em&gt;sd&lt;em&gt;bc04sc80i00&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;sudo modprobe nvidia&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://forums.aws.amazon.com/thread.jspa?messageID=894021"&gt;NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA drive&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-06-22 Week 25 工具合集</title><link href="/pages/2019/06/22/2019-06-22-week-25-gong-ju-he-ji/" rel="alternate"></link><published>2019-06-22T00:00:00-05:00</published><updated>2019-06-22T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-06-22:/pages/2019/06/22/2019-06-22-week-25-gong-ju-he-ji/</id><summary type="html">&lt;ol&gt;
&lt;li&gt;&lt;a href="https://harttle.land/2015/11/06/tmux-startup.html"&gt;优雅地使用命令行：Tmux 终端复用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://juejin.im/post/5a8917336fb9a0633e51ddb9"&gt;Tmux入门教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/vim-video"&gt;玩转 vim 与 Terminal (视频)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=5r6yzFEXajQ"&gt;vim + tmux - OMG!Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=BHhA_ZKjyxo"&gt;Basic tmux Tutorial - Windows, Panes, and Sessions over SSH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=AVfvhEJ9XD0"&gt;Raspberry Pi 4 Cooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=8grooZWbH9Y"&gt;Getting Started With The Raspberry Pi 4 - Use It As A Linux …&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;ol&gt;
&lt;li&gt;&lt;a href="https://harttle.land/2015/11/06/tmux-startup.html"&gt;优雅地使用命令行：Tmux 终端复用&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://juejin.im/post/5a8917336fb9a0633e51ddb9"&gt;Tmux入门教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/vim-video"&gt;玩转 vim 与 Terminal (视频)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=5r6yzFEXajQ"&gt;vim + tmux - OMG!Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=BHhA_ZKjyxo"&gt;Basic tmux Tutorial - Windows, Panes, and Sessions over SSH&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=AVfvhEJ9XD0"&gt;Raspberry Pi 4 Cooling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=8grooZWbH9Y"&gt;Getting Started With The Raspberry Pi 4 - Use It As A Linux PC&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://askubuntu.com/questions/23009/why-crontab-scripts-are-not-working"&gt;Why crontab scripts are not working?: crontab tricks&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;restart cron in redhat: &lt;code&gt;sudo /etc/init.d/crond restart&lt;/code&gt; (&lt;code&gt;sudo service crond restart&lt;/code&gt; does not work)&lt;/li&gt;
&lt;li&gt;add &lt;code&gt;#!/bin/bash&lt;/code&gt; at the top to make sure run in bash&lt;/li&gt;
&lt;li&gt;PATH&lt;/li&gt;
&lt;li&gt;crontab file should end with an empty line&lt;/li&gt;
&lt;li&gt;change file to executable as &lt;code&gt;755&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To test if an cron job works or not, put the following code in the &lt;code&gt;.sh&lt;/code&gt; cron file and then wait for the execution result:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt; * * * * * env &amp;gt; /home/songjack/cron/env.from.cron.output
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-06-15 Week 24</title><link href="/pages/2019/06/15/2019-06-15-week-24/" rel="alternate"></link><published>2019-06-15T00:00:00-05:00</published><updated>2019-06-15T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-06-15:/pages/2019/06/15/2019-06-15-week-24/</id><content type="html"></content><category term="python"></category><category term="python"></category></entry><entry><title>2019-06-08 Week 23 Regular Expression to clean data</title><link href="/pages/2019/06/08/2019-06-08-week-23-regular-expression-to-clean-data/" rel="alternate"></link><published>2019-06-08T00:00:00-05:00</published><updated>2019-06-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-06-08:/pages/2019/06/08/2019-06-08-week-23-regular-expression-to-clean-data/</id><summary type="html">&lt;h1&gt;regular expression example&lt;/h1&gt;
&lt;p&gt;We have many sql alike code to patch the system. The purpose is to clean the code to do two things: 1) to find what variables are used; 2) to find the value used in the code. That is, transfer to key-value form so that it can …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;regular expression example&lt;/h1&gt;
&lt;p&gt;We have many sql alike code to patch the system. The purpose is to clean the code to do two things: 1) to find what variables are used; 2) to find the value used in the code. That is, transfer to key-value form so that it can be easily analyzed in python dataFrame.&lt;/p&gt;
&lt;p&gt;Some examples are:&lt;/p&gt;
&lt;p&gt;The input string is s = 'segment == 5 &amp;amp;&amp;amp; age &amp;gt; 30', the output will be (segment, == 5) and (age, &amp;gt; 30)&lt;/p&gt;
&lt;p&gt;The input string is s = 'imatches("^(US|)&lt;span class="math"&gt;\(", ctry_cd) &amp;amp;&amp;amp; (age &amp;gt;= 10)', the output will be (ctry_cd, ^(US|)\)&lt;/span&gt;) and (age, &amp;gt;= 10)&lt;/p&gt;
&lt;p&gt;Let's start from an example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cond = ['$seg == 5 ', ' imatches(&amp;quot;^228$&amp;quot;,$gls) ', ' $ipage &amp;gt; 30 ', ' $fpage &amp;lt; 2 ', ' $dayob &amp;gt; 200 ', 
        ' $ccage &amp;gt; 30 ', ' $val &amp;gt; 99 ', ' $day7GLRepeatCount &amp;gt; 0 ', ' $opvCustomerGCOrderCount90Days == 0 ', 
        ' imatches(&amp;quot;US&amp;quot;,$ictry_cd) ', ' imatches(&amp;quot;US&amp;quot;,$bctry_cd) ', ' imatches(&amp;quot;^(US|)$&amp;quot;,$cctry_cd) ', ' $sqPercentile &amp;gt; 0.7']

def replaceMultiple(s, rep = {'(':'', ')':''}):
    '''
    re multile pattern in s
    '''
    rep = dict((re.escape(k), v) for k, v in rep.items())
    pattern = re.compile(&amp;quot;|&amp;quot;.join(rep.keys()))
    #print(pattern)
    return pattern.sub(lambda m: rep[re.escape(m.group(0))], s)

def getVarFromCondition2(string):
    '''
    return the variables and its value in the rule
    # rep1 -- lookforward anything followed by pattern ==,&amp;lt;=,&amp;gt;=,&amp;lt;,&amp;gt; but pattern not included in output, 也就是pattern ==,&amp;lt;=,&amp;gt;=,&amp;lt;,&amp;gt; 的左边
    # rep2 -- lookbehind anything before pattern ==,&amp;lt;=,&amp;gt;=,&amp;lt;,&amp;gt; but patten not included in output, 也就是 pattern ==,&amp;lt;=,&amp;gt;=,&amp;lt;,&amp;gt; 的右边
    '''
    res = []
    rep1 = r'.*(?===)|.*(?=&amp;lt;=)|.*(?=&amp;gt;=)|.*(?=&amp;lt;)|.*(?=&amp;gt;)' 
    rep2 = r'(?&amp;lt;===).*|(?&amp;lt;=&amp;lt;=).*|(?&amp;lt;=&amp;gt;=).*|(?&amp;lt;=&amp;lt;).*|(?&amp;lt;=&amp;gt;).*'  
    rep3 = r'==.+ | &amp;lt;.+ | &amp;gt;.* | &amp;lt;=.+ | &amp;gt;=.+'
    #rep1 = r'[^ -][^==]*$'
    for s in string:
        # remove blanks
        s = &amp;quot;&amp;quot;.join(s.split())
        print(s)
        if 'match' in s.lower():
            res.append(replaceMultiple(re.findall(r'\(.+\)', s)[0]).split(',')[::-1])
        else:
            res.append((re.findall(rep1, s)[0], re.findall(rep2, s)[0]))
    return res

getVarFromCondition2(cond)


    Out[38]:
    [('$seg', '5'),
     ['$gls', '&amp;quot;^228$&amp;quot;'],
     ('$ipage', '30'),
     ('$fpage', '2'),
     ('$dayob', '200'),
     ('$ccage', '30'),
     ('$val', '99'),
     ('$day7GLRepeatCount', '0'),
     ('$opvCustomerGCOrderCount90Days', '0'),
     ['$ictry_cd', '&amp;quot;US&amp;quot;'],
     ['$bctry_cd', '&amp;quot;US&amp;quot;'],
     ['$cctry_cd', '&amp;quot;^US|$&amp;quot;'],
     ('$sqPercentile', '0.7')]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/6116978/how-to-replace-multiple-substrings-of-a-string"&gt;How to replace multiple substrings of a string?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/37190978/regex-get-text-before-and-after-a-hyphen"&gt;Regex get text before and after a hyphen&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/3739909/how-to-strip-all-whitespace-from-string"&gt;How to strip all whitespace from string&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;https://www.regular-expressions.info/lookaround.html&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-06-01 Week 22 Linux bash date time</title><link href="/pages/2019/06/01/2019-06-01-week-22-linux-bash-date-time/" rel="alternate"></link><published>2019-06-01T00:00:00-05:00</published><updated>2019-06-01T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-06-01:/pages/2019/06/01/2019-06-01-week-22-linux-bash-date-time/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Linux bash date time&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;export day=2019-06-02
#day=$(date +%Y-%m-%d)
sunday_1=$(date -d &amp;quot;$day -$(date -d $day +%w) days&amp;quot; +%Y-%m-%d)  # last sunday include current date
sunday_2=$(date -d &amp;quot;$day -$(date -d $day +%u) days&amp;quot; +%Y-%m-%d)  # last sunday not include current date …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;Linux bash date time&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;export day=2019-06-02
#day=$(date +%Y-%m-%d)
sunday_1=$(date -d &amp;quot;$day -$(date -d $day +%w) days&amp;quot; +%Y-%m-%d)  # last sunday include current date
sunday_2=$(date -d &amp;quot;$day -$(date -d $day +%u) days&amp;quot; +%Y-%m-%d)  # last sunday not include current date

echo $sunday_1      #2019-06-02    
echo $sunday_2      #2019-05-26

last_sat=$(date -d &amp;quot;last saturday&amp;quot; +%Y-%m-%d)       # Last Saturday
echo &amp;quot;last saturday is &amp;quot; $last_sat                  # last saturday is  2019-06-01
echo $(date -d &amp;quot;$last_sat +1 day&amp;quot; +%Y-%m-%d)        # 2019-06-02

# The Sunday before last Saturday
sun_before_last_sat=$(date -d &amp;quot;$last_sat -$(date -d $last_sat +%w) days&amp;quot; +%Y-%m-%d)
echo $sun_before_last_sat       # 2019-05-26

echo '-----------------------'

# Loop through from the Sunday before last Saturday to last Saturday
d=$sun_before_last_sat
while [ $d != $(date -d &amp;quot;$last_sat +1 day&amp;quot; +%Y-%m-%d) ];do
 echo $d
 d=$(date -I -d &amp;quot;$d + 1 day&amp;quot;)
done

2019-05-26
2019-05-27
2019-05-28
2019-05-29
2019-05-30
2019-05-31
2019-06-01

&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/28226339/1876887"&gt;Bash: Looping through dates&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cyberciti.biz/tips/linux-unix-get-yesterdays-tomorrows-date.html"&gt;Getting Yesterdays or Tomorrows Day With Bash Shell Date Command&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://unix.stackexchange.com/a/49055/319361"&gt;linux + add X days to date and get new virtual date&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-05-25 Week 21</title><link href="/pages/2019/05/25/2019-05-25-week-21/" rel="alternate"></link><published>2019-05-25T00:00:00-05:00</published><updated>2019-05-25T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-05-25:/pages/2019/05/25/2019-05-25-week-21/</id><summary type="html">&lt;p&gt;python regular expression to clean the RMP data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import re

strtest = &amp;quot;&amp;quot;&amp;quot;  3602433631519&amp;quot; /&amp;gt;                                &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt; 7 &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;&amp;lt; = &amp;quot;&amp;gt; &amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;HRB_HighClaim_Sideline&amp;lt;/td&amp;gt;

                &amp;lt;!-- Align Rule condition to variable expression --&amp;gt;
                    &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;

                &amp;lt;td&amp;gt;MVEL&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;
                        &amp;lt;pre class=&amp;quot;code&amp;quot;&amp;gt;get(&amp;amp;quot;$BFS.hrb_claims_by_customer_us.n_claim_count&amp;amp;quot;)!\
=empty &amp;amp;amp;&amp;amp;amp; 
get(&amp;amp;quot;$var_001&amp;amp;quot …&lt;/code&gt;&lt;/pre&gt;</summary><content type="html">&lt;p&gt;python regular expression to clean the RMP data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import re

strtest = &amp;quot;&amp;quot;&amp;quot;  3602433631519&amp;quot; /&amp;gt;                                &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt; 7 &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;&amp;lt; = &amp;quot;&amp;gt; &amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;HRB_HighClaim_Sideline&amp;lt;/td&amp;gt;

                &amp;lt;!-- Align Rule condition to variable expression --&amp;gt;
                    &amp;lt;td&amp;gt;&amp;lt;/td&amp;gt;

                &amp;lt;td&amp;gt;MVEL&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;
                        &amp;lt;pre class=&amp;quot;code&amp;quot;&amp;gt;get(&amp;amp;quot;$BFS.hrb_claims_by_customer_us.n_claim_count&amp;amp;quot;)!\
=empty &amp;amp;amp;&amp;amp;amp; 
get(&amp;amp;quot;$var_001&amp;amp;quot;)!=empty &amp;amp;amp;&amp;amp;amp; 
get(&amp;amp;quot;$var_002&amp;amp;quot;)!=empty &amp;amp;amp;&amp;amp;amp; 
$var_001&amp;amp;gt;0 &amp;amp;amp;&amp;amp;amp; 
$var_002&amp;amp;lt; 3650&amp;lt;/pre&amp;gt;
                &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt; 
                &amp;lt;td&amp;gt;
                    &amp;lt;table&amp;gt;
                                &amp;lt;/tr&amp;gt;
                        &amp;lt;tr&amp;gt;
                            &amp;lt;td&amp;gt; &amp;lt;a href=
                        &amp;lt;/tr&amp;gt;
                    &amp;lt;/table&amp;gt;
                &amp;lt;/td&amp;gt;tail&amp;quot;&amp;gt; 
                                &amp;lt;td&amp;gt;
                                 &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt; 8 &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;&amp;lt;a href=&amp;quot; 05&amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;fortress_test_bf_continue_mo&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;
                    &amp;lt;table&amp;gt;
                                &amp;lt;tr&amp;gt;
                                    &amp;lt;td&amp;gt; &amp;lt;a href=&amp;quot; &amp;quot; &amp;gt;&amp;lt;i class=&amp;quot;fa fa-bell&amp;quot;&amp;gt;&amp;lt;/i&amp;gt;&amp;lt;/a&amp;gt; &amp;lt;/td&amp;gt;
                                &amp;lt;/tr&amp;gt;
                        &amp;lt;tr&amp;gt;
                            &amp;lt;td&amp;gt; &amp;lt;a href=&amp;quot;#&amp;quot;  &amp;lt;/td&amp;gt;
                        &amp;lt;/tr&amp;gt;
                    &amp;lt;/table&amp;gt;
                &amp;lt;/td&amp;gt;
                                &amp;lt;td&amp;gt;
                                  &amp;lt;a class=&amp;quot;btn btn-link&amp;quot; href=&amp;quot;/ru
                                  &amp;lt;/div&amp;gt;
                                &amp;lt;/td&amp;gt;
            &amp;lt;/tr&amp;gt;
                &amp;lt;td&amp;gt; 999 &amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;&amp;lt;a href=&amp;quot;/rule/show&amp;lt;/td&amp;gt;
                &amp;lt;td&amp;gt;fraudAmtRatioInHL30ForASIN&amp;lt;/td&amp;gt;
&amp;quot;&amp;quot;&amp;quot;

# 1. find pattern like   &amp;lt;td&amp;gt; 7 &amp;lt;/td&amp;gt;,  &amp;lt;td&amp;gt; 8 &amp;lt;/td&amp;gt;,   &amp;lt;td&amp;gt; 999 &amp;lt;/td&amp;gt;
pattern = re.compile(r&amp;quot;&amp;lt;td&amp;gt; [0-9]+ &amp;lt;/td&amp;gt;&amp;quot;)
re.findall(pattern, strtest)
str_split = re.split(pattern, strtest)

# 2. find pattern like &amp;lt;pre class=\&amp;quot;code\&amp;quot;&amp;gt;   ----------any text inside this--------   &amp;lt;/pre&amp;gt;
pattern2 = re.compile(r'&amp;lt;pre class=\&amp;quot;code\&amp;quot;&amp;gt;(.*?)&amp;lt;/pre&amp;gt;')

res = []
for x in str_split:
    res += (re.findall(pattern2, x.replace('\n', '').replace('\t', '')))

# 3. find pattern to remove the check the value is empty or not. if note remove, the count will be doubled
pattern3 = re.compile(r'get(.*?)!=empty')

re.sub(pattern3, '', res[0])
re.sub(pattern3, '', res[0]).replace('&amp;amp;amp;', '&amp;amp;') 
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-05-18 Week 20 -- awk</title><link href="/pages/2019/05/18/2019-05-18-week-20-awk/" rel="alternate"></link><published>2019-05-18T00:00:00-05:00</published><updated>2019-05-18T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-05-18:/pages/2019/05/18/2019-05-18-week-20-awk/</id><summary type="html">&lt;pre&gt;&lt;code class="language-bash"&gt;awk -F, '{OFS=&amp;quot;\t&amp;quot;;print $3,$4}' mo_orders_weekly_2019-04-27.txt   ==&amp;gt;   awk '{print $4&amp;quot;,&amp;quot;$5}' mo_orders_weekly_2019-04-27.txt

cat mo_orders_weekly_2019-04-27.txt | cut -d ',' -f3    ==&amp;gt;    cat mo_orders_weekly_2019-04-27.txt | cut  -f3-4

awk -F, '{OFS=&amp;quot;,&amp;quot;;print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$20,$21}' infile.csv &amp;gt; outfile.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;something else to consider …&lt;/p&gt;</summary><content type="html">&lt;pre&gt;&lt;code class="language-bash"&gt;awk -F, '{OFS=&amp;quot;\t&amp;quot;;print $3,$4}' mo_orders_weekly_2019-04-27.txt   ==&amp;gt;   awk '{print $4&amp;quot;,&amp;quot;$5}' mo_orders_weekly_2019-04-27.txt

cat mo_orders_weekly_2019-04-27.txt | cut -d ',' -f3    ==&amp;gt;    cat mo_orders_weekly_2019-04-27.txt | cut  -f3-4

awk -F, '{OFS=&amp;quot;,&amp;quot;;print $1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$20,$21}' infile.csv &amp;gt; outfile.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;something else to consider - and this faster and more concise:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;cut -d &amp;quot;,&amp;quot; -f1-10,20-25,30-33 infile.csv &amp;gt; outfile.csv
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/drawings/d/19YiBr851A9iurgQ_ceyOLArpM2OAaHd___rmouO11C8/edit?usp=sharing"&gt;graph link&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/52270276/1876887"&gt;How to concatenate multiple column values into a single column in Panda dataframe&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/18674915/1876887"&gt;how do I insert a column at a specific column index in pandas?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/49221349/1876887"&gt;What is the 'cls' variable used for in Python classes?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://unix.stackexchange.com/a/184503/319361"&gt;Bash - how to run a command after the previous finished?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.unix.com/unix-for-dummies-questions-and-answers/68844-generate-csv-file-using-awk-script.html"&gt;generate CSV file using AWK script&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/26842504/printing-column-separated-by-comma-using-awk-command-line"&gt;Printing column separated by comma using Awk command line&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/6482403/1876887"&gt;Check existence of input argument in a Bash shell script&lt;/a&gt;
    &lt;code&gt;bash
    if [ $# -eq 0 ]
      then
        echo "No arguments supplied"
    fi&lt;/code&gt;
    The &lt;code&gt;$#&lt;/code&gt; variable will tell you the number of input arguments the script was passed.&lt;/p&gt;
&lt;p&gt;Or you can check if an argument is an empty string or not like:
&lt;code&gt;bash
if [ -z "$1" ]
  then
    echo "No argument supplied"
fi&lt;/code&gt;
The &lt;code&gt;-z&lt;/code&gt; switch will test if the expansion of "$1" is a null string or not. If it is a null string then the body is executed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/8899878/1876887"&gt;Crontab - Run in directory&lt;/a&gt;
    &lt;code&gt;bash
    cd /path/to/directory &amp;amp;&amp;amp; ./bin/myapp&lt;/code&gt;
    Concerning the use of &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; instead of &lt;code&gt;;&lt;/code&gt;: normally it doesn't make a difference, but if the cd command fails (e.g. because the directory doesn't exist) with &lt;code&gt;&amp;amp;&amp;amp;&lt;/code&gt; the application isn't executed, whereas with &lt;code&gt;;&lt;/code&gt; it's executed (&lt;strong&gt;but not in the intended directory&lt;/strong&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://serverfault.com/questions/85893/running-a-cron-job-manually-and-immediately"&gt;Running a cron job manually and immediately&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cyberciti.biz/faq/python-execute-unix-linux-command-examples/"&gt;Python Execute Unix / Linux Command Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/19325761/python-executing-shell-script-with-argumentsvariable-but-argument-is-not-rea"&gt;Python: executing shell script with arguments(variable), but argument is not read in shell script&lt;/a&gt;
    &lt;code&gt;python
    Process=Popen('./childdir/execute.sh %s %s' % (str(var1),str(var2),), shell=True)&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/9935151/popen-error-errno-2-no-such-file-or-directory"&gt;Popen error: [Errno 2] No such file or directory&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/15107714/wait-process-until-all-subprocess-finish"&gt;wait process until all subprocess finish? [duplicate]&lt;/a&gt;
    &lt;code&gt;python
    exit_codes = [p.wait() for p in p1, p2]&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/questions/4142151/how-to-import-the-class-within-the-same-directory-or-sub-directory"&gt;How to import the class within the same directory or sub directory?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/7857134/1876887"&gt;Extract specific columns from delimited file using Awk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/a/6655669/1876887"&gt;How to split a string in bash delimited by tab&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-05-11 Week 19</title><link href="/pages/2019/05/11/2019-05-11-week-19/" rel="alternate"></link><published>2019-05-11T00:00:00-05:00</published><updated>2019-05-11T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-05-11:/pages/2019/05/11/2019-05-11-week-19/</id><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.unix.com/unix-for-dummies-questions-and-answers/256950-tree-command-does-not-print-properly.html"&gt;linux tree display&lt;/a&gt;: use &lt;code&gt;LC_ALL=C tree&lt;/code&gt; to display the directory structure in tree format. By default &lt;code&gt;tree&lt;/code&gt; may give weired strings. Need &lt;code&gt;LC_ALL=C&lt;/code&gt; to change to default language.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://zh.mxnet.io/blog/learn-mxnet-for-pytorch-users"&gt;十分钟从 PyTorch 转 MXNet&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-05-05 Week 18</title><link href="/pages/2019/05/05/2019-05-05-week-18/" rel="alternate"></link><published>2019-05-05T00:00:00-05:00</published><updated>2019-05-05T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-05-05:/pages/2019/05/05/2019-05-05-week-18/</id><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/pytorch"&gt;kaggle pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006"&gt;visualization of Resnet-50&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kuangliu/pytorch-cifar/blob/master/models/resnet.py"&gt;resnet with pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tablesgenerator.com/latex_tables"&gt;LaTeX Table Generator&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://yuandong-tian.com/five_year_summary_of_PhD.pdf"&gt;博士五年总结系列 田渊栋&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yuandong-tian.com/usage_of_math.pdf"&gt;数学的用处 田渊栋&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/25099638"&gt;博士这五年&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-04-27 Week 16</title><link href="/pages/2019/04/27/2019-04-27-week-16/" rel="alternate"></link><published>2019-04-27T00:00:00-05:00</published><updated>2019-04-27T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-04-27:/pages/2019/04/27/2019-04-27-week-16/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/41107733/1876887"&gt;RHEL 6 - how to install 'GLIBC_2.14' or 'GLIBC_2.15'?&lt;/a&gt;: when install lightgbm in RHEL 6, it asks for intall GLIBC_2.14 to execute the software is built in RHEL 7 and try to run on RHEL 6. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/jhc888007/p/9400450.html"&gt;使用import lightgbm模块， 报： libc.so.6: version …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/41107733/1876887"&gt;RHEL 6 - how to install 'GLIBC_2.14' or 'GLIBC_2.15'?&lt;/a&gt;: when install lightgbm in RHEL 6, it asks for intall GLIBC_2.14 to execute the software is built in RHEL 7 and try to run on RHEL 6. &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.cnblogs.com/jhc888007/p/9400450.html"&gt;使用import lightgbm模块， 报： libc.so.6: version 'GLIBC_2.14' not found&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/playlist?list=PLLbeS1kM6teJqdFzw1ICHfa4a1y0hg8Ax"&gt;动手学深度学习&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://zh.d2l.ai/index.html"&gt;动手学深度学习&lt;/a&gt;: 对于二维卷积，我们要对每个通道进行batchnorm，并且需要保持四维形状使得可以正确的传播&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://d2l.ai/index.html"&gt;Dive into Deep Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-04-20 Week 16</title><link href="/pages/2019/04/20/2019-04-20-week-16/" rel="alternate"></link><published>2019-04-20T00:00:00-05:00</published><updated>2019-04-20T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-04-20:/pages/2019/04/20/2019-04-20-week-16/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch"&gt;code-of-learn-deep-learning-with-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cnblogs.com/bonelee/p/9089984.html"&gt;从信用卡欺诈模型看不平衡数据分类&lt;/a&gt;：这个作者总结了kaggle上面不同的人使用不同的抽样办法，不同的模型，以及异常值检验的办法。我以前也做过这 …&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/L1aoXingyu/code-of-learn-deep-learning-with-pytorch"&gt;code-of-learn-deep-learning-with-pytorch&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.cnblogs.com/bonelee/p/9089984.html"&gt;从信用卡欺诈模型看不平衡数据分类&lt;/a&gt;：这个作者总结了kaggle上面不同的人使用不同的抽样办法，不同的模型，以及异常值检验的办法。我以前也做过这个project，还是挺有意思的一个项目。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://ask.hellobi.com/blog/datakong/8876"&gt;何用Deep Autoencoder实现信用卡欺诈侦测建模&lt;/a&gt;：这是另外一个办法，使用autoencoder来压缩数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/syncthing/syncthing"&gt;The Syncthing Project&lt;/a&gt;: 一个开源软件用来备份文件夹&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://pytorch-cn.readthedocs.io/zh/latest/"&gt;PyTorch中文文档&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-04-13 Week 15</title><link href="/pages/2019/04/13/2019-04-13-week-15/" rel="alternate"></link><published>2019-04-13T00:00:00-05:00</published><updated>2019-04-13T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-04-13:/pages/2019/04/13/2019-04-13-week-15/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;convert dataframe other types: &lt;code&gt;df.dtypes&lt;/code&gt;, &lt;code&gt;df.astype(str)&lt;/code&gt;, &lt;code&gt;df.astype(float)&lt;/code&gt;, &lt;code&gt;df.astype('category')&lt;/code&gt;, &lt;code&gt;df.astype('object')&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/17840195/1876887"&gt;ValueError: If using all scalar values, you must pass an index&lt;/a&gt;
    &lt;code&gt;df = pd.DataFrame({'A': [1], 'B': [2]})&lt;/code&gt; or &lt;code&gt;df = pd.DataFrame({'A': 1, 'B': 2}, index=[0])&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/44661513/1876887"&gt;&lt;code&gt;os …&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;convert dataframe other types: &lt;code&gt;df.dtypes&lt;/code&gt;, &lt;code&gt;df.astype(str)&lt;/code&gt;, &lt;code&gt;df.astype(float)&lt;/code&gt;, &lt;code&gt;df.astype('category')&lt;/code&gt;, &lt;code&gt;df.astype('object')&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/17840195/1876887"&gt;ValueError: If using all scalar values, you must pass an index&lt;/a&gt;
    &lt;code&gt;df = pd.DataFrame({'A': [1], 'B': [2]})&lt;/code&gt; or &lt;code&gt;df = pd.DataFrame({'A': 1, 'B': 2}, index=[0])&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://stackoverflow.com/a/44661513/1876887"&gt;&lt;code&gt;os.listdir&lt;/code&gt;, &lt;code&gt;os.path.exists&lt;/code&gt;, &lt;code&gt;os.path.isfile&lt;/code&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://www.wildml.com/2016/10/learning-reinforcement-learning/"&gt;Learning Reinforcement Learning (with Code, Exercises and Solutions)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/dennybritz/reinforcement-learning"&gt;dennybritz/reinforcement-learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/ShangtongZhang/reinforcement-learning-an-introduction"&gt;ShangtongZhang/reinforcement-learning-an-introduction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/Ju-jl/ReinforcementLearningAnIntroduction.jl"&gt;Ju-jl/ReinforcementLearningAnIntroduction.jl&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://learn-anything.xyz/"&gt;search engine: I want to learn&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-04-06 Week 14</title><link href="/pages/2019/04/06/2019-04-06-week-14/" rel="alternate"></link><published>2019-04-06T00:00:00-05:00</published><updated>2019-04-06T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-04-06:/pages/2019/04/06/2019-04-06-week-14/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u"&gt;Stanford CS234: Reinforcement Learning | Winter 2019&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://hookrace.net/blog/linux-desktop-setup/"&gt;Linux Desktop Setup:linux桌面设置和软件选择&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=6iACK-LNnzM"&gt;1997 WWDC Fireside Chat with Steve Jobs: 乔布斯在1997年的WWDC上的演讲&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://spectrum.ieee.org/energy/the-smarter-grid/chinas-ambitious-plan-to-build-the-worlds-biggest-supergrid"&gt;China’s Ambitious Plan to Build the World’s Biggest Supergrid(中国要建世界最大的超高压电网 …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u"&gt;Stanford CS234: Reinforcement Learning | Winter 2019&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://hookrace.net/blog/linux-desktop-setup/"&gt;Linux Desktop Setup:linux桌面设置和软件选择&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=6iACK-LNnzM"&gt;1997 WWDC Fireside Chat with Steve Jobs: 乔布斯在1997年的WWDC上的演讲&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://spectrum.ieee.org/energy/the-smarter-grid/chinas-ambitious-plan-to-build-the-worlds-biggest-supergrid"&gt;China’s Ambitious Plan to Build the World’s Biggest Supergrid(中国要建世界最大的超高压电网)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=tpypnQGC9KY"&gt;记录LA地区一个做服装进出口的中国老板一天的故事&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/syncthing/syncthing"&gt;syncthing/syncthing: continuous file synchronization program&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-03-30 Week 13</title><link href="/pages/2019/03/30/2019-03-30-week-13/" rel="alternate"></link><published>2019-03-30T00:00:00-05:00</published><updated>2019-03-30T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-03-30:/pages/2019/03/30/2019-03-30-week-13/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ArPaAX_PhIs&amp;amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF"&gt;Convolutional Neural Networks (Course 4 of the Deep Learning Specialization)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ZILIbUvp5lk"&gt;Resnets&lt;/a&gt;
&lt;img alt="alt text" src="/figures/20190330_ResNet_01.png"&gt;
&lt;img alt="alt text" src="/figures/20190330_ResNet_02.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://help.ubuntu.com/stable/ubuntu-help/screen-shot-record.html"&gt;ubuntu screenshots and screencasts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Prt Scrn&lt;/code&gt; to take a screenshot of the desktop.
&lt;code&gt;Alt+Prt Scrn&lt;/code&gt; to take a screenshot of a window.
&lt;code&gt;Shift+Prt Scrn&lt;/code&gt; to take a screenshot of an area you select.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://blog.geohey.com/chang-yong-de-ji-chong-juan-ji-shen-jing-wang-luo-jie-shao/"&gt;常 …&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ArPaAX_PhIs&amp;amp;list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF"&gt;Convolutional Neural Networks (Course 4 of the Deep Learning Specialization)&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ZILIbUvp5lk"&gt;Resnets&lt;/a&gt;
&lt;img alt="alt text" src="/figures/20190330_ResNet_01.png"&gt;
&lt;img alt="alt text" src="/figures/20190330_ResNet_02.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://help.ubuntu.com/stable/ubuntu-help/screen-shot-record.html"&gt;ubuntu screenshots and screencasts&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Prt Scrn&lt;/code&gt; to take a screenshot of the desktop.
&lt;code&gt;Alt+Prt Scrn&lt;/code&gt; to take a screenshot of a window.
&lt;code&gt;Shift+Prt Scrn&lt;/code&gt; to take a screenshot of an area you select.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://blog.geohey.com/chang-yong-de-ji-chong-juan-ji-shen-jing-wang-luo-jie-shao/"&gt;常用的几种卷积神经网络介绍&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://github.com/songhuiming/pytorch_intro"&gt;songhuiming/pytorch introduction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;amp;list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ"&gt;RL Course by David Silver&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-03-23 Week 12</title><link href="/pages/2019/03/23/2019-03-23-week-12/" rel="alternate"></link><published>2019-03-23T00:00:00-05:00</published><updated>2019-03-23T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-03-23:/pages/2019/03/23/2019-03-23-week-12/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=18428017"&gt;Experienced programmers, how do you learn new language?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=17132256"&gt;Learning multiple languages vs. mastering one&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Python - You can do almost everything well in Python. Make lots of stuff  &lt;/li&gt;
&lt;li&gt;Command Line &amp;amp; Bash - How systems are glued together. Deploy the stuff you make.  &lt;/li&gt;
&lt;li&gt;GNU Regex &amp;amp; awk - How to find things …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=18428017"&gt;Experienced programmers, how do you learn new language?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=17132256"&gt;Learning multiple languages vs. mastering one&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Python - You can do almost everything well in Python. Make lots of stuff  &lt;/li&gt;
&lt;li&gt;Command Line &amp;amp; Bash - How systems are glued together. Deploy the stuff you make.  &lt;/li&gt;
&lt;li&gt;GNU Regex &amp;amp; awk - How to find things and process text. A key building block of programming is processing text.  &lt;/li&gt;
&lt;li&gt;HTML5 &amp;amp; Javascript - The right way to make most visuals. A frontend for your stuff.  &lt;/li&gt;
&lt;li&gt;C - How all the above works underneath. Understand the stuff you made.  &lt;/li&gt;
&lt;li&gt;SQL - How to handle data in databases. Persist the stuff.  &lt;/li&gt;
&lt;li&gt;Haskell - Pure functional programming. Your Python and JS code will improve. Make the stuff beautiful.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/275351176/answer/421551347"&gt;有哪些书非常有利于年轻人未来发展？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/27234078"&gt;理解 Word2Vec 之 Skip-Gram 模型&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/34879333"&gt;Batch Normalization原理与实战&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category><category term="pytorch"></category></entry><entry><title>2019-03-16 Week 11</title><link href="/pages/2019/03/16/2019-03-16-week-11/" rel="alternate"></link><published>2019-03-16T00:00:00-05:00</published><updated>2019-03-16T18:18:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-03-16:/pages/2019/03/16/2019-03-16-week-11/</id><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903"&gt;Why do we need to set the gradients manually to zero in pytorch?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903/12"&gt;pytorch &lt;code&gt;loss.backward()&lt;/code&gt; calculates the gradient and cumsum it&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=19423228"&gt;How to learn best practices when you have no one to teach you?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category><category term="pytorch"></category></entry><entry><title>2019-03-09 Week 10</title><link href="/pages/2019/03/09/2019-03-09-week-10/" rel="alternate"></link><published>2019-03-09T00:00:00-06:00</published><updated>2019-03-09T18:18:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-03-09:/pages/2019/03/09/2019-03-09-week-10/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"&gt;How to Develop a Word Embedding Model for Predicting Movie Review Sentiment&lt;/a&gt;: introduce how to do word embedding in keras and gensim and compare their performance on the movie review data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://uber.github.io/ludwig/"&gt;Ludwig&lt;/a&gt;: a toolbox from uber that allows you to train and test deep learning models without …&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"&gt;How to Develop a Word Embedding Model for Predicting Movie Review Sentiment&lt;/a&gt;: introduce how to do word embedding in keras and gensim and compare their performance on the movie review data.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://uber.github.io/ludwig/"&gt;Ludwig&lt;/a&gt;: a toolbox from uber that allows you to train and test deep learning models without writing code. Ludwig is build on top of Tensorflow.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/BayesWitnesses/m2cgen/"&gt;m2cgen: Model2Code Generator&lt;/a&gt; is a lightweight tool to convert your model to C, python or java. Most tree-based models are supported. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.cnbc.com/2019/02/20/amazon-ceo-jeff-bezos-this-choice-is-the-key-to-success.html"&gt;This choice is the key to success&lt;/a&gt;: Choosing to work hard is key when it comes to being successful, according to Jeff Bezos.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=19349676"&gt;How to speak like a leader, not like an engineer?&lt;/a&gt;. Some useful suggestions: (1) communicate in the language of options, not demands. (2) summarize your choices, decisions and solutions in high level language from the perspective of what they care about. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://discuss.pytorch.org/t/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch/4903"&gt;Why do we need to set the gradients manually to zero in pytorch?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-03-02 Week 9</title><link href="/pages/2019/03/02/2019-03-02-week-9/" rel="alternate"></link><published>2019-03-02T00:00:00-06:00</published><updated>2019-03-02T18:38:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-03-02:/pages/2019/03/02/2019-03-02-week-9/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/gist/joshfp/85d96f07aaa5f4d2c9eb47956ccdcc88/lesson2-sgd-in-action.ipynb"&gt;Visualizing gradient descent in action&lt;/a&gt;  An ineresting project to show how learning rate will affect the converge. If LR is too small, it will converge very slowly. If LR is too big(like 1.01), it will diverge. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo"&gt;How to add new embeddings for unknown words in …&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://nbviewer.jupyter.org/gist/joshfp/85d96f07aaa5f4d2c9eb47956ccdcc88/lesson2-sgd-in-action.ipynb"&gt;Visualizing gradient descent in action&lt;/a&gt;  An ineresting project to show how learning rate will affect the converge. If LR is too small, it will converge very slowly. If LR is too big(like 1.01), it will diverge. &lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/45113130/how-to-add-new-embeddings-for-unknown-words-in-tensorflow-training-pre-set-fo"&gt;How to add new embeddings for unknown words in Tensorflow (training &amp;amp; pre-set for testing)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/"&gt;How to Develop a Word Embedding Model for Predicting Movie Review Sentiment&lt;/a&gt;  It is possible that the loaded embedding does not contain all of the words in our chosen vocabulary. As such, when creating the Embedding weight matrix, we need to &lt;strong&gt;skip words that do not have a corresponding vector in the loaded GloVe data&lt;/strong&gt;. Or if the testing data has some out of vocabulary words, you may add a "UNKNOWN" word in the vocabulary and give a unique index for this special word, then you should random initialize the weights of this word.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/keras-team/keras/issues/6480"&gt;Using pre-trained embeddings for out of vocabulary words&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=19280341"&gt;Ask HN: Tools or sites you use to scope out a workplace before taking a job?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/Microsoft/nni"&gt;AutoML toolkit for neural architecture search and hyper-parameter tuning&lt;/a&gt;  NNI (Neural Network Intelligence) 是自动机器学习（AutoML）的工具包。 它通过多种调优的算法来搜索最好的神经网络结构和（或）超参，并支持单机、本地多机、云等不同的运行环境。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://immersivemath.com/ila/index.html"&gt;immersive linear algebra&lt;/a&gt;: Linear Algebra with interactive display and graphs. &lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>How to keep alive ssh sessions</title><link href="/pages/2019/02/28/how-to-keep-alive-ssh-sessions/" rel="alternate"></link><published>2019-02-28T00:00:00-06:00</published><updated>2019-02-28T18:18:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-28:/pages/2019/02/28/how-to-keep-alive-ssh-sessions/</id><summary type="html">&lt;h2&gt;1. On AWS to connect with &lt;code&gt;ServerAliveInterval=100&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;ssh -i ~/.ssh/xxxx.pem ubuntu@hithere.amazonaws.com -o ServerAliveInterval=100
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. General Setup&lt;/h2&gt;
&lt;p&gt;Many NAT firewalls time out idle sessions after a certain period of time to keep their trunks clean. Sometimes the interval between session drops is 24 hours …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. On AWS to connect with &lt;code&gt;ServerAliveInterval=100&lt;/code&gt;&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;ssh -i ~/.ssh/xxxx.pem ubuntu@hithere.amazonaws.com -o ServerAliveInterval=100
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. General Setup&lt;/h2&gt;
&lt;p&gt;Many NAT firewalls time out idle sessions after a certain period of time to keep their trunks clean. Sometimes the interval between session drops is 24 hours, but on many commodity firewalls, connections are killed after as little as 300 seconds. To avoid having your SSH sessions become unresponsive after e.g. 5 minutes, do the following:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On Windows (PuTTY)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In your session properties, go to &lt;strong&gt;&lt;code&gt;Connection&lt;/code&gt;&lt;/strong&gt; and under &lt;strong&gt;&lt;code&gt;Sending of null packets to keep session active&lt;/code&gt;&lt;/strong&gt;, set &lt;strong&gt;&lt;code&gt;Seconds between keepalives (0 to turn off)&lt;/code&gt;&lt;/strong&gt; to e.g. &lt;strong&gt;&lt;code&gt;300&lt;/code&gt;&lt;/strong&gt; (5 minutes).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;On Linux (ssh)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To enable the keep alive system-wide (root access required), edit &lt;code&gt;/etc/ssh/ssh_config&lt;/code&gt;; to set the settings for just your user, edit &lt;code&gt;~/.ssh/config&lt;/code&gt; (create the file if it doesn’t exist). Insert the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Host *
    ServerAliveInterval 300
    ServerAliveCountMax 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can also make your OpenSSH server keep alive all connections with clients by adding the following to &lt;code&gt;/etc/ssh/sshd_config&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ClientAliveInterval 300
ClientAliveCountMax 2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These settings will make the SSH client or server send a null packet to the other side every &lt;strong&gt;300 seconds&lt;/strong&gt; (5 minutes), and give up if it doesn’t receive any response after &lt;strong&gt;2 tries&lt;/strong&gt;, at which point the connection is likely to have been discarded anyway.&lt;/p&gt;
&lt;p&gt;From the ssh_config man page:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;ServerAliveCountMax&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sets the number of server alive messages (see below) which may be sent without ssh(1) receiving any messages back from the server. If this threshold is reached while server alive messages are being sent, ssh will disconnect from the server, terminating the session. It is important to note that the use of server alive messages is very different from TCPKeepAlive (below). The server alive messages are sent through the encrypted channel and therefore will not be spoofable. The TCP keepalive option enabled by TCPKeepAlive is spoofable. The server alive mechanism is valuable when the client or server depend on knowing when a connection has become inactive.&lt;/p&gt;
&lt;p&gt;The default value is 3. If, for example, ServerAliveInterval (see below) is set to 15 and ServerAliveCountMax is left at the default, if the server becomes unresponsive, ssh will disconnect after approximately 45 seconds. This option applies to protocol version 2 only; in protocol version 1 there is no mechanism to request a response from the server to the server alive messages, so disconnection is the responsibility of the TCP stack.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;ServerAliveInterval&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Sets a timeout interval in seconds after which if no data has been received from the server, ssh(1) will send a message through the encrypted channel to request a response from the server. The default is 0, indicating that these messages will not be sent to the server, or 300 if the BatchMode option is set. This option applies to protocol version 2 only. ProtocolKeepAlives and SetupTimeOut are Debian-specific compatibility aliases for this option.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://patrickmn.com/aside/how-to-keep-alive-ssh-sessions/"&gt;How to Keep Alive SSH Sessions&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Linux"></category><category term="linux"></category></entry><entry><title>2019-02-23 Week 8</title><link href="/pages/2019/02/23/2019-02-23-week-8/" rel="alternate"></link><published>2019-02-23T00:00:00-06:00</published><updated>2019-02-23T18:38:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-23:/pages/2019/02/23/2019-02-23-week-8/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.datacamp.com/community/tutorials/categorical-data"&gt;Handling Categorical Data in Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Categorical features may have a very large number of levels, known as high cardinality, (for example, cities or URLs), where most of the levels appear in a relatively small number of instances.&lt;/li&gt;
&lt;li&gt;Many machine learning models, such as regression or SVM or …&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.datacamp.com/community/tutorials/categorical-data"&gt;Handling Categorical Data in Python&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Categorical features may have a very large number of levels, known as high cardinality, (for example, cities or URLs), where most of the levels appear in a relatively small number of instances.&lt;/li&gt;
&lt;li&gt;Many machine learning models, such as regression or SVM or sklearn packages, are algebraic.&lt;/li&gt;
&lt;li&gt;One popular method is embedding by Neural Network&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://aws.amazon.com/getting-started/tutorials/build-train-deploy-machine-learning-model-sagemaker/"&gt;How to Build, Train, and Deploy a Machine Learning Model with Amazon SageMaker | AWS&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://my.oschina.net/taogang/blog/2222908"&gt;谈谈机器学习模型的部署&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://my.oschina.net/taogang/blog/1544709"&gt;图解机器学习&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://linuxize.com/post/how-to-use-linux-screen/"&gt;linux screen command: How To Use Linux Screen&lt;/a&gt;&lt;ol&gt;
&lt;li&gt;&lt;code&gt;screen -S my_screen_job_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;run my python job&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ctrl + a&lt;/code&gt;, then &lt;code&gt;d&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;screen -ls&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;screen -r my_screen_job_name&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ctrl + A&lt;/code&gt;, then &lt;code&gt;:quit&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/1509677/kill-detached-screen-session"&gt;Kill detached screen session&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/yunjey/pytorch-tutorial"&gt;Github上7k+星的Pytorch教程和2w+星的tensorflow教程推荐: pytorch&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/aymericdamien/TensorFlow-Examples"&gt;Github上7k+星的Pytorch教程和2w+星的tensorflow教程推荐: tensorflow&lt;/a&gt;
&lt;img alt="alt text" src="https://pic2.zhimg.com/v2-4bb17051de7118bd0613ca2fb7f7c795_1200x500.jpg" title="Neural Network"&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-02-16 Week 7</title><link href="/pages/2019/02/16/2019-02-16-week-7/" rel="alternate"></link><published>2019-02-16T00:00:00-06:00</published><updated>2019-02-16T18:38:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-16:/pages/2019/02/16/2019-02-16-week-7/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://mlwhiz.com/blog/2018/12/17/text_classification/"&gt;What Kagglers are using for Text Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=18769363"&gt;What Kagglers Are Using for Text Classification | Hacker News&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec?hl=zh-cn"&gt;字词的向量表示法  |  TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/the-book.html"&gt;Sutton &amp;amp; Barto Book: Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sdmg15/Best-websites-a-programmer-should-visit"&gt;GitHub - sdmg15/Best-websites-a-programmer-should-visit: Some useful websites for programmers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/14214315/how-to-find-user-memory-usage-in-linux"&gt;memory ram usage in linux&lt;/a&gt;
    &lt;code&gt;bash
    echo …&lt;/code&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://mlwhiz.com/blog/2018/12/17/text_classification/"&gt;What Kagglers are using for Text Classification&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=18769363"&gt;What Kagglers Are Using for Text Classification | Hacker News&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/tutorials/representation/word2vec?hl=zh-cn"&gt;字词的向量表示法  |  TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://incompleteideas.net/book/the-book.html"&gt;Sutton &amp;amp; Barto Book: Reinforcement Learning: An Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/sdmg15/Best-websites-a-programmer-should-visit"&gt;GitHub - sdmg15/Best-websites-a-programmer-should-visit: Some useful websites for programmers.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/14214315/how-to-find-user-memory-usage-in-linux"&gt;memory ram usage in linux&lt;/a&gt;
    &lt;code&gt;bash
    echo "USER                 RSS      PROCS" ; echo "-------------------- -------- -----" ; ps hax -o rss,user | awk '{rss[$2]+=$1;procs[$2]+=1;}END{for(user in rss) printf "%-20s %8.0f %5.0f\n", user, rss[user]/1024, procs[user];}' | sort -rnk2&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pinyinjoe.com/linux/ubuntu-18-gnome-chinese-setup.htm"&gt;install pinyin in ubuntu 安装 中文输入法&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gto76.github.io/python-cheatsheet/"&gt;Comprehensive Python Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>Install nvidia driver on ubuntu, install fastai and pytorch</title><link href="/pages/2019/02/16/install-nvidia-driver-on-ubuntu-install-fastai-and-pytorch/" rel="alternate"></link><published>2019-02-16T00:00:00-06:00</published><updated>2019-02-16T18:18:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-16:/pages/2019/02/16/install-nvidia-driver-on-ubuntu-install-fastai-and-pytorch/</id><summary type="html">&lt;p&gt;summary of how to install nvidia drivers on ubuntu for deep learning.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Part 1. Install Nvidia driver&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;remove the existed drivers&lt;br&gt;
&lt;code&gt;sudo apt-get purge nvidia*&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;add repo&lt;br&gt;
&lt;code&gt;sudo add-apt-repository ppa:graphics-drivers/ppa&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;update&lt;br&gt;
&lt;code&gt;sudo apt update&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;install version 410&lt;br&gt;
&lt;code&gt;sudo apt install nvidia-driver-410 nvidia-settings&lt;/code&gt;    ## sudo ubuntu-drivers autoinstall&lt;/li&gt;
&lt;li&gt;reboot system&lt;br&gt;
&lt;code&gt;sudo reboot&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;check&lt;br&gt;
&lt;code&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Part 2. Install fastai in venv&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;create venv (current pytorch only support py3.6) 
&lt;code&gt;conda create -n fastai python=3.6&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;activate the venv&lt;br&gt;
&lt;code&gt;conda activate fastai&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;install pytorch and fastai&lt;br&gt;
&lt;code&gt;conda install -c pytorch -c fastai fastai pytorch torchvision cuda92&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;check in python it use gpu&lt;br&gt;
&lt;code&gt;python -c 'import torch; print(torch.rand(2,3).cuda())'&lt;/code&gt;    # test if torch use gpu or not&lt;/li&gt;
&lt;li&gt;install jupyter&lt;br&gt;
&lt;code&gt;conda install -c anaconda jupyter&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Part 3. Test&lt;/h2&gt;
&lt;h3&gt;3.1 test if pytorch use gpu or not&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;nvidia-smi
python -c 'import torch; print(torch.rand(2,3).cuda())'            # 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3.2 test in jupyter notebook to make sure gpu is used&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')         # return device(type='cuda')
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Install python 3.7&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;conda create -n python37 python=3.7
conda activate python37
conda install numpy scipy pandas scikit-learn notebook
which pip
pip install pg8000 category_encoders wordcloud networkx matplotlib xlrd xgboost
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/@sh.tsang/geforce-gtx-1080ti-gpu-nvidia-driver-installation-in-ubuntu-18-04-1d3407ecfd5e"&gt;geforce-gtx-1080ti-gpu-nvidia-driver-installation-in-ubuntu-18-04&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;The following will not work&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;First, remove any previously installed Nvidia driver by entering the following command in the terminal:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;sudo apt-get purge nvidia*&lt;/code&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download latest nvidia drive (https://www.nvidia.com/Download/index.aspx?lang=en-us) &lt;code&gt;NVIDIA-Linux-x86_64-410.93.run&lt;/code&gt; and run it&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;sudo sh NVIDIA-Linux-x86_64-410.93.run&lt;/code&gt;&lt;/p&gt;</content><category term="python"></category><category term="python"></category><category term="pytorch"></category></entry><entry><title>2019-02-09 Week 6</title><link href="/pages/2019/02/09/2019-02-09-week-6/" rel="alternate"></link><published>2019-02-09T00:00:00-06:00</published><updated>2019-02-09T18:38:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-09:/pages/2019/02/09/2019-02-09-week-6/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/33996159"&gt;如何配置一台适用于深度学习的工作站？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/28064776"&gt;配置深度学习主机与环境（TensorFlow+1080Ti）：（四）基于Anaconda的TensorFlow安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/50449900"&gt;不踩坑：Ubuntu下安装TensorFlow的最简单方法（无需手动安装CUDA和cuDNN   远程 …&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/33996159"&gt;如何配置一台适用于深度学习的工作站？&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/28064776"&gt;配置深度学习主机与环境（TensorFlow+1080Ti）：（四）基于Anaconda的TensorFlow安装&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/50449900"&gt;不踩坑：Ubuntu下安装TensorFlow的最简单方法（无需手动安装CUDA和cuDNN   远程运行Jupyter Notebook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/21434933"&gt;DQN实战篇1 从零开始安装Ubuntu, Cuda, Cudnn, Tensorflow, OpenAI Gym&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/35834028"&gt;服务器配置多版本CUDA、CUdnn(不同Linux账户使用不同CUDA、CUdnn版本）&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/37569310"&gt;Ubuntu16.04+Ananconda3+CUDA+CUDNN+Tensorflow-gpu配置教程&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.pugetsystems.com/labs/hpc/Build-TensorFlow-GPU-with-CUDA-9-1-MKL-and-Anaconda-Python-3-6-using-a-Docker-Container-1134/"&gt;Build TensorFlow-GPU with CUDA 9.1 MKL and Anaconda Python 3.6 using a Docker Container&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.msra.cn/zh-cn/news/features/gan-20170511"&gt; 到底什么是生成式对抗网络GAN？&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;# Dockerfile to setup a build environment for TensorFlow
# using Intel MKL and Anaconda3 Python
# GPU support with CUDA 9.1 and cudnn7.1

FROM nvidia/cuda:9.1-cudnn7-devel-ubuntu16.04

MAINTAINER nobody, not even me

# Add a few needed packages to the base Ubuntu 16.04
# OK, maybe *you* don't need emacs :-)
RUN \
    apt-get update &amp;amp;&amp;amp; apt-get install -y \
    build-essential \
    curl \
    emacs-nox \
    git \
    openjdk-8-jdk \
    &amp;amp;&amp;amp; rm -rf /var/lib/lists/*

# Use version 11.1 bazel! install from the deb file.
COPY bazel_0.11.1-linux-x86_64.deb /root/
RUN \
  cd /root; dpkg -i bazel_0.11.1-linux-x86_64.deb &amp;amp;&amp;amp; \
  rm -f bazel_0.11.1-linux-x86_64.deb

# Copy in and install Anaconda3 from the shell archive
# Anaconda3-5.1.0-Linux-x86_64.sh
COPY Anaconda3* /root/
RUN \
  cd /root; chmod 755 Anaconda3*.sh &amp;amp;&amp;amp; \
  ./Anaconda3*.sh -b &amp;amp;&amp;amp; \
  echo 'export PATH=&amp;quot;$HOME/anaconda3/bin:$PATH&amp;quot;' &amp;gt;&amp;gt; .bashrc &amp;amp;&amp;amp; \
  rm -f Anaconda3*.sh

# Copy in the CUDA configuration files
COPY cuda.sh /etc/profile.d/
COPY cuda.conf /etc/ld.so.conf.d/

# That's it! That should be enough to do a TensorFlow 1.7 GPU build
# using CUDA 9.1 Anaconda Python 3.6 Intel MKL with gcc 5.4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;swap交换空间，相当于Win中的虚拟内存，通常需要划分对应物理内存2倍的空间，考虑到深度学习主机内存一般都是32G、64G或者128G，所以选择忽略不划分，之后如有需要还可以在系统设置中添加swap部分。&lt;/p&gt;
&lt;p&gt;EFI系统分区，选择分区类型为“逻辑分区”，分区位置为“空间起始位置”。分配大小为512M，足矣。&lt;/p&gt;
&lt;p&gt;挂载“/”,类型为EXT4日志文件系统，选择“逻辑分区”和“空间起始位置”。根目录将挂载除了“/home”和“/usr”之外的其他目录，分配50G。&lt;/p&gt;
&lt;p&gt;挂载“/usr”,类型为EXT4日志文件系统，选择“逻辑分区”和“空间起始位置”。“/usr”为Linux存放软件的地方，分配100G。&lt;/p&gt;
&lt;p&gt;挂载“/home”，类型为EXT4日志文件系统，选择“逻辑分区”和“空间起始位置”。剩余120G左右空间全部分配给“/home”。/home 用户的home目录所在地，这个分区的大小取决于有多少用户。如果是多用户共同使用一台电脑的话，这个分区是完全有必要的，况且根用户也可以很好地控制普通用户使用计算机，如对用户或者用户组实行硬盘限量使用，限制普通用户访问哪些文件等。 以往Linux系统主要是提供服务器使用，所以/home这个目录使用量并不高。但随著Linux的桌面应用发展，不少人也拿来在日常上使用，这时/home就变成存储媒体中，最占容量的目录。假如你安装Ubuntu主要是桌面应用，那你可能需要把最大的空间留给他。 额外分割出/home有个最大的好处，当你重新安装系统时，你不需要特别去备份你的个人文件，只要在安装时，选择不要格式化这个分区，重新挂载为/home就不会丢失你的数据。 还有一个特别的应用：假如你会在你的计算机上，安装两个或更多的Linux系统，你可以共享/home这个分区。简单地说，你的个人文件可以在切换到其它Linux系统时，仍正常使用&lt;/p&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>2019-02-02 Week 5</title><link href="/pages/2019/02/02/2019-02-02-week-5/" rel="alternate"></link><published>2019-02-02T00:00:00-06:00</published><updated>2019-02-02T18:38:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2019-02-02:/pages/2019/02/02/2019-02-02-week-5/</id><summary type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-tensorflow.html"&gt;AWS Activating TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gto76.github.io/python-cheatsheet/"&gt;Comprehensive Python Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0"&gt;GAN by Example using Keras on Tensorflow Backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/44061208/how-to-implement-the-conv1dtranspose-in-keras"&gt;How to Implement the Conv1DTranspose in keras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras"&gt;Advanced-Deep-Learning-with-Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32085405"&gt;人人都能看懂的LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/30465140"&gt;Step-by-step to LSTM: 解析LSTM神经网络设计原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/chap5.html#what's_causing_the_vanishing_gradient_problem_unstable_gradients_in_deep_neural_nets"&gt;What's causing the vanishing gradient problem? Unstable gradients in deep neural …&lt;/a&gt;&lt;/li&gt;&lt;/ol&gt;</summary><content type="html">&lt;h1&gt;Shares From Internet&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-tensorflow.html"&gt;AWS Activating TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gto76.github.io/python-cheatsheet/"&gt;Comprehensive Python Cheatsheet&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0"&gt;GAN by Example using Keras on Tensorflow Backend&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/44061208/how-to-implement-the-conv1dtranspose-in-keras"&gt;How to Implement the Conv1DTranspose in keras?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/PacktPublishing/Advanced-Deep-Learning-with-Keras"&gt;Advanced-Deep-Learning-with-Keras&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32085405"&gt;人人都能看懂的LSTM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/30465140"&gt;Step-by-step to LSTM: 解析LSTM神经网络设计原理&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://neuralnetworksanddeeplearning.com/chap5.html#what's_causing_the_vanishing_gradient_problem_unstable_gradients_in_deep_neural_nets"&gt;What's causing the vanishing gradient problem? Unstable gradients in deep neural nets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zhuanlan.zhihu.com/p/32481747"&gt;人人都能看懂的GRU&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f"&gt;GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/playlists"&gt;李宏毅 -- NTU&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category><category term="deep learning"></category></entry><entry><title>leetcode 312. Burst Balloons</title><link href="/pages/2018/06/18/leetcode-312-burst-balloons/" rel="alternate"></link><published>2018-06-18T00:00:00-05:00</published><updated>2018-06-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-06-18:/pages/2018/06/18/leetcode-312-burst-balloons/</id><summary type="html">&lt;p&gt;题目大意 给定n个气球，每个气球有一个数字，用数组nums来表示n个气球上面的数字。如果气球i被打破，那么你会得到nums[left] * nums[i] * nums[right].这里left和right分别为气球i的左右两边的气球。打破所有的气球后把所有的数字相加，求解怎么打破这些气球使得最后得到的数字最大。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/burst-balloons/description/"&gt;312. Burst Balloons&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意 给定n个气球，每个气球有一个数字，用数组nums来表示n个气球上面的数字。如果气球i被打破，那么你会得到nums[left] * nums[i] * nums[right] 。 这里left和right分别为气球i的左右两边的气球。打破所有的气球后把所有的数字相加，求解怎么打破这些气球使得最后得到的数字最大。&lt;/p&gt;
&lt;p&gt;比如给定 [3, 1, 5, 8]，最大可以得到 167。办法如下&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    nums = [3,1,5,8] --&amp;gt; [3,5,8] --&amp;gt;   [3,8]   --&amp;gt;  [8]  --&amp;gt; []
   coins =  3*1*5      +  3*5*8    +  1*3*8      + 1*8*1   = 167
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;解题思路：取一个子串从nums[i]到nums[j]，加入中间有一个k，对应的nums[k]，如果nums[k]没有被打破，那么nums[k]把nums[i]到nums[j]分成两个子串，分别为nums[i]到nums[k-1]和nums[k+1]到nums[j]。这样我们可以对这两个子串分别求解打破以后的最大值，然后对不同的k来寻找最后的最大值，这样把一个nums[i]到nums[j]的大的问题就拆分成两个小的问题来求解。&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0312_burst_balloons.svg"&gt;&lt;/p&gt;
&lt;h2&gt;1. 记忆化递归&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxCoins(self, nums):
        nums.insert(0, 1)
        nums.append(1)
        n = len(nums)
        dp = [[0 for _ in range(n + 1)] for _ in range(n + 1)]
        def divid(nums, dp, low, high):
            if low + 1 == high:
                return 0
            if dp[low][high] &amp;gt; 0:
                return dp[low][high]
            ans = 0
            for i in range(low + 1, high):
                ans = max(ans, nums[low]*nums[i]*nums[high] + divid(nums, dp, low, i) + divid(nums, dp, i, high))
            dp[low][high] = ans
            return ans
        return divid(nums, dp, 0, n - 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 递推/态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxCoins(self, nums):
        n = len(nums)
        if n == 0:
            return 0
        nums.insert(0, 1)
        nums.append(1)
        c = [[0 for _ in range(n+2)] for _ in range(n+2)]
        for l in range(1, n + 1):
            # l is the len between j(include j) and i, so l = j - i + 1
            for i in range(1, n - l + 2):
                j = i + l - 1
                for k in range(i, j + 1):
                    c[i][j] = max(c[i][j], c[i][k-1] + nums[i-1]*nums[k]*nums[j+1] + c[k+1][j])
        return c[1][n]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxCoins(self, nums):
        n = len(nums)
        if n == 0:
            return 0
        nums.insert(0, 1)
        nums.append(1)
        c = [[0 for _ in range(n+2)] for _ in range(n+2)]
        for i in range(n-1, -1, -1):
            for j in range(i + 2, n + 2):
                for k in range(i + 1, j):
                    c[i][j] = max(c[i][j], c[i][k] + nums[i]*nums[k]*nums[j] + c[k][j])
        return c[0][n+1]
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 321. Create Maximum Number</title><link href="/pages/2018/06/18/leetcode-321-create-maximum-number/" rel="alternate"></link><published>2018-06-18T00:00:00-05:00</published><updated>2018-06-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-06-18:/pages/2018/06/18/leetcode-321-create-maximum-number/</id><summary type="html">&lt;p&gt;题目大意 给定长度为m和n的两个数组，数组里面数字为0到9.要求从这两个数组里面去k个数字构成的数字的最大值。k&amp;lt;=m+n。题目要求新的数字的顺序跟原来数组的数字顺序保持一致。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/create-maximum-number/description/"&gt;321. Create Maximum Number&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意 给定长度为m和n的两个数组，数组里面数字为0到9.要求从这两个数组里面去k个数字构成的数字的最大值。k&amp;lt;=m+n。题目要求新的数字的顺序跟原来数组的数字顺序保持一致。&lt;/p&gt;
&lt;p&gt;比如数组1为：nums1 = [3, 4, 6, 5]，数组2为nums2 = [9, 1, 2, 5, 8, 3]，给定k = 5，那么结果应该为 [9, 8, 6, 5, 3]&lt;/p&gt;
&lt;p&gt;解题思路：这个问题可以转化为两个子问题&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;从数组nums1中找出k个元素的子数组，保持顺序不变，并且使得子数组的值最大 (getK)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;合并从nums1和nums2按上面方法取得的子数组(长度分别为k1和k-k1)，使得合并以后的数组最大&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后对k1进行迭代就可以了&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0321_create_maximum_number.svg"&gt;&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxNumber(self, nums1, nums2, k):

        def getK(nums, k):
            n = len(nums)
            to_pop = n - k
            ans = []
            for num in nums:
                while len(ans) &amp;gt; 0 and num &amp;gt; ans[-1] and to_pop &amp;gt; 0:
                    to_pop -= 1
                    ans.pop()
                ans.append(num)
            return ans[:k]

        def getMax(nums1, nums2):
            ans = []
            while nums1 and nums2:
                if nums1 &amp;gt; nums2:
                    ans.append(nums1.pop(0))
                else:
                    ans.append(nums2.pop(0))
            if nums1:
                ans += nums1
            else:
                ans += nums2
            return ans

        n1 = len(nums1)
        n2 = len(nums2)
        ans = []
        for k1 in range(k+1):
            k2 = k - k1
            if k1 &amp;gt; n1 or k2 &amp;gt; n2:
                continue
            ans = max(ans, getMax(getK(nums1, k1), getK(nums2, k2)))
        return ans
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;getK&lt;/code&gt;和&lt;code&gt;getMax&lt;/code&gt;还可以这么写：&lt;/p&gt;
&lt;h4&gt;&lt;code&gt;getK&lt;/code&gt;函数还可以这么写：&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def getK(nums, k):
    n = len(nums)
    ans = [0 for _ in range(k)]
    j = 0
    for i in range(n):
        while(j &amp;gt; 0 and ans[j-1] &amp;lt; nums[i] and n - i &amp;gt; k - j):
            j -= 1
        if j &amp;lt; k:
            ans[j] = nums[i]
            j += 1
    return ans
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;code&gt;getMax&lt;/code&gt;，从两个数组里面优先取较大的元素合并的方程:&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def getMax(nums1, nums2):
    ans = []
    while nums1 or nums2:
        if nums1 &amp;gt; nums2:
            ans += nums1[0],
            nums1 = nums1[1:]
        else:
            ans += nums2[0],
            nums2 = nums2[1:]
    return ans
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;&lt;code&gt;getMax&lt;/code&gt;还可以这么写：&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def getMax(nums1, nums2):
    return [max(nums1, nums2).pop(0) for _ in nums1 + nums2]
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 322. Coin Change</title><link href="/pages/2018/06/18/leetcode-322-coin-change/" rel="alternate"></link><published>2018-06-18T00:00:00-05:00</published><updated>2018-06-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-06-18:/pages/2018/06/18/leetcode-322-coin-change/</id><summary type="html">&lt;p&gt;题目大意 给定一堆硬币和一个数字，求组成那个数字的最少硬币个数。如果币值没法构成给定数字，返回 -1&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/coin-change/description/"&gt;322. Coin Change&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意 给定一堆硬币和一个数字，求组成那个数字的最少硬币个数.如果币值没法构成给定数字，返回 -1&lt;/p&gt;
&lt;p&gt;比如coins = [1, 2, 5], amount = 11，那么最少需要三枚硬币： (11 = 5 + 5 + 1)&lt;/p&gt;
&lt;p&gt;解题思路：本题可以用动态规划和binary search来做。&lt;/p&gt;
&lt;p&gt;动态规划：定义dp[i][j]表示用前i种硬币构成数字j的最少硬币个数。那么前i-1种硬币可以构成j-k * coin_i，都是合法的。所以dp[i][j] = min(dp[i][j], dp[i-1][j - k * coin_i]+ k ). 状态转移方程还有一个简化的版本：用前i个硬币构成j - coin_i，然后把coin_i加上就可以了。所以dp[i][j] = min(dp[i][j], dp[i][j - coin_i] + 1).&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0322_coin_change_01.svg"&gt;&lt;/p&gt;
&lt;h2&gt;1. 动态规划，第一个写法&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def coinChange(self, coins, amount):
        dp = [float('inf') for _ in range(amount + 1)]
        dp[0] = 0
        for coin in coins:
            for i in range(amount-coin, -1, -1):
                if(dp[i] != float('inf')):
                    for k in range(1, amount + 1):
                        if k*coin + i &amp;lt;= amount:
                            dp[i+k*coin] = min(dp[i+k*coin], dp[i]+k)
        if dp[amount] &amp;lt; float('inf'):
            return dp[amount]
        else:
            return -1
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 动态规划，第二种解法&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def coinChange(self, coins, amount):
        dp = [amount + 1 for _ in range(amount + 1)]
        dp[0] = 0
        for coin in coins:
            for i in range(coin, amount + 1):
                dp[i] = min(dp[i], dp[i - coin] + 1)
        if dp[amount] &amp;gt;= amount + 1:
            return -1
        else:
            return dp[amount]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3. dfs&lt;/h2&gt;
&lt;p&gt;第三种办法，dfs加强剪枝。首先把coins从大到小排列，先使用大的。如果剩余amount正好整除币值，就是一个合法解。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def coinChange(self, coins, amount):
        coins = sorted(coins)[::-1]
        self.ans = float('inf')
        def dfs(coins, s, amount, count):
            coin = coins[s]
            # reach last coin
            if s == len(coins) - 1:
                if amount % coin == 0:
                    self.ans = min(self.ans, count + amount / coin)
            else:
                for k in range(amount/coin, -1, -1):
                    if count + k &amp;lt; self.ans:
                        # pruning, only search if new ans is less than current ans
                        dfs(coins, s + 1, amount - k*coin, count+k)           
        dfs(coins, 0, amount, 0)
        if self.ans &amp;lt; float('inf'):
            return self.ans
        else:
            return -1
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 377. Combination Sum IV</title><link href="/pages/2018/06/18/leetcode-377-combination-sum-iv/" rel="alternate"></link><published>2018-06-18T00:00:00-05:00</published><updated>2018-06-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-06-18:/pages/2018/06/18/leetcode-377-combination-sum-iv/</id><summary type="html">&lt;p&gt;题目大意 给定一个整型数组，里面没有重复数字。求解所有可能的组合使得组合的和为给定的整数。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/combination-sum-iv/description/"&gt;377. Combination Sum IV&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意 给定一个整型数组，里面没有重复数字。求解所有可能的组合使得组合的和为给定的整数。&lt;/p&gt;
&lt;p&gt;解题思路：使用dp(nums, target)表示所有的组合来得到目标值。那么对nums里的每一个num_i，dp(nums, target) = num_i X dp(nums, target - num_i)&lt;/p&gt;
&lt;p&gt;所以可以使用记忆化递归的办法来求解。&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0377_combination_sum_iv.svg"&gt;&lt;/p&gt;
&lt;h2&gt;1. 记忆化递归&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def combinationSum4(self, nums, target):
        self.f = [-1 for _ in range(target + 1)]
        self.f[0] = 1
        def dp(nums, target):
            if target &amp;lt; 0:
                return 0
            if self.f[target] != -1:
                return self.f[target]
            ans = 0
            for num in nums:
                ans += dp(nums, target - num)
            self.f[target] = ans
            return ans
        return dp(nums, target)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 递推/动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def combinationSum4(self, nums, target):
        f = [0 for _ in range(target + 1)]
        f[0] = 1
        for i in range(1, target + 1):
            for num in nums:
                if i - num &amp;gt;= 0:
                    f[i] += f[i - num]
        return f[target]
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 416. Partition Equal Subset Sum</title><link href="/pages/2018/06/18/leetcode-416-partition-equal-subset-sum/" rel="alternate"></link><published>2018-06-18T00:00:00-05:00</published><updated>2018-06-28T18:38:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-06-18:/pages/2018/06/18/leetcode-416-partition-equal-subset-sum/</id><summary type="html">&lt;p&gt;题目大意 给定一个数组，求能不能把数组分成两部分使得两个子数组的和相等&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/partition-equal-subset-sum/description/"&gt;416. Partition Equal Subset Sum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意 给定一个数组，求能不能把数组分成两部分使得两个子数组的和相等&lt;/p&gt;
&lt;p&gt;解题思路：直接brute force解，数组的每个元素都取二进制0或者1来表示有没有选取，这样时间复杂度太高。会超时&lt;/p&gt;
&lt;p&gt;动态规划：dp[i][j]表示能否用数组前i个元素的到和j。退一步就是能否用前i-1个元素得到和 j - num_i 。所以如果 dp[i-1][j - num_i] 成立，那么dp[i][j]也成立。最后就是检查dp[n-1][sum/2]是不是成立。所以如果数组的和为基数的话，那么肯定无解。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def canPartition(self, nums):
        n = len(nums)
        s = sum(nums)
        if s % 2 != 0:
            return False
        dp = [0 for _ in range(s + 1)]
        dp[0] = 1
        for num in nums:
            for i in range(s, -1, -1):
                if dp[i]:
                    dp[i+num] = 1
            if dp[s/2]:
                return True
        return False
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>Credit Card Fraud Detection / Imbalanced data modeling - Part III: Ensembling/Stacking models</title><link href="/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-iii-ensemblingstacking-models/" rel="alternate"></link><published>2018-05-12T19:08:00-05:00</published><updated>2018-05-12T19:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-05-12:/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-iii-ensemblingstacking-models/</id><summary type="html">&lt;p&gt;&lt;a href="https://www.kaggle.com/mlg-ulb/creditcardfraud"&gt;It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase&lt;/a&gt;. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Table of Contents&lt;span class="tocSkip"&gt;&lt;/span&gt;&lt;/h1&gt;
&lt;div class="toc"&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#0.-Data-Preparation" data-toc-modified-id="0.-Data-Preparation-1"&gt;0. Data Preparation&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Modeling-Part-3:-Ensembing(Stacking)-Models" data-toc-modified-id="Modeling-Part-3:-Ensembing(Stacking)-Models-2"&gt;Modeling Part 3: Ensembing(Stacking) Models&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.-More-models-with-GridSearchCV-to-Select-Hyperparameters" data-toc-modified-id="1.-More-models-with-GridSearchCV-to-Select-Hyperparameters-3"&gt;1. More models with GridSearchCV to Select Hyperparameters&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.1.-Prepare-base-models" data-toc-modified-id="1.1.-Prepare-base-models-3.1"&gt;1.1. Prepare base models&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.2.-GridSearch-to-find-the-best-hyperparameters" data-toc-modified-id="1.2.-GridSearch-to-find-the-best-hyperparameters-3.2"&gt;1.2. GridSearch to find the best hyperparameters&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.3.-Base-models-performance" data-toc-modified-id="1.3.-Base-models-performance-3.3"&gt;1.3. Base models performance&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.4.-Stacking" data-toc-modified-id="1.4.-Stacking-3.4"&gt;1.4. Stacking&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.5.-Single-base-model-parameters-and-results" data-toc-modified-id="1.5.-Single-base-model-parameters-and-results-3.5"&gt;1.5. Single base model parameters and results&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, Normalizer, scale
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, log_loss, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve
from sklearn.metrics import make_scorer, precision_score, fbeta_score, f1_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedShuffleSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression

%matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)

seed = 999

creditcard = pd.read_csv('creditcard.csv')
creditcard.columns = [x.lower() for x in creditcard.columns]
creditcard.rename(columns = {'class': 'fraud'}, inplace = True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;0. Data Preparation&lt;/h1&gt;
&lt;p&gt;Read in the data. Since the data is from &lt;code&gt;PCA&lt;/code&gt;, there is no missing data issue. Then we will normalize the amount.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Split Test Data Out
creditcard.drop(columns = 'time', inplace = True)

# Normalize the 'amount' column
scaler = StandardScaler()
creditcard['amount'] = scaler.fit_transform(creditcard['amount'].values.reshape(-1, 1))
# creditcard.drop(columns = 'amount', inplace = True)

X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;Modeling Part 3: Ensembing(Stacking) Models&lt;/h1&gt;
&lt;p&gt;In &lt;a href="http://songhuiming.github.io/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/"&gt;Part I&lt;/a&gt; and &lt;a href="http://songhuiming.github.io/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/"&gt;Part II&lt;/a&gt;, we have tested the Logistic Regression and Random Forest models on this imbalanced data.&lt;/p&gt;
&lt;p&gt;Here we will try the ebsembing models by combining the predictions of multiple machine learning models. The basic idea is to use multiple base models to predict the data, and then use another model to combine these base model results. Since these base models results will be used as input in the new model, the base models results will be better if they are higher in performance and less correlated. If the base models results are high correlated, then combining them will not improve too much since all the input are correlated. Ensembling / Stacking models have these advantages:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. A lot of the time, it can beat most state-of-art single model
2. Each single base model can be simple and built quickly
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we will do these things:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Prepare a series of base models: RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier, and LinearSVC models.
2. Use GridSearchCV to select the hyperparameters for these candidate models
3. Use the output from the base models as input to build a new model
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;1. More models with GridSearchCV to Select Hyperparameters&lt;/h1&gt;
&lt;h2&gt;1.1. Prepare base models&lt;/h2&gt;
&lt;p&gt;We will use these models: &lt;code&gt;RandomForestClassifier&lt;/code&gt;, &lt;code&gt;ExtraTreesClassifier&lt;/code&gt;, &lt;code&gt;AdaBoostClassifier&lt;/code&gt;, &lt;code&gt;GradientBoostingClassifier&lt;/code&gt;, and &lt;code&gt;LinearSVC&lt;/code&gt; models.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;seed = 999

oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

models = {
    'rf': RandomForestClassifier(random_state = seed, warm_start = True),  # 0:17:24.925482
    'et': ExtraTreesClassifier(random_state = seed, warm_start = True),  # 0:02:45.797856
    'ada': AdaBoostClassifier(random_state = seed),   #0:05:17.973671
    'gb': GradientBoostingClassifier(random_state = seed, warm_start = True),   #0:54:11.773175
    'svc': LinearSVC(random_state = seed) }   #0:02:01.656640

gv_parameters = {
    'rf': {'n_estimators': [50, 100, 200, 500], 'max_depth': [10, 20, 50, 100], 'min_samples_leaf': [10, 20, 50]},
    'et': {'n_estimators': [50, 100, 200, 500], 'max_depth': [10, 20, 50, 100], 'min_samples_leaf': [10, 20, 50]},
    'ada': {'n_estimators': [50, 100, 200, 500], 'learning_rate': [0.75, 1, 1.5]},
    'gb': {'n_estimators': [50, 100, 200, 500], 'max_depth': [10, 20, 50, 100], 'min_samples_leaf': [10, 20, 50]},
    'svc': {'C': np.power(5.0, np.arange(-3,3))} }

&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;1.2. GridSearch to find the best hyperparameters&lt;/h2&gt;
&lt;p&gt;For each model's hyperparameter combinations, &lt;code&gt;GridSearchCV&lt;/code&gt; will loop through the combinations and find the parameters that gives the best performance metric. This will be time consuming if the data is big or if the model is slow(like &lt;code&gt;SVM&lt;/code&gt;) or complicated.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;grid_searches = {}
for key in models.keys():
    print(&amp;quot;---------------------------------Running GridSearchCV for %s. ---------------&amp;quot; % key)
    model = models[key]
    params = gv_parameters[key]
    gs = GridSearchCV(model, params, cv = 3, n_jobs = 70, verbose = 3, scoring=wt_loss_score)
    ts = datetime.datetime.now()
    gs.fit(Xtrain_oversample, ytrain_oversample)
    te = datetime.datetime.now()
    print (&amp;quot;run time for &amp;quot; + str(key) + &amp;quot; is: &amp;quot; + str(ts-te))
    grid_searches[key] = gs

# to get the summary information of the model results from GridSearchCV on all parameter combinations
def grid_cv_summary(grid_searches):
    cv_results = pd.DataFrame()
    for key in grid_searches.keys():
        model_result = pd.DataFrame(grid_searches[key].cv_results_)
        model_result['model'] = key
        cv_results = pd.concat([cv_results, model_result], axis = 0)
    return cv_results

cv_sum = grid_cv_summary(grid_searches)
cv_grp = cv_sum.groupby('model')
# get the best_params_ for the model with highest mean_test_score
best_params = cv_grp.apply(lambda x: x[x.mean_test_score == x.mean_test_score.max()]).params
best_params = dict(zip(best_params.index.get_level_values(0), best_params.values))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;1.3. Base models performance&lt;/h2&gt;
&lt;p&gt;For each base model, we print out their recall/performance/roc_auc score and confusion matrix. If we only look at ROC, then &lt;code&gt;LinearSVC&lt;/code&gt; gives the best single model, although it has lower precision score than the other models.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;for key in grid_searches.keys():
    pred_test = grid_searches[key].predict(Xtest)
    print (&amp;quot;\n\n\n----------- For %s, the metrics on TEST data is: ---------- \n&amp;quot; %key)
    print(&amp;quot;recall score on test data is %s&amp;quot; %str(recall_score(ytest, pred_test)))
    print(&amp;quot;precision score on test data is %s&amp;quot; %str(precision_score(ytest, pred_test)))
    print(&amp;quot;roc_auc score on test data is %s&amp;quot; %str(roc_auc_score(ytest, pred_test)))
    print(&amp;quot;confusion matrix on the test data is: \n&amp;quot;)
    print(confusion_matrix(ytest, pred_test))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;----------- For et, the metrics on TEST data is: ----------

recall score on test data is 0.845679012345679
precision score on test data is 0.7696629213483146
roc_auc score on test data is 0.9226210142996714
confusion matrix on the test data is:

[[93784    41]
 [   25   137]]


----------- For rf, the metrics on TEST data is: ----------

recall score on test data is 0.8271604938271605
precision score on test data is 0.8481012658227848
roc_auc score on test data is 0.9134523492317259
confusion matrix on the test data is:

[[93801    24]
 [   28   134]]


----------- For ada, the metrics on TEST data is: ----------

recall score on test data is 0.845679012345679
precision score on test data is 0.7828571428571428
roc_auc score on test data is 0.9226370015099032
confusion matrix on the test data is:

[[93787    38]
 [   25   137]]


----------- For svc, the metrics on TEST data is: ----------

recall score on test data is 0.9320987654320988
precision score on test data is 0.06425531914893617
roc_auc score on test data is 0.9543307576161294
confusion matrix on the test data is:

[[91626  2199]
 [   11   151]]


----------- For gb, the metrics on TEST data is: ----------

recall score on test data is 0.8271604938271605
precision score on test data is 0.8535031847133758
roc_auc score on test data is 0.9134576783018031
confusion matrix on the test data is:

[[93802    23]
 [   28   134]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;1.4. Stacking&lt;/h2&gt;
&lt;p&gt;We will collect the output from the base models and put them as input into the new model. Here the base model outputs are correlated since there are too many 0. So the outpur from stacking did not improve too much compared to the single model. We can try GridSearch again to see if it can be improved. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def multiple_pred(grid_searches, Xtrain, Xtest, ytrain, ytest):
    train_pred = {}
    test_pred = {}
    for key in grid_searches.keys():
        train_pred[key] = grid_searches[key].predict(Xtrain)
        test_pred[key] = grid_searches[key].predict(Xtest)
    return ((pd.DataFrame(train_pred), ytrain), (pd.DataFrame(test_pred), ytest))

ensemble_data = multiple_pred(grid_searches, Xtrain, Xtest, ytrain, ytest)

# ensembling model, change to xgb later
gbc = GradientBoostingClassifier()
gbc.fit(ensemble_data[0][0], ensemble_data[0][1])

pred = gbc.predict(ensemble_data[1][0])
print recall_score(ensemble_data[1][1], pred)
print confusion_matrix(ensemble_data[1][1], pred)
print classification_report(ensemble_data[1][1], pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.827160493827

[[93802    23]
 [   28   134]]

             precision    recall  f1-score   support

          0       1.00      1.00      1.00     93825
          1       0.85      0.83      0.84       162

avg / total       1.00      1.00      1.00     93987
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;1.5. Single base model parameters and results&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;for key in grid_searches.keys():
    print(key)
    print(grid_searches[key].best_params_)
    pred_grid = grid_searches[key].predict(Xtest)
    print(roc_auc_score(ytest, pred_grid))
    print(recall_score(ytest, pred_grid))
    print(confusion_matrix(ytest, pred_grid))
    print('\n')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;et
{'n_estimators': 500, 'max_depth': 50, 'min_samples_leaf': 10}
0.9226210143
0.845679012346
[[93784    41]
 [   25   137]]


rf
{'n_estimators': 200, 'max_depth': 50, 'min_samples_leaf': 10}
0.913452349232
0.827160493827
[[93801    24]
 [   28   134]]


ada
{'n_estimators': 500, 'learning_rate': 1.5}
0.92263700151
0.845679012346
[[93787    38]
 [   25   137]]


svc
{'C': 1.0}
0.954330757616
0.932098765432
[[91626  2199]
 [   11   151]]


gb
{'n_estimators': 200, 'max_depth': 50, 'min_samples_leaf': 20}
0.913457678302
0.827160493827
[[93802    23]
 [   28   134]]
&lt;/code&gt;&lt;/pre&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category></entry><entry><title>Credit Card Fraud Detection / Imbalanced data modeling - Part II: Random Forest</title><link href="/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/" rel="alternate"></link><published>2018-05-12T18:08:00-05:00</published><updated>2018-05-12T18:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-05-12:/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/</id><summary type="html">&lt;p&gt;&lt;a href="https://www.kaggle.com/mlg-ulb/creditcardfraud"&gt;It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase&lt;/a&gt;. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Table of Contents&lt;span class="tocSkip"&gt;&lt;/span&gt;&lt;/h1&gt;
&lt;div class="toc"&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#0.-Introduction" data-toc-modified-id="0.-Introduction-1"&gt;0. Introduction&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Conclusion:" data-toc-modified-id="Conclusion:-1.1"&gt;Conclusion:&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Modeling-Part-2:-RandomForestClassifier" data-toc-modified-id="Modeling-Part-2:-RandomForestClassifier-2"&gt;Modeling Part 2: RandomForestClassifier&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.-Use-the-Imbalanced-Data-Directly-in-RandomForestClassifier" data-toc-modified-id="1.-Use-the-Imbalanced-Data-Directly-in-RandomForestClassifier-3"&gt;1. Use the Imbalanced Data Directly in RandomForestClassifier&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#2.-Create-Over-sampling-data-and-Fit-the-model" data-toc-modified-id="2.-Create-Over-sampling-data-and-Fit-the-model-4"&gt;2. Create Over-sampling data and Fit the model&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#3.-RandomForestClassifier-with-class_weight" data-toc-modified-id="3.-RandomForestClassifier-with-class_weight-5"&gt;3. RandomForestClassifier with class_weight&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#4.-Self-defined-Score-and-GridSearchCV-of-hyperparameter" data-toc-modified-id="4.-Self-defined-Score-and-GridSearchCV-of-hyperparameter-6"&gt;4. Self-defined Score and GridSearchCV of hyperparameter&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;h1&gt;0. Introduction&lt;/h1&gt;
&lt;p&gt;It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.&lt;/p&gt;
&lt;p&gt;The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://songhuiming.github.io/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/"&gt;In the first part, Loigstic regression model was built do different kind of analysis&lt;/a&gt;. In this part, we will try Random Forest models.Since this is imbalanced data, we will try different methods and compare their results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Model on imbalanced data directly
2. Model on over-sampling data
3. Assign more weights on rare class
4. Use customed loss function
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Conclusion:&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;As expected, use the imbalanced data is not a good way. The performance is the worst compaed to using over-sampling or class weights&lt;/li&gt;
&lt;li&gt;Use imbalanced data, RandomForestClassifier result is better than LogisticRegression. &lt;/li&gt;
&lt;li&gt;If we can custom a good loss function, the model performance will be better: here the customed loss function performance is better than roc_auc scoring function.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, Normalizer, scale
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, log_loss, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve
from sklearn.metrics import make_scorer, precision_score, fbeta_score, f1_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedShuffleSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression

%matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)

seed = 999

creditcard = pd.read_csv('creditcard.csv')
creditcard.columns = [x.lower() for x in creditcard.columns]
creditcard.rename(columns = {'class': 'fraud'}, inplace = True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Split Test Data Out
creditcard.drop(columns = 'time', inplace = True)

# Normalize the 'amount' column
scaler = StandardScaler()
creditcard['amount'] = scaler.fit_transform(creditcard['amount'].values.reshape(-1, 1))
# creditcard.drop(columns = 'amount', inplace = True)

X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason why I check this:&lt;/p&gt;
&lt;p&gt;For non-fraud transactions, the average amount is 88. For fraud transactions, the average amount is 122.
So, in average there will be 122 loss for a fraud. Suppose for each transaction, the company can get 2% transaction fee. 
That is, the average is 88 * 2% = 1.76. &lt;/p&gt;
&lt;p&gt;That means: if we predict a non-fraud as fraud, we might loss 1.76. However, if we miss to detect a fraud transaction, we will
loss about 122.&lt;/p&gt;
&lt;p&gt;Later I will use this to build a customed loss function.&lt;/p&gt;
&lt;h1&gt;Modeling Part 2: RandomForestClassifier&lt;/h1&gt;
&lt;p&gt;Usually for imbalanced data, we can try:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Collect more data (which not work here since the data is given)
2. Down-Sampling or Over-Sampling to get balanced samples
3. Change the Thresholds to adjust the prediction
4. Assign class weights for the low rate class
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we will try 4 different ways and compare their results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2.1. Do nothing, use original data to model
2.2. Do Over-Sampling, use the over-sampled data to model
2.3. Assigning sample weights in RandomForestClassifier
2.4. Use customed loss function
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is Fraud detection question, if we miss predicting a fraud, the credit company will lose a lot. If we miss predicting a normal transaction as Fraud, we can still let the exprt to review the transactions or we can ask the user to verify the transaction. So in this specific case, False Positive will cause more loss than False Negative.  &lt;/p&gt;
&lt;h1&gt;1. Use the Imbalanced Data Directly in RandomForestClassifier&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain, ytrain)

# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&amp;quot;\t%s: %r&amp;quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.65
print(precision_score(ytest, pred_test))  # 0.85
print(roc_auc_score(ytest, pred_test))    # 0.83
print(&amp;quot;confustion matrix on validation data: \n&amp;quot; + str(confusion_matrix(ytest, pred_test)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;confustion matrix on validation data: &lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[93807    18]
 [   57   105]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we use the imbalanced data directly in the RandomForestClassifier, we will find the result is not very good: recall score is 0.65 and the auc = 0.83. Although this result is better than the result from Logistic Regression using imbalanced data directly. To improve the model performance, we will try two methods: over-sampling and assigning more weights to rare class.&lt;/p&gt;
&lt;h1&gt;2. Create Over-sampling data and Fit the model&lt;/h1&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain_oversample, ytrain_oversample)

# print cv_grid.best_params_
# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&amp;quot;\t%s: %r&amp;quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.83
print(precision_score(ytest, pred_test))  # 0.83
print(roc_auc_score(ytest, pred_test))    # 0.92
print(&amp;quot;\n confustion matrix on validation data: \n&amp;quot; + str(confusion_matrix(ytest, pred_test)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[93798    27]
 [   27   135]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;By using over-sampling, we can find the model performance is improved a lot. recall score = 0.83 now and the auc = 0.92
From the confusion matrix, 135 frauds from 162 True frauds are detected. There are 27 non-frauds are mistakenly predicted
as frauds. We can do fine-tuning by changeing the thresholds to get less false negatives. The price will be getting more
false positives.&lt;/p&gt;
&lt;p&gt;Next we will test using class_weights rather than over-sampling. We know if Logistic Regression these two are equivalent.
We will have a try to see what will happen for RandomForest?&lt;/p&gt;
&lt;h1&gt;3. RandomForestClassifier with class_weight&lt;/h1&gt;
&lt;p&gt;Rather than over-sampling, we can assign more weights to the lower rate class.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)

positive_weight = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1

estimator = RandomForestClassifier(random_state=0, class_weight = {0 : 1, 1 : positive_weight}, warm_start = True)

rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain, ytrain)

# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&amp;quot;\t%s: %r&amp;quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     #  0.85
print(precision_score(ytest, pred_test))  #  0.81
print(roc_auc_score(ytest, pred_test))    #  0.92
print(&amp;quot;\n confustion matrix on validation data: \n&amp;quot; + str(confusion_matrix(ytest, pred_test)))

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[93793    32]
 [   25   137]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compared with over-sampling, the recall score increased from 0.83 to 0.85, while the precision decreases from 0.83 to 0.81.
The auc is 0.92, which is close the the result of over-sampling.&lt;/p&gt;
&lt;p&gt;Overall, I think this model works well: 85% of the frauds can be detected by this model, which will prevent a lot of loss.
At the same time, only 0.03% of the non-frauds will be mistakenly predicted as frauds. This will result in very little potential
loss for the company. The company can also do manual review of these false fraud detections.&lt;/p&gt;
&lt;h1&gt;4. Self-defined Score and GridSearchCV of hyperparameter&lt;/h1&gt;
&lt;p&gt;Since the loss from frauds and false predicted frauds are different for us. We will define a function to re-weight the 
effects by average loss from missing predicted frauds and falsely predicted frauds.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def scoring(ground_truth, predictions):
    '''
    based on results above about the average loss from false positive and false negative predictions.
    '''
    cmatrix = confusion_matrix(ground_truth, predictions)
    fp = cmatrix[0, 1]
    fn = cmatrix[1, 0]
    return  fn * 122 + fp * 1.76

wt_loss_score = make_scorer(scoring, greater_is_better = False)

oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 
                       'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = wt_loss_score, verbose = 5, n_jobs = 70)
cv_grid.fit(Xtrain_oversample, ytrain_oversample)

# print cv_grid.best_params_

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.84
print(precision_score(ytest, pred_test))  # 0.84
print(roc_auc_score(ytest, pred_test))    # 0.92
print(&amp;quot;\n confustion matrix on validation data: \n&amp;quot; + str(confusion_matrix(ytest, pred_test)))

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;With the self defined loss function, the confusion matrix is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[93800    25]
 [   26   136]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Compared to the same setup but using 'roc_auc' as the scoring function results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[[93798    27]
 [   27   135]]
&lt;/code&gt;&lt;/pre&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category></entry><entry><title>Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression</title><link href="/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/" rel="alternate"></link><published>2018-05-05T18:08:00-05:00</published><updated>2018-05-05T18:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-05-05:/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/</id><summary type="html">&lt;p&gt;&lt;a href="https://www.kaggle.com/mlg-ulb/creditcardfraud"&gt;It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase&lt;/a&gt;. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced.&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Table of Contents&lt;span class="tocSkip"&gt;&lt;/span&gt;&lt;/h1&gt;
&lt;div class="toc"&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#0.-Introduction" data-toc-modified-id="0.-Introduction-1"&gt;0. Introduction&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.-Read-in-data-and-the-libraries" data-toc-modified-id="1.-Read-in-data-and-the-libraries-2"&gt;1. Read in data and the libraries&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Modeling-Part-I:-Logistic-Regression-method" data-toc-modified-id="Modeling-Part-I:-Logistic-Regression-method-3"&gt;Modeling Part I: Logistic Regression method&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Preprocess-data" data-toc-modified-id="Preprocess-data-3.1"&gt;Preprocess data&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.-Model-imbalanced-data-directly" data-toc-modified-id="1.-Model-imbalanced-data-directly-4"&gt;1. Model imbalanced data directly&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#1.1.-Change-the-Thresholds" data-toc-modified-id="1.1.-Change-the-Thresholds-4.1"&gt;1.1. Change the Thresholds&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#2.-Create-Over-sampling-data-and-Fit-the-model" data-toc-modified-id="2.-Create-Over-sampling-data-and-Fit-the-model-5"&gt;2. Create Over-sampling data and Fit the model&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#2.1-Change-the-Thresholds" data-toc-modified-id="2.1-Change-the-Thresholds-5.1"&gt;2.1 Change the Thresholds&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#3.-Logistic-Regression-with-class_weight" data-toc-modified-id="3.-Logistic-Regression-with-class_weight-6"&gt;3. Logistic Regression with class_weight&lt;/a&gt;&lt;/span&gt;&lt;ul class="toc-item"&gt;&lt;li&gt;&lt;span&gt;&lt;a href="#Reference" data-toc-modified-id="Reference-6.1"&gt;Reference&lt;/a&gt;&lt;/span&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;

&lt;div id="toc"&gt;&lt;/div&gt;

&lt;h1&gt;0. Introduction&lt;/h1&gt;
&lt;p&gt;It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.&lt;/p&gt;
&lt;p&gt;The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.&lt;/p&gt;
&lt;p&gt;It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.&lt;/p&gt;
&lt;h1&gt;1. Read in data and the libraries&lt;/h1&gt;
&lt;p&gt;The data is in good shape, that is, there is no missing. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Data Clean (missing impute / outliers / normalize)&lt;/li&gt;
&lt;li&gt;Feature Engineering (Categorical variables, transformation, correlation analysis)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The provided data is imbalanced, with positive rate around 0.17%.&lt;/p&gt;
&lt;p&gt;If we use this data directly to feed the model, the model will prefer to predict all as 0 for a high accuracy of 0 prediction. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, Normalizer, scale
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, log_loss, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve
from sklearn.metrics import make_scorer, precision_score, fbeta_score, f1_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedShuffleSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression

%matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)

seed = 999

creditcard = pd.read_csv('creditcard.csv')
creditcard.columns = [x.lower() for x in creditcard.columns]
creditcard.rename(columns = {'class': 'fraud'}, inplace = True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Imbalanced data with very low proportion of positive signals&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;creditcard.fraud.value_counts(dropna = False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0    284315
1       492
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;There are 284315 rows (99.8%) with y = 0, and only 492 rows (0.172%) with y = 1. So it is very imbalanced data. &lt;/p&gt;
&lt;p&gt;Usually we have these methods to deal with imbalanced data:
1. Collect more data
2. Over-Sampling or Down-Sampling
3. Change the prediction thresholds
4. Assign weights&lt;/p&gt;
&lt;p&gt;Here we will do two things: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Use LogisticRegression directly to model the data; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Over-sampling the data to get a balanced proportion of positive/negative values&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Before oversampling, we will first take a random sample as Test data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;creditcard.groupby('fraud').amount.mean()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;fraud
0     88.291022
1    122.211321
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The reason why I check this:&lt;/p&gt;
&lt;p&gt;For non-fraud transactions, the average amount is 88. For fraud transactions, the average amount is 122.
So, in average there will be 122 loss for a fraud. Suppose for each transaction, the company can get 2% transaction fee. 
That is, the average is 88*2% = 1.76. &lt;/p&gt;
&lt;p&gt;That means: if we predict a non-fraud as fraud, we might loss 1.76. However, if we miss to detect a fraud transaction, we will
loss about 122.&lt;/p&gt;
&lt;p&gt;Later I will use this to build a self-defined loss function.&lt;/p&gt;
&lt;h1&gt;Modeling Part I: Logistic Regression method&lt;/h1&gt;
&lt;p&gt;Usually for imbalanced data, we can try:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. Collect more data (which not work here since the data is given)
2. Down-Sampling or Over-Sampling to get balanced samples
3. Change the Thresholds to adjust the prediction
4. Assign class weights for the low rate class
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we will try 5 different ways and compare their results:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;2.1. Do nothing, use original data to model
2.2. Do Over-Sampling, use the over-sampled data to model
2.3. Change the threshold to selected value, rather than using default 0.5
2.4. Assigning sample weights in Logistic Regression
2.5. Change the performance metric, like using ROC, f1-score rather than using accuracy
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this is Fraud detection question, if we miss predicting a fraud, the credit company will lose a lot. If we miss predicting a normal transaction as Fraud, we can still let the exprt to review the transactions or we can ask the user to verify the transaction. So in this specific case, False Positive will cause more loss than False Negative.  &lt;/p&gt;
&lt;h2&gt;Preprocess data&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 1. Split Test Data Out
creditcard.drop(columns = 'time', inplace = True)

# Normalize the 'amount' column
scaler = StandardScaler()
creditcard['amount'] = scaler.fit_transform(creditcard['amount'].values.reshape(-1, 1))
# creditcard.drop(columns = 'amount', inplace = True)

X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)
&lt;/code&gt;&lt;/pre&gt;
&lt;h1&gt;1. Model imbalanced data directly&lt;/h1&gt;
&lt;p&gt;We will use the imbalanced data directly in logistic regression. That is, the positive rate is about 0.172%. Accuracy is not good since if all predicted as 0, the accuracy for 0 is very high. So, here &lt;code&gt;recall&lt;/code&gt;, &lt;code&gt;precision&lt;/code&gt;, &lt;code&gt;roc&lt;/code&gt; and &lt;code&gt;confusion_matrix&lt;/code&gt; are listed to compare model performance.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 2. If we don't do Over-sampling, what will happen?
X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)

logitreg_parameters = {'C': np.power(10.0, np.arange(-3, 3))}
logitreg = LogisticRegression(verbose = 3, warm_start = True)
logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = 'roc_auc', n_jobs = 70)

logitreg_grid.fit(Xtrain, ytrain)   # logitreg_grid.best_params_ ; logitreg_grid.best_estimator_

# on OVER-Sampled TRAINing data
print(&amp;quot;\n Thre recall score on Training data is:&amp;quot;)
print recall_score(ytrain, logitreg_grid.predict(Xtrain))    # 0.58
print(&amp;quot;\n Thre precision score on Training data is:&amp;quot;)
print precision_score(ytrain, logitreg_grid.predict(Xtrain))   # 0.89

# on the separated TEST data
print(&amp;quot;\n Thre recall score on Test data is:&amp;quot;)
print recall_score(ytest, logitreg_grid.predict(Xtest))   # 0.58
print(&amp;quot;\n Thre precision score on Test data is:&amp;quot;)
print precision_score(ytest, logitreg_grid.predict(Xtest))   # 0.86
print(&amp;quot;\n Thre Confusion Matrix on Test data is:&amp;quot;)
print confusion_matrix(ytest, logitreg_grid.predict(Xtest))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[LibLinear]
 Thre recall score on Training data is:
0.584848484848

 Thre precision score on Training data is:
0.893518518519

 Thre recall score on Test data is:
0.586419753086

 Thre precision score on Test data is:
0.863636363636

 Thre Confusion Matrix on Test data is:
[[93810    15]
 [   67    95]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusions:&lt;/p&gt;
&lt;p&gt;From the output above, we know on the training data, the recall score is 0.58 which means 58 over 100 of the 
True positive conditions are predicted correctly. And 89 over 100 of the predicted positives are True Positive.&lt;/p&gt;
&lt;p&gt;On the Test data, the model performance metric evalued by recall or precision are close to the Training data.
There is a precision score of 0.86 on the Test data, which means 86 out of 100 predicted positives are True positives.&lt;/p&gt;
&lt;p&gt;From Confusion Matrix, 95 of 162 True Positives are predicted as positives. And of all 110 predicted as positive, 95 of 
them are True positives.&lt;/p&gt;
&lt;h2&gt;1.1. Change the Thresholds&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ytrain_pred_probas = logitreg_grid.predict_proba(Xtrain)[:, 1]   # prob of predict as 1
fpr, tpr, thresholds = roc_curve(ytrain, ytrain_pred_probas)   # precision_recall_curve
roc = pd.DataFrame({'FPR':fpr,'TPR':tpr,'Thresholds':thresholds})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;_ = plt.figure()
plt.plot(roc.FPR, roc.TPR)
plt.axvline(0.1, color = '#00C851', linestyle = '--')
plt.xlabel(&amp;quot;FPR&amp;quot;)
plt.ylabel(&amp;quot;TPR&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20180505_creditCardFraudDetection_01.png"&gt;&lt;/p&gt;
&lt;p&gt;By default, the threshold is 0.5. Since the recall score is low, we shall lower the threshold to get more predicted as Positive. 
At the same time, more True Negative data will be falsely predicted as Positive. So the Precision score will be lower.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ytest_pred_probas = logitreg_grid.predict_proba(Xtest)[:, 1]
new_threshold = 0.1   # 0.5 is the default value
ytest_pred = (ytest_pred_probas &amp;gt;= new_threshold).astype(int)

print(&amp;quot;After change threshold to 0.1, the recall socre on Test data is:&amp;quot;)
print recall_score(ytest, ytest_pred)     # 0.827
print(&amp;quot;\n After change threshold to 0.1, the precision socre on Test data is:&amp;quot;)
print precision_score(ytest, ytest_pred)  # 0.812
print(&amp;quot;\nAfter change threshold to 0.1, the Confusion Matrix on Test data is:&amp;quot;)
print confusion_matrix(ytest, ytest_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;After change threshold to 0.1, the recall socre on Test data is:
0.827160493827

 After change threshold to 0.1, the precision socre on Test data is:
0.812121212121

After change threshold to 0.1, the Confusion Matrix on Test data is:
[[93794    31]
 [   28   134]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we lower the threshold to 0.1, we will get recall rate of 0.827. That is, 134 of 162 True Frauds will be detected while
only 31 on 165 predicted Frauds are not True Frauds.&lt;/p&gt;
&lt;p&gt;If we lower the threshold to 0.01, then the recall score will be 0.91 while the precision score is 0.1.&lt;/p&gt;
&lt;h1&gt;2. Create Over-sampling data and Fit the model&lt;/h1&gt;
&lt;p&gt;Since there are much more samples &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50

logitreg_parameters = {'C': np.power(10.0, np.arange(-3, 3))}
logitreg = LogisticRegression(verbose = 3, warm_start = True)
logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = 'roc_auc', n_jobs = 70)

logitreg_grid.fit(Xtrain_oversample, ytrain_oversample)   # logitreg_grid.best_params_ ; logitreg_grid.best_estimator_

# on OVER-Sampled TRAINing data
print(&amp;quot;\n After Over-Sampling, the recall score on Training data is&amp;quot;)
print recall_score(ytrain_oversample, logitreg_grid.predict(Xtrain_oversample))    # 0.918
print(&amp;quot;\n After Over-Sampling, the precision score on Training data is&amp;quot;)
print precision_score(ytrain_oversample, logitreg_grid.predict(Xtrain_oversample))   # 0.971

# on the separated TEST data
print(&amp;quot;\n After Over-Sampling, the recall score on Test data is&amp;quot;)
print recall_score(ytest, logitreg_grid.predict(Xtest))   #  0.932
print(&amp;quot;\n After Over-Sampling, the precision score on Test data is&amp;quot;)
print precision_score(ytest, logitreg_grid.predict(Xtest))   # 0.056
print(&amp;quot;\n After Over-Sampling, the Confusion Matrix on Test data is&amp;quot;)
print confusion_matrix(ytest, logitreg_grid.predict(Xtest))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[LibLinear]
 After Over-Sampling, the recall score on Training data is
0.918181818182

 After Over-Sampling, the precision score on Training data is
0.971089227494

 After Over-Sampling, the recall score on Test data is
0.932098765432

 After Over-Sampling, the precision score on Test data is
0.0562383612663

 After Over-Sampling, the Confusion Matrix on Test data is
[[91291  2534]
 [   11   151]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the output above, we know on the training data, the recall score is 0.918 which means 91.8 over 100 of the  True conditions are predicted correctly. And 97 over 100 of the predicted positives are really positive.&lt;/p&gt;
&lt;p&gt;However, there is only a precision score of 0.056 on the Test data, which means only 5.6 out of 100 predicted positives are real positives.&lt;/p&gt;
&lt;p&gt;From Confusion Matrix, 151 of 162 True Positives are predicted as positives. However, the model predicted 2533 Negative data as Positive.&lt;/p&gt;
&lt;p&gt;That is, out model has pretty strong over-fitting.&lt;/p&gt;
&lt;h2&gt;2.1 Change the Thresholds&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ytrain_pred_probas = logitreg_grid.predict_proba(Xtrain)[:, 1]
fpr, tpr, thresholds = roc_curve(ytrain, ytrain_pred_probas)   # precision_recall_curve
roc = pd.DataFrame({'FPR':fpr,'TPR':tpr,'Thresholds':thresholds})

_ = plt.figure()
plt.plot(roc.FPR, roc.TPR)
plt.axvline(0.2, color = '#00C851', linestyle = '--')
plt.xlabel(&amp;quot;FPR&amp;quot;)
plt.ylabel(&amp;quot;TPR&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20180505_creditCardFraudDetection_02.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ytest_pred_probas = logitreg_grid.predict_proba(Xtest)[:, 1]
new_threshold = 0.2
ytest_pred = (ytest_pred_probas &amp;gt;= new_threshold).astype(int)

print(&amp;quot;After change threshold to 0.2, the recall socre on Test data is:&amp;quot;)
print recall_score(ytest, ytest_pred)
print(&amp;quot;\n After change threshold to 0.2, the precision socre on Test data is:&amp;quot;)
print precision_score(ytest, ytest_pred)
print(&amp;quot;\n After change threshold to 0.2, the Confusion Matrix on Test data is:&amp;quot;)
print confusion_matrix(ytest, ytest_pred)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;After change threshold to 0.2, the recall socre on Test data is:
0.956790123457

 After change threshold to 0.2, the precision socre on Test data is:
0.016130710792

 After change threshold to 0.2, the Confusion Matrix on Test data is:
[[84371  9454]
 [    7   155]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Conclusion: After over-sampling, the model will have higher recall rate. That is, the model will work better on detect the Frauds from True Frauds. The price we paid is the lower precision rate.&lt;/p&gt;
&lt;h1&gt;3. Logistic Regression with class_weight&lt;/h1&gt;
&lt;p&gt;Rather than over-sampling, we can assign more weights to the lower rate class. In fact, if you write out the Likelihood function for Logistic Regression, the Over-Sampling and the assigning more Weights will be equivalent.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;positive_weight = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1

logitreg_parameters = {'C': np.power(10.0, np.arange(-3, 3))}
logitreg = LogisticRegression(class_weight = {0 : 1, 1 : positive_weight}, verbose = 3, warm_start = True)
logitreg_grid = GridSearchCV(logitreg, param_grid = logitreg_parameters, scoring = 'roc_auc', n_jobs = 70)

logitreg_grid.fit(Xtrain, ytrain)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[LibLinear]


GridSearchCV(cv=None, error_score='raise',
       estimator=LogisticRegression(C=1.0, class_weight={0: 1, 1: 577}, dual=False,
          fit_intercept=True, intercept_scaling=1, max_iter=100,
          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,
          solver='liblinear', tol=0.0001, verbose=3, warm_start=True),
       fit_params=None, iid=True, n_jobs=70,
       param_grid={'C': array([  1.00000e-03,   1.00000e-02,   1.00000e-01,   1.00000e+00,
         1.00000e+01,   1.00000e+02])},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring='roc_auc', verbose=0)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(&amp;quot;\n After assign class_weight, the recall score on Training data is&amp;quot;)
print recall_score(ytrain_oversample, logitreg_grid.predict(Xtrain_oversample))    # 0.912
print(&amp;quot;\n After assign class_weight, the precision score on Training data is&amp;quot;)
print precision_score(ytrain_oversample, logitreg_grid.predict(Xtrain_oversample))   # 0.972

# on the separated TEST data
print(&amp;quot;\n After assign class_weight, the recall score on Test data is&amp;quot;)
print recall_score(ytest, logitreg_grid.predict(Xtest))   #  0.932
print(&amp;quot;\n After assign class_weight, the precision score on Test data is&amp;quot;)
print precision_score(ytest, logitreg_grid.predict(Xtest))   # 0.058
print(&amp;quot;\n After assign class_weight, the Confusion Matrix on Test data is&amp;quot;)
print confusion_matrix(ytest, logitreg_grid.predict(Xtest))
print(&amp;quot;\n After assign class_weight, the ROC AUC Score on Test data is&amp;quot;)
print roc_auc_score(ytest, logitreg_grid.predict(Xtest))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; After assign class_weight, the recall score on Training data is
0.912121212121

 After assign class_weight, the precision score on Training data is
0.972226568612

 After assign class_weight, the recall score on Test data is
0.932098765432

 After assign class_weight, the precision score on Test data is
0.0581888246628

 After assign class_weight, the Confusion Matrix on Test data is
[[91381  2444]
 [   11   151]]

 After assign class_weight, the ROC AUC Score on Test data is
0.953025135447
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we set up the class weight for the positive as the ratio of non-Fraud / Fraud, we will get the result close to the over-sampling.&lt;/p&gt;
&lt;p&gt;So, in summary:
This specific data is about fraud detection. So the model should focus on to find the frauds
to avoid potential loss for the bank. That is, we should focus on RECALL rate. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If we use the imbalanced data directly, we will get low performance model since the model prefer to predict to the class with dominated frequency class. The recall rate is 0.58. That is, only 58% of the frauds can be detected by this model.&lt;/li&gt;
&lt;li&gt;To fix that, one way is to do over-sampling or down-sampling. If we use over-sampling, the model performance will be  improved a lot. For this specific case, the recall rate on the independent test set will be improved from 0.58 to 0.932&lt;/li&gt;
&lt;li&gt;Another way to improve the model performance is to assign more weights to the low frequency class. Generally speaking, for Logistic Regression, assigning weights is similar to over-sampling, from the likelihood function perspective. The final output results are close too as demonstrated above.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.kaggle.com/mlg-ulb/creditcardfraud"&gt;Credit Card Fraud Detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://bitbucket.org/songhuiming/test/src/master/test1/20180510_CreditCardFraudDetection.py"&gt;CreditCardFraudDetection.py&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category></entry><entry><title>241. Different Ways to Add Parentheses</title><link href="/pages/2018/04/08/241-different-ways-to-add-parentheses/" rel="alternate"></link><published>2018-04-08T00:00:00-05:00</published><updated>2018-04-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-04-08:/pages/2018/04/08/241-different-ways-to-add-parentheses/</id><summary type="html">&lt;p&gt;题目大意，给定一个数学表达式，在表达式的不同地方添加括号来表示不同的运算顺序，求最后有多少种不同的运算结果&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/different-ways-to-add-parentheses/description/"&gt;241. Different Ways to Add Parentheses&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意，给定一个数学表达式，在表达式的不同地方添加括号来表示不同的运算顺序，求最后有多少种不同的运算结果。比如给定字符串'2-1-1'，可以有两种添加括号的办法，'(2-1)-1' 和 '2-(1-1)'，结果为[0, 2]。&lt;/p&gt;
&lt;p&gt;这个题目和word break有些类似，都是记忆化递归的题目。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/word-break/description/"&gt;139. Word Break&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/word-break-ii/description/"&gt;140. Word Break II&lt;/a&gt; &lt;/p&gt;
&lt;p&gt;本题主要的分为下面三步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;找到位置来分割字符串。在这里是运算符号 +, -, * . 在word break里面是预先给定的的单词。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对分割出来的左右两个子表达式，分别递归求解子表达式的所有的可能解。子表达式可能会重复出现，所以要用记忆体来记住已经求解过的表达式的值，这样可以节省时间。在word break II中是递归验证子表达式是不是在给定的单词集合里面。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;对两个子表达式的值的集合，根据给定的运算符号，计算笛卡尔积&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;这个题目要注意的是，在递归求解子问题的时候，如果子问题中没有运算符了，比如helper('12')，应该返回'12'的整型值。否则不管怎么迭代，结果都是空集。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution:
    def diffWaysToCompute(self, input):
        self.res = {}
        def helper(input):
            n = len(input)
            if input in self.res:
                return self.res[input]
            ans = []
            for i in range(1, n):
                op = input[i]
                if op in ('+', '-', '*'):
                    left = helper(input[:i])
                    right = helper(input[(i+1):])
                    for s in left:
                        for k in right:
                            if op == &amp;quot;+&amp;quot;:
                                ans.append(s + k)
                            elif op == &amp;quot;-&amp;quot;:
                                ans.append(s - k)
                            elif op == &amp;quot;*&amp;quot;:
                                ans.append(s * k)
            if ans == []:
                ans = [int(input)]
            self.res[input] = ans
            return self.res[input]
        return helper(input)
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>300. Longest Increasing Subsequence</title><link href="/pages/2018/04/08/300-longest-increasing-subsequence/" rel="alternate"></link><published>2018-04-08T00:00:00-05:00</published><updated>2018-04-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-04-08:/pages/2018/04/08/300-longest-increasing-subsequence/</id><summary type="html">&lt;p&gt;题目大意，给定一个无序的数列，求这个序列的最长单调增的子序列的长度。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/longest-increasing-subsequence/description/"&gt;300. Longest Increasing Subsequence&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意，给定一个无序的数列，求这个序列的最长单调增的子序列的长度。比如给定&lt;code&gt;[10, 9, 2, 5, 3, 7, 101, 18]&lt;/code&gt;,那么最长递增子序列为&lt;code&gt;[2, 3, 7, 101]&lt;/code&gt; or &lt;code&gt;[2, 3, 7, 18]&lt;/code&gt;。长度都为4.所以最后的答案是4&lt;/p&gt;
&lt;p&gt;解题思路：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;f[i]　表示子序列 [nums[0], ..., nums[i]] 里面的最长递增子序列的长度(包含nums[i])&lt;/p&gt;
&lt;p&gt;f[i] 可以通过递归得到&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;for j in range(i):
    if nums[j] &amp;lt; nums[i]:
        f[i] = max(f[i], f[j] + 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def lengthOfLIS(self, nums):
        n = len(nums)
        if n &amp;lt;= 1:
            return n
        f = [1 for _ in range(n)]
        for i in range(n):
            for j in range(i + 1):
                if nums[j] &amp;lt; nums[i]:
                    f[i] = max(f[i], f[j] + 1)
        return max(f)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这道题目还可以用记忆化递归来解，详细的讲解请参阅&lt;a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-300-longest-increasing-subsequence/"&gt;[解题报告] LeetCode 300. Longest Increasing Subsequence&lt;/a&gt;&lt;/p&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 303. Range Sum Query - Immutable</title><link href="/pages/2018/04/08/leetcode-303-range-sum-query-immutable/" rel="alternate"></link><published>2018-04-08T00:00:00-05:00</published><updated>2018-04-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-04-08:/pages/2018/04/08/leetcode-303-range-sum-query-immutable/</id><summary type="html">&lt;p&gt;这是一道easy的题目，但是有很多变种&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/range-sum-query-immutable/description/"&gt;303. Range Sum Query - Immutable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这是一道easy的题目。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class NumArray(object):
    def __init__(self, nums):
        self.nums = nums     

    def sumRange(self, i, j):
        return sum(self.nums[i:(j+1)])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def sunRange(nums, i, j):
    sum = 0
    for x in range(i, j +1):
        sum += nums[x]
    return sum

def sunRange(nums, i, j):
    sum[0] = nums[0]
    for x in range(1, len(nums)):
        sum[x] = sum[x - 1] + nums[x]
    return sum[j] - sum[i - 1]
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 304. Range Sum Query 2D - Immutable</title><link href="/pages/2018/04/08/leetcode-304-range-sum-query-2d-immutable/" rel="alternate"></link><published>2018-04-08T00:00:00-05:00</published><updated>2018-04-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-04-08:/pages/2018/04/08/leetcode-304-range-sum-query-2d-immutable/</id><summary type="html">&lt;p&gt;题目大意，给定一个二维矩阵，再给定左上角和右下角的坐标，求坐标围成的矩形的元素的和&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/range-sum-query-2d-immutable/description/"&gt;304. Range Sum Query 2D - Immutable&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意，给定一个二维矩阵，再给定左上角和右下角的坐标，求坐标围成的矩形的元素的和&lt;/p&gt;
&lt;p&gt;解题思路： 这个题目和前面的&lt;a href="https://leetcode.com/problems/maximal-square/description/"&gt;221. Maximal Square&lt;/a&gt;类似。把矩形围成的区域表示成以(0, 0)为初始点的矩形的和的运算。这样在运算部分时间复杂度就是O(1).&lt;/p&gt;
&lt;p&gt;在求解(0, 0)开始的矩形的元素和的时候，使用递推/动态规划。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class NumMatrix(object):

    def __init__(self, matrix):
        &amp;quot;&amp;quot;&amp;quot;
        :type matrix: List[List[int]]
        &amp;quot;&amp;quot;&amp;quot;
        m = len(matrix)
        if m == 0:
            return
        n = len(matrix[0])
        sum_ = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        # padding sum_ to make sum calculation easier
        for i in range(1, m+1):
            for j in range(1, n+1):
                sum_[i][j] = sum_[i-1][j] + sum_[i][j-1] - sum_[i-1][j-1] + matrix[i-1][j-1]
        self.sum_ = sum_        

    def sumRegion(self, row1, col1, row2, col2):
        &amp;quot;&amp;quot;&amp;quot;
        :type row1: int
        :type col1: int
        :type row2: int
        :type col2: int
        :rtype: int
        &amp;quot;&amp;quot;&amp;quot;
        return self.sum_[row2 + 1][col2 + 1] - self.sum_[row1 - 1 + 1][col2 + 1] \
            - self.sum_[row2 + 1][col1 - 1 + 1] + self.sum_[row1 - 1 + 1][col1 - 1 + 1]

# Your NumMatrix object will be instantiated and called as such:
# obj = NumMatrix(matrix)
# param_1 = obj.sumRegion(row1,col1,row2,col2)
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 309. Best Time to Buy and Sell Stock with Cooldown</title><link href="/pages/2018/04/08/leetcode-309-best-time-to-buy-and-sell-stock-with-cooldown/" rel="alternate"></link><published>2018-04-08T00:00:00-05:00</published><updated>2018-04-08T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-04-08:/pages/2018/04/08/leetcode-309-best-time-to-buy-and-sell-stock-with-cooldown/</id><summary type="html">&lt;p&gt;题目大意：给定一个数组，第i个元素表示第i天的股票价格。每天只能最多一次买或者卖(必须有股票才能卖)。卖了以后第二天不能买(cooldown)。求最大的利润。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/description/"&gt;309. Best Time to Buy and Sell Stock with Cooldown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：给定一个数组，第i个元素表示第i天的股票价格。每天只能最多一次买或者卖(必须有股票才能卖)。卖了以后第二天不能买(cooldown)。求最大的利润。&lt;/p&gt;
&lt;p&gt;解题思路：&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0309_best_time_to_buy_and_sell_stock_with_cooldown.svg"&gt;&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxProfit(self, prices):
        sold = 0
        rest = 0
        hold = -float('inf')
        for price in prices:
            prev_sold = sold
            sold = hold + price
            hold = max(hold, rest - price)
            rest = max(rest, prev_sold)
        return max(rest, sold)
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 139. Word Break</title><link href="/pages/2018/03/18/leetcode-139-word-break/" rel="alternate"></link><published>2018-03-18T00:00:00-05:00</published><updated>2018-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-18:/pages/2018/03/18/leetcode-139-word-break/</id><summary type="html">&lt;p&gt;题目大意：给定一个字符串和一个字典，查看字符串能不能由字典里的单词构成。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/word-break/description/"&gt;139. Word Break&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：给定一个字符串和一个字典，查看字符串能不能由字典里的单词构成。&lt;/p&gt;
&lt;p&gt;解题思路：假如前&lt;span class="math"&gt;\(i\)&lt;/span&gt;个字符串(左半部分)可以被字典里的单词分解，从&lt;span class="math"&gt;\(i\)&lt;/span&gt;字符串后面的字符串(右半部分)也在字典里，那么这个字符串就有解。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;p&gt;假如前&lt;span class="math"&gt;\(i\)&lt;/span&gt;个字符串(左半部分)可以被字典里的单词分解，从&lt;span class="math"&gt;\(i\)&lt;/span&gt;字符串后面的字符串(右半部分)也在字典里，那么这个字符串就有解。
&lt;img src="/figures/leetcode_139_word_break_01.svg"&gt;&lt;/p&gt;
&lt;p&gt;如果string没有padding，因为f有padding，那么f[j]对应的应该是s[j-1]的位置.所以新的string应该从j位置开始。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def wordBreak(self, s, wordDict):
    n = len(s)
    # padding
    f = [0 for _ in range(n + 1)]
    f[0] = 1
    for i in range(1, n + 1):
        for j in range(0, i):
            if (f[j] == 1):
                new_s = s[j:i]  #s[j, ..., i-1]
                # python slice diff from c++
                if new_s in wordDict:
                    f[i] = 1
                    break
    return f[n]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果string前面有 space padding，那么f和s都是从同样的位置开始。要注意的是python的slices[j:i]的话i是不包含的。如果要包含i，那么要用slices[j:i+1]&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def wordBreak(self, s, wordDict):
    n = len(s)
    s = ' ' + s
    # padding
    f = [False for _ in range(n + 1)]
    f[0] = True
    for i in range(1, n + 1):
        for j in range(0, i):
            if (f[j] == True):
                new_s = s[(j+1):(i+1)]  #s[j+1, ..., i]
                if new_s in wordDict:
                    f[i] = True
                    break
    return f[n]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;revisit&lt;/h2&gt;
&lt;p&gt;记忆化递归，把前面的结果存在mem_里面
&lt;img src="/figures/leetcode_139_word_break_02.svg"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def wordBreak(self, s, wordDict):
    self.mem_ = {}
    def dfs(s, wordDict):
        # recursion termination conditions
        # 1. if s already calculated in mem_, dont need to recalculate
        if s in self.mem_:
            return self.mem_[s]
        # 2. if s is in wordDict, then it is true
        if s in wordDict:
            self.mem_[s] = True
            return True
        # try every break porint, break to left and right
        for j in range(1, len(s)):
            left = s[:j]
            right = s[j:]
            if right in wordDict and dfs(left, wordDict):
                self.mem_[s] = True
                return True
        self.mem_[s] = False
        return False
    return dfs(s, wordDict)
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 140. Word Break II</title><link href="/pages/2018/03/18/leetcode-140-word-break-ii/" rel="alternate"></link><published>2018-03-18T00:00:00-05:00</published><updated>2018-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-18:/pages/2018/03/18/leetcode-140-word-break-ii/</id><summary type="html">&lt;p&gt;题目大意：单词分割，给定一个字符串和一个字典，如果字符串能由字典里的单词组成，那么返回组成的单词。最后要求返回所有的可能的解。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/word-break-ii/description/"&gt;140. Word Break II&lt;/a&gt;是一道难度为hard的题，不太好写。&lt;/p&gt;
&lt;p&gt;题目大意：单词分割，给定一个字符串和一个字典，如果字符串能由字典里的单词组成，那么返回组成的单词。最后要求返回所有的可能的解。&lt;/p&gt;
&lt;p&gt;跟&lt;a href="https://leetcode.com/problems/word-break/description/"&gt;139. Word Break&lt;/a&gt;不同的是，这道题要找到解，而且要找出所有的解。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_140_word_break_ii_01.svg"&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;对每个分割点，如果右边在字典里，同时如果左边有解，那么这个分割有效&lt;/p&gt;
&lt;p&gt;对左边采取同样的办法，先分割成左右，然后看右边是不是在字典里，在的话再检查左边&lt;/p&gt;
&lt;p&gt;如此迭代下去&lt;/p&gt;
&lt;p&gt;注意的是左边可能不止一个分割办法，最要要把右边跟左边的所有可能解加在一起，这样才能找到所有的解&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def wordBreak(self, s, wordDict):
    self.mem_ = {}

    def dfs(s, wordDict):
        if s in self.mem_:
            return self.mem_[s]
        ans = [] # answer for curr s
        if s in wordDict:
            ans.append(s)
        for j in range(1, len(s)):
            right = s[j:]
            if right not in wordDict:
                continue
            left = s[:j]
            left_ans = [x + ' ' + right for x in dfs(left, wordDict)]
            ans += left_ans
        self.mem_[s] = ans
        return self.mem_[s]
    return dfs(s, wordDict)

Solution().wordBreak(&amp;quot;catsanddog&amp;quot;, ['cat', 'cats', 'and', 'sand', 'dog'])
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 174. Dungeon Game</title><link href="/pages/2018/03/18/leetcode-174-dungeon-game/" rel="alternate"></link><published>2018-03-18T00:00:00-05:00</published><updated>2018-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-18:/pages/2018/03/18/leetcode-174-dungeon-game/</id><summary type="html">&lt;p&gt;题目大意：骑士在左上角&lt;code&gt;(0, 0)&lt;/code&gt;,公主在右下角&lt;code&gt;(m, n)&lt;/code&gt;。骑士每次只能向右或者向下，每个方格表示骑士消耗获得的hp，题目要求骑士出发时候所需要的最小hp。在任何位置骑士的hp都要大于0&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/dungeon-game/description/"&gt;174. Dungeon Game&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：骑士在左上角&lt;code&gt;(0, 0)&lt;/code&gt;,公主在右下角&lt;code&gt;(m, n)&lt;/code&gt;。骑士每次只能向右或者向下，每个方格表示骑士消耗获得的hp，题目要求骑士出发时候所需要的最小hp。在任何位置骑士的hp都要大于0&lt;/p&gt;
&lt;p&gt;解题思路：假设dungeon的shape为(m, n)，我们建立一个shape为(m+1, n+1)的矩阵，index从0开始，最终要在(m-1, n-1)的位置有1个hp。那么按照题目的逻辑，从(m-1, n-1)能继续下去，在(m, n-1)和(m-1,n)都要至少要为1. 然后一步步朝左上角返回上去。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def calculateMinimumHP(self, dungeon):
        m = len(dungeon)
        n = len(dungeon[0])
        hp = [[float('inf') for _ in range(n + 1)] for _ in range(m + 1)]
        hp[m][n - 1] = hp[m - 1][n] = 1
        for y in range(m-1, -1, -1):
            for x in range(n-1, -1, -1):
                hp[y][x] = max(1, min(hp[y + 1][x], hp[y][x + 1]) - dungeon[y][x])
        return hp[0][0]

if __name__ == &amp;quot;__main__&amp;quot;:
    Solution().calculateMinimumHP([[100]])
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 198. House Robber</title><link href="/pages/2018/03/18/leetcode-198-house-robber/" rel="alternate"></link><published>2018-03-18T00:00:00-05:00</published><updated>2018-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-18:/pages/2018/03/18/leetcode-198-house-robber/</id><summary type="html">&lt;p&gt;题目大意，一个强盗沿着一排房子盗窃，强盗不能盗连续的两个房子，求强盗能盗窃的最大值&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/house-robber/description/"&gt;198. House Robber&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意，一个强盗沿着一排房子盗窃，强盗不能盗连续的两个房子，求强盗能盗窃的最大值. 比如6个房子，强盗抢劫每个房子得到的值为 nums = [3, 1, 5, 3, 7, 9]。最大获利为 3 + 5 + 9 = 17.&lt;/p&gt;
&lt;p&gt;解题思路：&lt;/p&gt;
&lt;p&gt;这个题如果用brute force直接解的话会超时，brute force每个房子有两个可能，robber or not，所以n个房子的时间复杂度是&lt;span class="math"&gt;\(O(2^n)\)&lt;/span&gt;。直接超时。&lt;/p&gt;
&lt;p&gt;可以用动态规划来解：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假设 d[i] = max money for up to ith home&lt;/p&gt;
&lt;p&gt;那么 d[i] = max(d[i - 1], d[i - 2] + nums[i])&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;这种题目通常可以用记忆化递归，或者递推来解。通常可以进行空间压缩。&lt;/p&gt;
&lt;h2&gt;1. recursion with memorization&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def rob(self, nums):
        n = len(nums)
        res = [-1 for _ in range(n)]
        def dp(nums, i, res):
            if i &amp;lt; 0:
                return 0
            if res[i] &amp;gt;= 0:
                return res[i]
            else:
                res[i] = max(dp(nums, i - 1, res), dp(nums, i - 2, res) + nums[i])
            return res[i]
        return dp(nums, n - 1, res)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 递推/动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def rob(self, nums):
        n = len(nums)
        if n == 0:
            return 0
        if n == 1:
            return nums[0]
        d = [0 for _ in range(n)]
        d[0] = nums[0]
        d[1] = max(nums[0], nums[1])
        for i in range(2, n):
            d[i] = max(d[i - 1], d[i - 2] + nums[i])
        return d[n - 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3. 递推/动态规划 + 压缩空间&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def rob(self, nums):
        n = len(nums)
        if n == 0:
            return 0
        dp1 = 0
        dp2 = 0
        for i in range(n):
            curr = max(dp1, dp2 + nums[i])
            dp2 = dp1
            dp1 = curr
        return curr
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 221. Maximal Square</title><link href="/pages/2018/03/18/leetcode-221-maximal-square/" rel="alternate"></link><published>2018-03-18T00:00:00-05:00</published><updated>2018-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-18:/pages/2018/03/18/leetcode-221-maximal-square/</id><summary type="html">&lt;p&gt;题目大意，给定一个矩阵，元素为0或者1. 求所有元素都为1的最大正方形子矩阵&lt;/p&gt;</summary><content type="html">&lt;p&gt;题目大意，给定一个矩阵，元素为0或者1. 求所有元素都为1的最大正方形子矩阵。把它跟第85题放在一起，&lt;a href="https://leetcode.com/problems/maximal-rectangle/description/"&gt;85题&lt;/a&gt;求最大的长方形子矩阵。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/maximal-square/"&gt;221. Maximal Square&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/maximal-rectangle/description/"&gt;85. Maximal Rectangle&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;详细的讲解，请参考&lt;a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-221-maximal-square/"&gt;[解题报告] LeetCode 221. Maximal Square&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;brute force&lt;/h2&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0221_maximal_square_01_brute_force.svg"&gt;&lt;/p&gt;
&lt;p&gt;对每一行(y)，对每一列(x)，对所有可能的size，检查以(y, x)为顶点，大小为size的区域是不是符合要求。对检查函数必须做优化，否则会TLE.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;for y in range(m):
    for x in range(n):
        for size in range(1, min(n - x, m - y)):
            check(matrix, x, y, x + size, y + size)

matrix = [[1, 0, 1, 0, 0], [1, 0, 1, 1, 1], [1, 1, 1, 1, 1], [1, 0, 0, 1, 0]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;p&gt;对每一行(y)，对每一列(x)，对所有可能的size，检查以(y, x)为顶点，大小为size的区域是不是符合要求。&lt;/p&gt;
&lt;p&gt;设置一个检查函数来降低时间复杂度：如图所示，把面积简化成从(0, 0)开始的几个长方形/正方形的面积的加减。&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0221_maximal_square_02.svg"&gt;&lt;/p&gt;
&lt;p&gt;而求从(0, 0)开始的任意的面积可以使用DP在时间复杂度为&lt;span class="math"&gt;\(O(m * n)\)&lt;/span&gt;的范围内解出来。&lt;/p&gt;
&lt;p&gt;&lt;img src="/figures/leetcode_0221_maximal_square_03.svg"&gt;&lt;/p&gt;
&lt;p&gt;最后的code如下。&lt;span class="math"&gt;\(sums[i][j]\)&lt;/span&gt;表示从&lt;span class="math"&gt;\((0, 0)\)&lt;/span&gt;到&lt;span class="math"&gt;\((i, j)\)&lt;/span&gt;的面积。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maximalSquare(self, matrix):
        m = len(matrix)
        if m == 0:
            return 0
        n = len(matrix[0])

        # sums[i][j] = sum(matrix[0][0] ~ matrix[i-1][j-1])
        sums = [[0 for _ in range(n+1)] for _ in range(m+1)]
        for i in range(1, m+1):
            for j in range(1, n+1):
                sums[i][j] = matrix[i-1][j-1] + sums[i-1][j] + sums[i][j-1] - sums[i-1][j-1]

        ans = 0
        for i in range(1, m+1):
            for j in range(1, n+1):
                for k in range(min(m-i+1, n-j+1), 0, -1):
                    sum = sums[i+k-1][j+k-1] - sums[i+k-1][j-1] - sums[i-1][j+k-1] + sums[i-1][j-1]
                    if sum == k*k:
                        ans = max(ans, sum)
                        break
        return ans
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 113. path sum</title><link href="/pages/2018/03/11/leetcode-113-path-sum/" rel="alternate"></link><published>2018-03-11T00:00:00-06:00</published><updated>2018-03-11T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-11:/pages/2018/03/11/leetcode-113-path-sum/</id><summary type="html">&lt;p&gt;题目大意：　given a binary tree and a sum, find all root-to-leaf paths where each path's sum equals the given sum.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/path-sum-ii/description/"&gt;113. Path Sum II&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/path-sum/description/"&gt;112. Path Sum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：　Given a binary tree and a sum, find all root-to-leaf paths where each path's sum equals the given sum.&lt;/p&gt;
&lt;p&gt;解题思路：把当前要找的sum减去node节点的值作为新的sum的值，然后递归求解左指数，递归求解右指数，直到leaf节点，判断当前剩下的sum跟leaf节点的.val是不是相同。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def pathSum(self, root, sum):
    if not root:
        return []
    self.res = []
    def dfs(node, sum, temp):
        if not node:
            return []
        if not node.left and not node.right:
            if node.val - sum == 0:
                temp.append(node.val)
                self.res.append(temp[:])
                temp.pop()
        temp.append(node.val)
        sum -= node.val
        dfs(node.left, sum, temp)
        dfs(node.right, sum, temp)
        temp.pop()
    dfs(root, sum, [])
    return self.res
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 120. Triangle</title><link href="/pages/2018/03/11/leetcode-120-triangle/" rel="alternate"></link><published>2018-03-11T00:00:00-06:00</published><updated>2018-03-11T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-11:/pages/2018/03/11/leetcode-120-triangle/</id><summary type="html">&lt;p&gt;题目大意：给定一个triangle，找出从上到下的和的最小值，每一步只能找下一行的相连的数字&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/triangle/description/"&gt;120. Triangle&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：给定一个triangle，找出从上到下的和的最小值，每一步只能找下一行的相连的数字&lt;/p&gt;
&lt;p&gt;解题思路：经典动态规划题。假如找到了上一层的每个点的最小值，那么下一层的最小值可以通过上一层最小值得到。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[
[2],
[3,4],
[6,5,7],
[4,1,8,3]
]

[
[2],
[5,6],
[11,10,13],
[15,11,18,16]
]
&lt;/code&gt;&lt;/pre&gt;
&lt;blockquote&gt;
&lt;p&gt;状态：&lt;span class="math"&gt;\(f[i][j]\)&lt;/span&gt;表示走到第&lt;span class="math"&gt;\(i\)&lt;/span&gt;行第&lt;span class="math"&gt;\(j\)&lt;/span&gt;列的最小值&lt;/p&gt;
&lt;p&gt;那么我们有&lt;/p&gt;
&lt;p&gt;转移：&lt;span class="math"&gt;\(f[i][j] = \mbox{min}(f[i - 1][j], f[i - 1][j - 1]) + \mbox{triangle}\)&lt;/span&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def minimumTotal(self, triangle):
    n = len(triangle)
    # f[i][j] = min(f[i - 1][j], f[i - 1][j - 1]) + triangle
    # padding
    f = [[0 for _ in range(n + 1)] for _ in range(n + 1)]
    for i in range(1, n + 1):
        for j in range(1, i + 1):
            f[i][j] = triangle[i - 1][j - 1]
            if i == 1 and j == 1:
                continue
            if j == 1:
                f[i][j] += f[i - 1][j]
            elif j == i:
                f[i][j] += f[i - 1][j - 1]
            else:
                f[i][j] += min(f[i - 1][j], f[i - 1][j - 1])
    return min(f[n][1:])

if __name__ == __main__:
    triangle = [[2], [3, 4], [6, 5, 7], [4, 1, 8, 3]]
    Solution().minimumTotal(triangle)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;因为&lt;span class="math"&gt;\(f[i]\)&lt;/span&gt;只跟&lt;span class="math"&gt;\(f[i-1]\)&lt;/span&gt;有关系，所有可以使用滚动数组来压缩空间。注意要使用deepcopy。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def minimumTotal(self, triangle):
    n = len(triangle)
    # padding
    # f[i][j] = min(f[i - 1][j], f[i - 1][j - 1]) + triangle
    f = [[0 for _ in range(n + 1)] for _ in range(2)]
    for i in range(1, n + 1):
        for j in range(1, i + 1):
            f[1][j] = triangle[i - 1][j - 1]
            if i == 1 and j == 1:
                continue
            if j == 1:
                f[1][j] += f[0][j]
            elif j == i:
                f[1][j] += f[0][j - 1]
            else:
                f[1][j] += min(f[0][j], f[0][j - 1])
        f[0] = f[1][:]
    return min(f[0][1:])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第三种办法，直接修改triangle，这样不需要额外的空间。因为直接修改triangle，所以不需要padding，注意相应的i和j的起始值也要改变。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def minimumTotal(self, triangle):
    n = len(triangle)
    t = triangle
    # t[i][j] = minTotalOf(i, j)
    # t[i][j] += min(t[i - 1][j], t[i - 1][j - 1])
    for i in range(0, n):
        for j in range(0, i + 1):
            if i == 0 and j == 0:
                continue
            if j == 0:
                t[i][j] += t[i - 1][j]
            elif j == i:
                t[i][j] += t[i - 1][j - 1]
            else:
                t[i][j] += min(t[i - 1][j], t[i -1][j - 1])
    return min(t[n - 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 121. Best Time to Buy and Sell Stock</title><link href="/pages/2018/03/11/leetcode-121-best-time-to-buy-and-sell-stock/" rel="alternate"></link><published>2018-03-11T00:00:00-06:00</published><updated>2018-03-11T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-11:/pages/2018/03/11/leetcode-121-best-time-to-buy-and-sell-stock/</id><summary type="html">&lt;p&gt;题目大意：给定一个数组表示一只股票每天的股票价格，求最大的收益。股票只有买了之后才能卖。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock/description/"&gt;121. Best Time to Buy and Sell Stock&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/best-time-to-buy-and-sell-stock-with-cooldown/description/"&gt;309. Best Time to Buy and Sell Stock with Cooldown&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：给定一个数组表示每天的股票价格，求最大的收益&lt;/p&gt;
&lt;p&gt;解题思路： 经典一维动态规划题。在最低价的时候购买，然后在第i天出售。因为这个题目允许回看最低的价格，所以能够找到所有天数的最低价格，然后用当前价格减去最低价格得到profit，再找出最大的profit：比较当前的profit跟到前一天位置最高profit取最大值&lt;/p&gt;
&lt;h2&gt;1. brute force裸写&lt;/h2&gt;
&lt;p&gt;时间复杂度是O(n^2).&lt;/p&gt;
&lt;p&gt;这个题如果直接裸写的话，会超时。200个测试中，199个跑过了，最后一个超时，所以这个办法不可行&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxProfit(self, prices):
        ans = 0
        for i in range(len(prices) - 1):
            for j in range(i, len(prices)):
                if ans &amp;lt; prices[j] - prices[i]:
                    ans = prices[j] - prices[i]
        return ans
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 动态规划&lt;/h2&gt;
&lt;p&gt;buy price:  prices[i] is the min(prices[k], k &amp;lt;= i)&lt;/p&gt;
&lt;p&gt;sell price: prices[j] is the max(prices[k], k &amp;gt;= j)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;L[i]: lowest price up to i_th day&lt;/p&gt;
&lt;p&gt;P[i]: max profit up to i_th day&lt;/p&gt;
&lt;p&gt;P[i] = max(P[i - 1], prices[i] - L[i])&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;在最低价的时候购买，然后在第i天出售。因为这个题目允许回看最低的价格，所以能够找到所有天数的最低价格，然后用当前价格减去最低价格得到profit，再找出最大的profit：比较当前的profit跟到前一天位置最高profit取最大值&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def maxProfit(self, prices):
    n = len(prices)
    if n == 0:
        return 0
    p = [0 for _ in range(n)]
    l = [0 for _ in range(n)]
    l[0] = prices[0]

    for i in range(1, n):
        l[i] = min(l[i - 1], prices[i])
        p[i] = max(prices[i] - l[i], p[i - 1]) 
    return p[n - 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;进一步，因为l和p只跟上一步有关系，所有可以进一步压缩空间成常量。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def maxProfit(self, prices):
    n = len(prices)
    if n == 0:
        return 0
    p = 0
    l = prices[0]

    for i in range(1, n):
        l = min(l, prices[i])
        p = max(prices[i] - l, p)
    return p
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3. Reduce to LC 53 Maximum Subarray&lt;/h2&gt;
&lt;p&gt;如果前一天买，后一天卖出，那么当天股价跟前一天股价的差价即为当天的盈利。从所有的盈利当中找出一个连续子集，这个连续子集的最大和就是最大的盈利值。求取最大连续子集的和的过程即是LC 53 Maximum Subarray.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;prince [7, 1, 5, 3, 6, 4]&lt;/p&gt;
&lt;p&gt;gain [-6, 4, -2, 3, -2]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;所以最大盈利是 4 - 2 + 3 = 5&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def maxProfit(self, prices):
    n = len(prices)
    if n == 0 or n == 1:
        return 0
    gain = [0 for _ in range(n - 1)]
    for i in range(n - 1):
        gain[i] = prices[i + 1] - prices[i]
    def maxSubarray(nums):
        n = len(nums)
        if n == 1:
            return nums[0]
        p = [-float('inf') for _ in range(n)]
        p[0] = nums[0]
        for i in range(n):
            p[i] = max(p[i - 1] + nums[i], nums[i])
        return max(p)
    return max(0, maxSubarray(gain))
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 72. Edit Distance</title><link href="/pages/2018/03/11/leetcode-72-edit-distance/" rel="alternate"></link><published>2018-03-11T00:00:00-06:00</published><updated>2018-03-11T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-11:/pages/2018/03/11/leetcode-72-edit-distance/</id><summary type="html">&lt;p&gt;题目大意，给定两个单词，问最少需要多少步能把单词1修改成单词2&lt;/p&gt;</summary><content type="html">&lt;p&gt;题目大意，给定两个单词，问最少需要多少步能把单词1修改成单词2&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/edit-distance/description/"&gt;72. Edit Distance&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://zxi.mytechroad.com/blog/dynamic-programming/leetcode-72-edit-distance/"&gt;参考解法&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大概思路:比较word1和word2的最后一个字母：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果相同，去掉最后一个字母，再比较去掉最后一位以后的两个单词的最后一个字母。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;如果不相同，那么有三种子问题&lt;/strong&gt;：&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;把word1去掉最后一个字母，然后再转为word2&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把word2去掉最后一个字母，然后再转为word1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把word1和word2都去掉最后一个字母，然后再转换&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;用 &lt;span class="math"&gt;\(d(i, j) = minDistance(word1[0 ... i-1], word2[0 ... j-1])\)&lt;/span&gt;, 即把word1前i个字母转换成word2前j个字母需要的最小edit distance。 那么 &lt;/p&gt;
&lt;p&gt;d(i, j) = &lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;i if j == 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;j if i == 0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;d(i - 1, j - 1) if word1[i - 1] == word2[j - 1]&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;min(d(i - 1, j), d(i, j - 1), d(i - 1, j - 1)) + 1&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h2&gt;1. 记忆化递归&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def minDistance(self, word1, word2):
        m = len(word1)
        n = len(word2)
        self.d = [[-1 for _ in range(n + 1)] for _ in range(m + 1)]
        def helper(word1, word2, m, n):
            if m == 0:
                return n
            if n == 0:
                return m
            if self.d[m][n] &amp;gt;= 0:
                return self.d[m][n]
            if word1[m - 1] == word2[n - 1]:
                ans = helper(word1, word2, m - 1, n - 1)
            else:
                ans = min(helper(word1, word2, m - 1, n - 1), helper(word1, word2, m - 1, n), helper(word1, word2, m, n - 1)) + 1
            self.d[m][n] = ans
            return self.d[m][n]
        return helper(word1, word2, m, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 递推/动态规划&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def minDistance(self, word1, word2):
        m = len(word1)
        n = len(word2)
        d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        for i in range(1, m + 1):
            d[i][0] = i
        for j in range(n + 1):
            d[0][j] = j
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if word1[i - 1] == word2[j - 1]:
                    d[i][j] = d[i - 1][j - 1]
                else:
                    d[i][j] = min(d[i - 1][j], d[i][j - 1], d[i - 1][j - 1]) + 1
        return d[m][n]
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 91. Decode Ways</title><link href="/pages/2018/03/11/leetcode-91-decode-ways/" rel="alternate"></link><published>2018-03-11T00:00:00-06:00</published><updated>2018-03-11T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-11:/pages/2018/03/11/leetcode-91-decode-ways/</id><summary type="html">&lt;p&gt;题目大意：1到26对应A到Z。现在给一个数字，问有多少中解码方式&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/decode-ways/description/"&gt;91. Decode Ways&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意：1到26对应A到Z。现在给一个数字，问有多少中解码方式&lt;/p&gt;
&lt;p&gt;解题思路：数字1到26只有一位或者两位，所以把数字去掉开始的第一位和第二位，那么原来数字的解码方式就等于去掉一位的解码方式加上去掉两位的解码方式之和。&lt;/p&gt;
&lt;p&gt;两个condition：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;空字符串解码方式为0，长度为1的字符串解码方式为1&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;0开头的数字解码方式为0，因为数字只有1到26&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;1. 记忆化递归&lt;/h2&gt;
&lt;p&gt;在记忆体中，空字符串的解码方式为1.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def numDecodings(self, s):
        if len(s) == 0:
            return 0
        self.m_ways = {}
        self.m_ways[&amp;quot;&amp;quot;] = 1

        def ways(s):
            if s in self.m_ways:
                return self.m_ways[s]
            if s[0] == &amp;quot;0&amp;quot;:
                return 0
            if len(s) == 1:
                return 1
            w = ways(s[1:])
            prefix = int(s[:2])
            if prefix &amp;lt;= 26:
                w += ways(s[2:])
            self.m_ways[s] = w
            return w
        return ways(s)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 记忆化递归, 使用index来考虑子问题&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def numDecodings(self, s):
    if len(s) == 0:
        return 0
    self.m_ways = {}
    def ways(s, l, r):
        if l in self.m_ways:
            return self.m_ways[l]
        if s[l] == &amp;quot;0&amp;quot;:
            return 0
        if l &amp;gt;= r:
            return 1
        w = ways(s, l + 1, r)
        prefix = int(s[l]) * 10 + int(s[l + 1])
        if prefix &amp;lt;= 26:
            w += ways(s, l + 2, r)
        self.m_ways[l] = w
        return w
    return ways(s, 0, len(s) - 1)
&lt;/code&gt;&lt;/pre&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 53. Maximum Subarray</title><link href="/pages/2018/03/03/leetcode-53-maximum-subarray/" rel="alternate"></link><published>2018-03-03T00:00:00-06:00</published><updated>2018-03-03T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-03:/pages/2018/03/03/leetcode-53-maximum-subarray/</id><summary type="html">&lt;p&gt;题目大意:给定一个数组，从这个数组中任取一个连续的子数组，找出最大的子数组的和。&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://leetcode.com/problems/maximum-subarray/description/"&gt;53. Maximum Subarray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意:给定一个数组，从这个数组中任取一个连续的子数组，找出最大的子数组的和。&lt;/p&gt;
&lt;p&gt;使用dp，&lt;span class="math"&gt;\(f[i]\)&lt;/span&gt;表示到数组第i个元素的时候的和的最大值，那么&lt;span class="math"&gt;\(f[i] = max(f[i-1] + nums[i], nums[i])\)&lt;/span&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxSubArray(self, nums):
        def dp(nums):
            n = len(nums)
            f = [0 for _ in range(n)]
            f[0] = nums[0]
            for i in range(1, n):
                f[i] = max(f[i-1] + nums[i], nums[i])
            return max(f)
        return dp(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;因为上面的f[i]只跟f[i-1]有关系，所有可以使用滚动数组降维,一维的滚动数组降维成常数项。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def maxSubArray(self, nums):
        def dp(nums):
            n = len(nums)
            curr = nums[0]
            ans = nums[0]
            for i in range(1, n):
                curr = max(curr + nums[i], nums[i])
                if curr &amp;gt; ans:
                    ans = curr
            return ans
        return dp(nums)
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 62. Unique Paths</title><link href="/pages/2018/03/03/leetcode-62-unique-paths/" rel="alternate"></link><published>2018-03-03T00:00:00-06:00</published><updated>2018-03-03T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-03:/pages/2018/03/03/leetcode-62-unique-paths/</id><summary type="html">&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的格子上左上角走到右下角，只能向右或者向下走，总共有多少种不同的走法？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;这几道题目很相似，把它们放在一起做个比较。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths/description/"&gt;62. Unique Paths&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths-ii/description/"&gt;63. Unique Paths II&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/minimum-path-sum/description/"&gt;64. Minimum Path Sum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的格子上左上角走到右下角，只能向右或者向下走，总共有多少种不同的走法？&lt;/p&gt;
&lt;h2&gt;1. 记忆化递归&lt;/h2&gt;
&lt;p&gt;这里的递归指的是函数自己调用自己。比如下面递归的办法中，函数&lt;code&gt;dp&lt;/code&gt;自己会调用&lt;code&gt;dp&lt;/code&gt;。 &lt;/p&gt;
&lt;p&gt;记忆化指的是把前面已经求解过的值记录在记忆体里面，这样后面要时候的时候就不需要重复计算。&lt;/p&gt;
&lt;p&gt;在位置&lt;span class="math"&gt;\((y, x)\)&lt;/span&gt;的走法上一行&lt;span class="math"&gt;\((y - 1, x)\)&lt;/span&gt;和前一列&lt;span class="math"&gt;\((y, x - 1)\)&lt;/span&gt;的走法的和。用一个记忆体&lt;span class="math"&gt;\(f[y][x]\)&lt;/span&gt;来记录&lt;span class="math"&gt;\((y, x)\)&lt;/span&gt;的走法。如果已经求解了，则不需要再求解。这样可以节省时间。考虑到边界会越界，对&lt;span class="math"&gt;\(f\)&lt;/span&gt;进行padding&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def uniquePaths(self, m, n):
        self.f = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        def dp(m, n):
            if m == 0 or n == 0:
                return 0
            if m == 1 and n == 1:
                return 1
            if self.f[m][n] &amp;gt; 0:
                return self.f[m][n]
            left_ans = dp(m, n - 1)
            upper_ans = dp(m - 1, n)
            self.f[m][n] = left_ans + upper_ans
            return self.f[m][n]
        return dp(m, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 递推 / 动态规划&lt;/h2&gt;
&lt;p&gt;递推(或者叫动态规划)的形式和递归很相似，只是把中间结果保存在数组里面从而避免重复求解。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def uniquePaths(self, m, n):
        self.f = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        self.f[1][1] = 1
        def dp(y, x):
            for y in range(1, m + 1):
                for x in range(1, n + 1):
                    if y == 1 and x == 1:
                        continue
                    else:
                        self.f[y][x] = self.f[y - 1][x] + self.f[y][x - 1]
        dp(m + 1, n + 1)
        return self.f[m][n]
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 63. Unique Paths II</title><link href="/pages/2018/03/03/leetcode-63-unique-paths-ii/" rel="alternate"></link><published>2018-03-03T00:00:00-06:00</published><updated>2018-03-03T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-03:/pages/2018/03/03/leetcode-63-unique-paths-ii/</id><summary type="html">&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的有障碍物的格子左上角走到右下角，只能向右或者向下走，总共有多少种不同的走法？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的有障碍物的格子上左上角走到右下角，只能向右或者向下走，总共有多少种不同的走法？&lt;/p&gt;
&lt;p&gt;63题和62题很相似，只是格子有障碍物。解决的办法是如果有障碍物，那么那个格子的可行办法是0.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths/description/"&gt;62. Unique Paths&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths-ii/description/"&gt;63. Unique Paths II&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/minimum-path-sum/description/"&gt;64. Minimum Path Sum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;使用递推(动态规划)的办法：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def uniquePathsWithObstacles(self, obstacleGrid):
        self.grid = obstacleGrid
        m = len(obstacleGrid)
        n = len(obstacleGrid[0])
        self.f = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        self.f[1][1] = 1 if obstacleGrid[0][0] == 0 else 0
        def dp(y, x):
            for y in range(1, m + 1):
                for x in range(1, n + 1):
                    if y == 1 and x == 1:
                        continue
                    elif self.grid[y - 1][x - 1] == 1:
                        self.f[y][x] = 0
                    else:
                        self.f[y][x] = self.f[y - 1][x] + self.f[y][x - 1]
        dp(m + 1, n + 1)
        return self.f[m][n]
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 64. Minimum Path Sum</title><link href="/pages/2018/03/03/leetcode-64-minimum-path-sum/" rel="alternate"></link><published>2018-03-03T00:00:00-06:00</published><updated>2018-03-03T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-03:/pages/2018/03/03/leetcode-64-minimum-path-sum/</id><summary type="html">&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的格子左上角走到右下角，每个格子有一个非负整数，只能向右或者向下走，要找到一条路径使得格子里的数字的和最小？&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</summary><content type="html">&lt;p&gt;题目大意:从一个&lt;span class="math"&gt;\(m \times n\)&lt;/span&gt;的格子左上角走到右下角，每个格子有一个非负整数，只能向右或者向下走，要找到一条路径使得格子里的数字的和最小？&lt;/p&gt;
&lt;p&gt;这道题和62 63都比较相似，只是格子里面加了数字。跟骑士跟公主的那道题也相似。&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths/description/"&gt;62. Unique Paths&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/unique-paths-ii/description/"&gt;63. Unique Paths II&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/minimum-path-sum/description/"&gt;64. Minimum Path Sum&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/dungeon-game/description/"&gt;174. Dungeon Game&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://leetcode.com/problems/cherry-pickup/description/"&gt;741. Cherry Pickup&lt;/a&gt;&lt;/p&gt;
&lt;h2&gt;1. 采用记忆化递归&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;class Solution(object):
    def minPathSum(self, grid):
        self.grid = grid
        m = len(grid)
        n = len(grid[0])
        self.f = [[0 for _ in range(n + 1)] for _ in range(m + 1)]
        self.f[1][1] = grid[0][0]
        def dp(y, x):
            if x &amp;lt; 1 or y &amp;lt; 1:
                return float('inf')
            if y == 1 and x == 1:
                return self.grid[0][0]
            if self.f[y][x] &amp;gt; 0:
                return self.f[y][x]
            else:
                self.f[y][x] = min(dp(y - 1, x), dp(y, x - 1)) + self.grid[y - 1][x - 1]
                return self.f[y][x]
        return dp(m, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 使用递推(动态规划)的办法，类似62题和63题：&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;class Solution(object):
    def minPathSum(self, grid):
        self.grid = grid
        m = len(grid)
        n = len(grid[0])
        self.f = [[float('inf') for _ in range(n + 1)] for _ in range(m + 1)]
        self.f[1][1] = grid[0][0]
        def dp(y, x):
            for y in range(1, m + 1):
                for x in range(1, n + 1):
                    if y == 1 and x == 1:
                        continue
                    else:
                        self.f[y][x] = min(self.f[y - 1][x], self.f[y][x - 1]) + self.grid[y - 1][x - 1]
        dp(m + 1, n + 1)
        return self.f[m][n]
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3. 使用递推，直接修改原矩阵&lt;/h2&gt;
&lt;p&gt;直接修改原来的矩阵grid，这样不会有额外的空间开销&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
class Solution(object):
    def minPathSum(self, grid):
        m = len(grid)
        n = len(grid[0])
        for y in range(m):
            for x in range(n):
                if y == 0 and x == 0:
                    continue
                elif y == 0:
                    grid[y][x] += grid[y][x - 1]
                elif x == 0:
                    grid[y][x] += grid[y - 1][x]
                else:
                    grid[y][x] += min(grid[y][x - 1], grid[y - 1][x])
        return grid[m - 1][n - 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>leetcode 70. Climbing Stairs</title><link href="/pages/2018/03/03/leetcode-70-climbing-stairs/" rel="alternate"></link><published>2018-03-03T00:00:00-06:00</published><updated>2018-03-03T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-03-03:/pages/2018/03/03/leetcode-70-climbing-stairs/</id><summary type="html"></summary><content type="html">&lt;p&gt;leetcode第70题：&lt;a href="https://leetcode.com/problems/climbing-stairs/description/"&gt;70. Climbing Stairs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这道题可以拆分为：到第&lt;span class="math"&gt;\(n\)&lt;/span&gt;级楼梯的爬法等于到到第&lt;span class="math"&gt;\(n-1\)&lt;/span&gt;级楼梯的爬法加上到第&lt;span class="math"&gt;\(n-2\)&lt;/span&gt;级楼梯的爬法。&lt;/p&gt;
&lt;h2&gt;动态规划&lt;/h2&gt;
&lt;h3&gt;1： 使用递推的办法：&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def climbingStairs(self, n):
    f = [0 for _ in range(n+1)]
    f[0] = 1
    f[1] = 1
    for i in range(2, n + 1):
        f[i] = f[i-1] + f[i-2]
    return f[n]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;2: 非记忆化递归&lt;/h3&gt;
&lt;p&gt;会超时，因为没有记住以前的答案，每次都要重复求解&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def climbingStairs(self, n):
    def numOfSolutions(n):
        if n &amp;lt;= 1:
            return 1
        return numOfSolutions(n-1) + numOfSolutions(n-2) 
    return numOfSolutions(n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3: 记忆化递归&lt;/h3&gt;
&lt;p&gt;每次求解的值都保存下来，这样下一步的时候就不要重复计算，所以比非记忆化递归快&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def climbingStairs(self, n):
    f_ = [0 for _ in range(n+1)]
    def numOfSolutions(n):
        if n &amp;lt;= 1: return 1
        if f_[n] &amp;gt; 0: return f_[n] # if already calc, then remember it
        f_[n] = numOfSolutions(n-1) + numOfSolutions(n-2) 
        return f_[n]
    return numOfSolutions(n)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;4: constant变量递推&lt;/h3&gt;
&lt;p&gt;上面的递推解法里面，第&lt;span class="math"&gt;\(i\)&lt;/span&gt;步的结果跟前面两步有关，而最终我们不要中间的结果，只要第&lt;span class="math"&gt;\(n\)&lt;/span&gt;步的结果。所以我们可以使用constant变量来压缩空间。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def climbingStairs(self, n):
    two = 1
    one = 1
    curr = 1
    for i in range(2, n + 1):
        curr = two + one
        two = one
        one = curr
    return curr
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category><category term="dynamic programming"></category></entry><entry><title>WSJ Web Crawler</title><link href="/pages/2018/02/18/wsj-web-crawler/" rel="alternate"></link><published>2018-02-18T00:00:00-06:00</published><updated>2018-02-18T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-02-18:/pages/2018/02/18/wsj-web-crawler/</id><summary type="html">&lt;p&gt;This is an introduction to the &lt;a href="http://songhuiming.com/wsj_counts/"&gt;Wall Street News Counts&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Introduction&lt;/h2&gt;
&lt;p&gt;This is an introduction to the &lt;a href="http://songhuiming.com/wsj_counts/"&gt;Wall Street News Counts&lt;/a&gt;. The idea is to crawl the online financial news of the public trading company. Do a sentiment analysis of the news and link the sentiment score with the stock price trend. It is believed the frequency of the news(volume / counts), the sentiment analysis score are good predictors to srock price trend.(see &lt;a href="http://www.seas.upenn.edu/~cse400/CSE400_2007_2008/DavdaMittal/NLPSentimentTrading.pdf"&gt;1&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The first step is to crawl the news from different webpages like &lt;a href="https://www.wsj.com"&gt;wall street journal&lt;/a&gt;, &lt;a href="https://www.bloomberg.com"&gt;bloomberg&lt;/a&gt; and so on. Here is from WSJ, which free users can only get news title but not the contents. The second step is to clean and reformat the data. Like encoding, date and time format, exception handling. The third step is to score the contents of the news. A popular way is to score the data with different models and then pick the median or the sum of the score(is this like random forest?) The forth step is to incrementally dump the cleaned data to the database. Make sure there is no dups. The fifth step is to back up the db and the tables after dumping. You will find backup will be more and more importand in your life. The sixth step is to create some summary information tables for data visualization. Like here, I don't want to pull the db every time when someone visits my page. That will most likely cause the db crashed. Insteadly, I will save the summary information in an independent table in memory. &lt;code&gt;Redis&lt;/code&gt; is very helpful to work as data cache. Finally the data will be sent to the frontend when it is called.&lt;/p&gt;
&lt;h2&gt;Main prograom&lt;/h2&gt;
&lt;h3&gt;1. crawl wsj web&lt;/h3&gt;
&lt;p&gt;I did not find wsj have good api to download the data.(by the way, it is like yahoo finance and google finance doesn't support well to download stock data in csv. Once it was very easy to do.) But python is very powerful for web crawling. &lt;code&gt;scrapy&lt;/code&gt; is one of the most famous and powerful tool to use. Some other light tools include &lt;code&gt;BeautifulSoup&lt;/code&gt;, &lt;code&gt;requests&lt;/code&gt;, &lt;code&gt;urllib&lt;/code&gt; and so on. If using these light tools, you need to spend some time to read the source code of the webpage. There are usually some nice features that make it easy for you to find out how to write your crawler. Also, regular expression will be your friend all the time.&lt;/p&gt;
&lt;h3&gt;2. clean data&lt;/h3&gt;
&lt;p&gt;This step will include cleaning the raw data(removing html tags, filling the missing part and so on), encoding the characters(utf-8), formating the data(date and time format, numers). Some useful packages are &lt;code&gt;datetime&lt;/code&gt;, &lt;code&gt;time&lt;/code&gt;, &lt;code&gt;re&lt;/code&gt;. &lt;/p&gt;
&lt;h3&gt;3. score the data&lt;/h3&gt;
&lt;p&gt;After getting the news, we need to do some transformation on the data. &lt;code&gt;nltk&lt;/code&gt; can help to do these work: &lt;code&gt;token&lt;/code&gt; the words, normalize the words, &lt;code&gt;stemmer&lt;/code&gt;, creates &lt;code&gt;tf-idf&lt;/code&gt; and calculate the distance between different documents. These will be be input for the sentimental model. Different model will be used to score the same contents to get more reliable results.&lt;/p&gt;
&lt;h3&gt;4. dump data to db&lt;/h3&gt;
&lt;p&gt;If you only create one time run job, then db might not be necessary. But if you want the job to be kicked off regularly, then dumping data to db is very very important. Please refer to &lt;a href="http://songhuiming.github.io/pages/2016/07/26/mysql-db-and-python-mysqldb/"&gt;this&lt;/a&gt; for a quick review. Since here the news will be pretty long, mysql &lt;code&gt;TEXT&lt;/code&gt; might not work depends on your mysql version. &lt;code&gt;MongoDB&lt;/code&gt; which is document oriented together with &lt;code&gt;redis&lt;/code&gt; as cache will work better.&lt;/p&gt;
&lt;p&gt;The crawling code will be run everyday to download the latest news. So only the incremental data will be saved to the database. Too frequent reading and writing will cause the db crash. Here is what I did: 1) download the news for each tickers and save them in the temp table. 2)select the latest time for that picker, and only dump the downloaded news after that time to the database. This will work like append a table to the existed table rather than doing it for each single record.&lt;/p&gt;
&lt;h3&gt;5. backup the data&lt;/h3&gt;
&lt;p&gt;Again, if for one time run, this is not necessary. But if you want to run the job regularly, it is very important to back up the data. &lt;/p&gt;
&lt;h3&gt;6. create summary information&lt;/h3&gt;
&lt;p&gt;It is not a good idea to read all the data in the database for every request. Here I will only show the number of news every week for each ticker and their stock price. A workable solution is to read the data out of the database at one time and then organize it to get their information every week. So any request from the front end will call this summary of data without having to read all the information in the database. This will not only save the time, but also reduce the possibility of system breakdown. &lt;/p&gt;
&lt;h3&gt;7. server end&lt;/h3&gt;
&lt;p&gt;The server will call the summary data generated, and then render them to the front end when the url triggers an event.&lt;/p&gt;
&lt;h3&gt;8. front end&lt;/h3&gt;
&lt;p&gt;If plot in python, &lt;code&gt;matplotlib&lt;/code&gt; will be number 1 choice to plot static graphs. For dynamic graphs, I did not figure out which is the best. &lt;code&gt;bokeh&lt;/code&gt;, &lt;code&gt;plotly&lt;/code&gt; are the two that are easy to learn and powerful to use. But for charts in the front end, &lt;code&gt;highcharts&lt;/code&gt; is a good choice. &lt;a href="https://www.federalreserve.gov/data/dataviz.htm"&gt;St Louis Federal Reserve&lt;/a&gt; has some good examples of data visualization with highcharts. A lot of work should be done here to improve the graphs. It is said you should know a back end language and a front end language. &lt;/p&gt;
&lt;h2&gt;Others&lt;/h2&gt;
&lt;h3&gt;1. cron job&lt;/h3&gt;
&lt;p&gt;Set up cron job to run the program every sunday. &lt;code&gt;* 8 * * 6 python /pathToCode/wsj.crawl.py&lt;/code&gt; will run the code every sunday at 8:00AM.&lt;/p&gt;
&lt;h3&gt;2. sync data&lt;/h3&gt;
&lt;p&gt;downloading host and web host are different. So after the data is download, it must be synced to the host server to display. &lt;/p&gt;
&lt;p&gt;最后要说的：一入前端深似海。从html到javascript, css, jQuery, node.js, vue.js, react等等等，各种东西只能临阵磨枪，了解个皮毛。&lt;/p&gt;</content><category term="python"></category><category term="python"></category><category term="flask"></category><category term="highcharts"></category><category term="sql"></category><category term="webCrawl"></category></entry><entry><title>SVM Introduction</title><link href="/pages/2018/02/04/svm-introduction/" rel="alternate"></link><published>2018-02-04T00:00:00-06:00</published><updated>2018-02-04T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-02-04:/pages/2018/02/04/svm-introduction/</id><summary type="html"></summary><content type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;We have two classes of data, the &lt;code&gt;+&lt;/code&gt; and the &lt;code&gt;-&lt;/code&gt;. Now if there is an unknown point &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt;, shall we classify it as &lt;code&gt;+&lt;/code&gt; or &lt;code&gt;-&lt;/code&gt;?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/figures/20180203_blog_svm_01.svg"&gt;&lt;/p&gt;
&lt;p&gt;1) suppose there is a line that can split the &lt;code&gt;+&lt;/code&gt; and the &lt;code&gt;-&lt;/code&gt;(the dashed line). We project &lt;span class="math"&gt;\(\vec{u}\)&lt;/span&gt; to the direction perpendicular to the split line, that is, the direction of &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Then the decition rule is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If projection length &lt;span class="math"&gt;\(\vec{\omega} \cdot \vec{u} \ge c\)&lt;/span&gt;, then &lt;code&gt;+&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;if projection length &lt;span class="math"&gt;\(\vec{\omega} \cdot \vec{u} \le c\)&lt;/span&gt;, then &lt;code&gt;-&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;let &lt;span class="math"&gt;\(b = -c\)&lt;/span&gt;, then we have the decision rule is &lt;span class="math"&gt;\(\vec{\omega} \cdot \vec{u} + b \ge 0\)&lt;/span&gt;, then &lt;code&gt;+&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now we need to find &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt; and &lt;span class="math"&gt;\(b\)&lt;/span&gt; (the parameters to decide the dashed line).&lt;/p&gt;
&lt;p&gt;2) Next, for given  &lt;code&gt;+&lt;/code&gt; and &lt;code&gt;-&lt;/code&gt; samples, we want&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \vec{\omega} \cdot \vec{x}_ {+} + b \ge 1  \mbox{  , right solid line} \\
    \vec{\omega} \cdot \vec{x}_ {-} + b \le -1  \mbox{  , left solid line} 
\end{aligned}&lt;/div&gt;
&lt;p&gt;For math convenience, we define &lt;span class="math"&gt;\(y_i\)&lt;/span&gt; s.t. &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    y_i =
        \left\{ 
            \begin{array}{lr}   
                1, \mbox{  for + sample}  \\
                -1, \mbox{ for - sample}  \\
            \end{array}
        \right.
\end{aligned}&lt;/div&gt;
&lt;p&gt;So we have&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y_i (\vec{\omega} \cdot \vec{x}_ {+} + b) \ge 1\)&lt;/span&gt;, that is,&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(y_i (\vec{\omega} \cdot \vec{x}_ {+} + b) - 1 \ge 0\)&lt;/span&gt; for all &lt;code&gt;+&lt;/code&gt; samples.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then for the points on the boundary, we have &lt;span class="math"&gt;\(y_i (\vec{\omega} \cdot \vec{x}_ {+} + b) - 1 = 0\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We want the street(distance between two solid line) to be as wide as possible. &lt;/p&gt;
&lt;p&gt;The width is &lt;span class="math"&gt;\((\vec{x}_ {+} - \vec{x}_ {-})\)&lt;/span&gt; projected on the unit direction vector. So the length is&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    (\vec{x}_ {+} - \vec{x}_ {-}) \cdot \frac{\vec{\omega}}{||\vec{\omega}||}   = \frac{2}{||\vec{\omega}||}
\end{aligned}&lt;/div&gt;
&lt;p&gt;&lt;img src="/figures/20180203_blog_svm_02.svg"&gt;&lt;/p&gt;
&lt;p&gt;The following are equvalent:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \mbox{max} \frac{2}{||\vec{\omega}||} \Leftrightarrow  \mbox{min} ||\vec{\omega}||  \Leftrightarrow    \mbox{min} \frac{1}{2} ||\vec{\omega}||^{2}
\end{aligned}&lt;/div&gt;
&lt;p&gt;Use Lagrange multipliers,&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    L =  \frac{1}{2} ||\vec{\omega}||^{2} - \sum_{i} \alpha_i \left[ y_i (\vec{\omega} \cdot \vec{x} + b) - 1 \right]
\end{aligned}&lt;/div&gt;
&lt;blockquote&gt;
&lt;p&gt;For &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; not on the boundary, $\alpha_i $ will be 0. So only the points on the boundary will play role in.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The floowing is an example to show how the boundary points are used in building the linear separater.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/figures/20180204_svm_01.gif"&gt;&lt;/p&gt;
&lt;p&gt;To max &lt;span class="math"&gt;\(L\)&lt;/span&gt;, take partial derivatives:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial{L}}{\partial{\vec{\omega}}} = \vec{\omega} -  \sum_{i} \alpha_{i} y_{i} \vec{x}_ {i}
\end{aligned}&lt;/div&gt;
&lt;p&gt;So we have &lt;/p&gt;
&lt;div class="math"&gt;$$\vec{\omega} = \sum_{i} \alpha_{i} y_{i} \vec{x}_ {i} $$&lt;/div&gt;
&lt;p&gt;So, &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt; is a linear sum of the vectors &lt;span class="math"&gt;\(\sum_ {i} \alpha_ {i} y_ {i} \vec{x}_ {i}\)&lt;/span&gt;. 
Since some &lt;span class="math"&gt;\(\alpha_ {i}\)&lt;/span&gt; will be 0, so, &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt; is only some of the vectors sum.&lt;/p&gt;
&lt;p&gt;Take partial derivative to &lt;span class="math"&gt;\(b\)&lt;/span&gt;, we have:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial{L}}{\partial{b}} = -  \sum_{i} \alpha_{i} y_{i} 
\end{aligned}&lt;/div&gt;
&lt;p&gt;So, &lt;span class="math"&gt;\(\sum_{i} \alpha_{i} y_{i}  = 0\)&lt;/span&gt;. This is a quadratic optimization problem.&lt;/p&gt;
&lt;p&gt;Plug in &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt; into &lt;span class="math"&gt;\(L\)&lt;/span&gt;, we will have&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    L =  \frac{1}{2} \left(\sum_{i} \alpha_{i} y_{i} \vec{x}_ {i} \right) \left(\sum_ {i} \alpha_{i} y_{i} \vec{x}_ {i} \right) -  \sum_{i} \alpha_{i} y_{i} \vec{x}_{i}  \cdot \left(\sum_{i} \alpha_{i} y_{i} \vec{x}_ {i} \right) -  \sum_{i} \alpha_{i} y_{i} b + \sum_{i} \alpha_{i}
\end{aligned}&lt;/div&gt;
&lt;p&gt;Since we have &lt;span class="math"&gt;\(\sum_{i} \alpha_{i} y_{i} = 0\)&lt;/span&gt;, So, &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    L =  \sum_{i} \alpha_{i} - \frac{1}{2}  \sum_{i} \sum_{j} \alpha_{i} \alpha_{j} y_ {i} y_ {j} \vec{x}_ {i} \cdot \vec{x}_ {j}  
\end{aligned}&lt;/div&gt;
&lt;p&gt;So the max of &lt;span class="math"&gt;\(L\)&lt;/span&gt; only depends on the pairs of &lt;span class="math"&gt;\(\vec{x}_ {i} \cdot \vec{x}_ {j}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Plug in &lt;span class="math"&gt;\(\vec{\omega}\)&lt;/span&gt; into the decision rule, we have&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \sum_{i}  \alpha_ {i} y_ {i} \vec{x}_ {i} \cdot \vec{u} + b \ge 0, \mbox{then +} 
\end{aligned}&lt;/div&gt;
&lt;p&gt;it is also the vector product.&lt;/p&gt;
&lt;p&gt;A little more, if the data cannot be splited by a linear line or linear space, what shall we do? Like the data points below, there is no linear method can split the data.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/figures/20180203_svm_kernel.JPG"&gt;&lt;/p&gt;
&lt;p&gt;If the data cannot split by a linear line, then we will change vector &lt;span class="math"&gt;\(\vec{x}\)&lt;/span&gt; to a kernel function &lt;span class="math"&gt;\(\phi(\vec{x})\)&lt;/span&gt;. 
So we need &lt;span class="math"&gt;\(\phi(\vec{x}_ {i}) \cdot \phi(\vec{x}_ {j})\)&lt;/span&gt; to max. The kernel is&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    k\left( \vec{x}_ {i}, \vec{x}_ {j}\right) = \phi(\vec{x}_ {i}) \cdot \phi(\vec{x}_ {j})
\end{aligned}&lt;/div&gt;
&lt;p&gt;So what I need is the kernel function &lt;span class="math"&gt;\(k\)&lt;/span&gt; to map &lt;span class="math"&gt;\(\vec{x}_ {i} \vec{x}_ {j}\)&lt;/span&gt; to another space.&lt;/p&gt;
&lt;p&gt;Choice of &lt;span class="math"&gt;\(k\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;1) &lt;span class="math"&gt;\(\left(\vec{x}_ {i} \vec{x}_ {j} + 1 \right)^{n}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;2) &lt;span class="math"&gt;\(e^{\frac{||\vec{x}_ {i} - \vec{x}_ {j}||}{\sigma}}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Finally, it is an example of kernel function to visualize the second kernel above(rbf kernel):&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alt text" src="/figures/20180204_svm_02.gif"&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="data mining"></category></entry><entry><title>动态规划</title><link href="/pages/2018/01/28/dong-tai-gui-hua/" rel="alternate"></link><published>2018-01-28T00:00:00-06:00</published><updated>2018-01-28T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-01-28:/pages/2018/01/28/dong-tai-gui-hua/</id><summary type="html">&lt;p&gt;很早以前学运筹学的时候学过动态规划。最近看题目的时候看到这个名字，借助&lt;a href="https://www.zhihu.com/question/23995189"&gt;zhihu&lt;/a&gt;上的一篇文章，重新温习一下。&lt;/p&gt;</summary><content type="html">&lt;h2&gt;动态规划的本质，是对问题状态的定义和状态转移方程的定义。&lt;/h2&gt;
&lt;p&gt;引自维基百科&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;dynamic programming is a method for solving a complex problem by breaking it down into a collection of simpler subproblems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;动态规划是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。如何拆分问题，是动态规划的核心。而拆分问题，靠的就是状态的定义和状态转移方程的定义。&lt;/p&gt;
&lt;h2&gt;什么是状态的定义？&lt;/h2&gt;
&lt;p&gt;首先想说大家千万不要被下面的数学式吓到，这里只涉及到了函数相关的知识。我们先来看一个动态规划的教学必备题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给定一个数列，长度为N，求这个数列的最长上升（递增）子数列（LIS）的长度.&lt;/p&gt;
&lt;p&gt;以1 7 2 8 3 4为例, 这个数列的最长递增子数列是 1 2 3 4，长度为4；&lt;/p&gt;
&lt;p&gt;次长的长度为3， 包括 1 7 8; 1 2 3 等.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;要解决这个问题，我们首先要定义这个问题和这个问题的子问题。&lt;/p&gt;
&lt;p&gt;我们来重新定义这个问题：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;给定一个数列，长度为&lt;span class="math"&gt;\(N\)&lt;/span&gt;，&lt;/p&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;为：以数列中第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项结尾的最长递增子序列的长度.&lt;/p&gt;
&lt;p&gt;求&lt;span class="math"&gt;\(F_1, ..., F_N\)&lt;/span&gt;中的最大值.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;显然，这个新问题与原问题等价。&lt;/p&gt;
&lt;p&gt;而对于&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;来讲，&lt;span class="math"&gt;\(F_1, ..., F_{k-1}\)&lt;/span&gt;都是&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;的子问题：因为以第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项结尾的最长递增子序列（下称LIS），包含着以第&lt;span class="math"&gt;\(1, ..., k-1\)&lt;/span&gt;中某项结尾的LIS。&lt;/p&gt;
&lt;p&gt;上述的新问题&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;也可以叫做状态，定义中的“&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;为数列中第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项结尾的LIS的长度”，就叫做对状态的定义。&lt;/p&gt;
&lt;p&gt;之所以把&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;做“状态”而不是“问题” ，一是因为避免跟原问题中“问题”混淆，二是因为这个新问题是数学化定义的。&lt;/p&gt;
&lt;h2&gt;什么是状态转移方程？&lt;/h2&gt;
&lt;p&gt;上述状态定义好之后，状态和状态之间的关系式，就叫做状态转移方程。&lt;/p&gt;
&lt;p&gt;比如，对于LIS问题，我们的第一种定义：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(F_k\)&lt;/span&gt;为：以数列中第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项结尾的最长递增子序列的长度.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;设&lt;span class="math"&gt;\(A\)&lt;/span&gt;为题中数列，状态转移方程为：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;span class="math"&gt;\(F_1 = 1\)&lt;/span&gt;（根据状态定义导出边界情况）&lt;/p&gt;
&lt;p&gt;$F_k = max(F_{i} + 1 | A_{k} &amp;gt; A_{i}, i \in (1, ..., k-1)), (k &amp;gt; 1) $&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;用文字解释一下是：以第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项结尾的LIS的长度是：保证第&lt;span class="math"&gt;\(i\)&lt;/span&gt;项比第&lt;span class="math"&gt;\(k\)&lt;/span&gt;项小的情况下，以第&lt;span class="math"&gt;\(i\)&lt;/span&gt;项结尾的LIS长度加一的最大值，取遍&lt;span class="math"&gt;\(i\)&lt;/span&gt;的所有值（&lt;span class="math"&gt;\(i\)&lt;/span&gt;小于&lt;span class="math"&gt;\(k\)&lt;/span&gt;）。&lt;/p&gt;
&lt;p&gt;这里可以看出，这里的状态转移方程，就是定义了问题和子问题之间的关系。&lt;/p&gt;
&lt;p&gt;可以看出，状态转移方程就是带有条件的递推式。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20180128_dynamic_programming_01.JPG" title="dp_lis"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def lengthOfLIS(nums):
    #Edge condition
    if not nums:
        return 0
    dp = [1 for i in xrange(len(nums))]
    for k in xrange(len(nums)):
        for i in xrange(k):
            # compare all F_i with F_k for i &amp;lt;ｋ
            if nums[i] &amp;lt; nums[k]:
                if dp[i] + 1 &amp;gt; dp[k]:
                    dp[k] = dp[i] + 1
    return max(dp)

lengthOfLIS([1, 7, 2, 8, 3, 4])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;动态规划迷思&lt;/h2&gt;
&lt;p&gt;本题下其他用户的回答跟动态规划都有或多或少的联系，我也讲一下与本答案的联系。&lt;/p&gt;
&lt;h3&gt;a. “缓存”，“重叠子问题”，“记忆化”：&lt;/h3&gt;
&lt;p&gt;这三个名词，都是在阐述递推式求解的技巧。以&lt;code&gt;Fibonacci&lt;/code&gt;数列为例，计算第100项的时候，需要计算第99项和98项；在计算第101项的时候，需要第100项和第99项，这时候你还需要重新计算第99项吗？不需要，你只需要在第一次计算的时候把它记下来就可以了。&lt;/p&gt;
&lt;p&gt;上述的需要再次计算的“第99项”，就叫“重叠子问题”。如果没有计算过，就按照递推式计算，如果计算过，直接使用，就像“缓存”一样，这种方法，叫做“记忆化”，这是递推式求解的技巧。这种技巧，通俗的说叫“花费空间来节省时间”。都不是动态规划的本质，不是动态规划的核心。&lt;/p&gt;
&lt;h3&gt;b. “递归”：递归是递推式求解的方法，连技巧都算不上。&lt;/h3&gt;
&lt;h3&gt;c. "无后效性"，“最优子结构”：&lt;/h3&gt;
&lt;p&gt;上述的状态转移方程中，等式右边不会用到下标大于左边&lt;span class="math"&gt;\(i\)&lt;/span&gt;或者&lt;span class="math"&gt;\(k\)&lt;/span&gt;的值，这是"无后效性"的通俗上的数学定义，符合这种定义的状态定义，我们可以说它具有“最优子结构”的性质，在动态规划中我们要做的，就是找到这种“最优子结构”。&lt;/p&gt;
&lt;p&gt;在对状态和状态转移方程的定义过程中，满足“最优子结构”是一个隐含的条件（否则根本定义不出来）。&lt;/p&gt;
&lt;p&gt;需要注意的是，一个问题可能有多种不同的状态定义和状态转移方程定义，存在一个有后效性的定义，不代表该问题不适用动态规划。这也是其他几个答案中出现的逻辑误区：&lt;/p&gt;
&lt;p&gt;动态规划方法要寻找符合“最优子结构“的状态和状态转移方程的定义，在找到之后，这个问题就可以以“记忆化地求解递推式”的方法来解决。而寻找到的定义，才是动态规划的本质。&lt;/p&gt;
&lt;p&gt;文艺的说，动态规划是寻找一种对问题的观察角度，让问题能够以递推（或者说分治）的方式去解决。寻找看问题的角度，才是动态规划中最耀眼的宝石！&lt;/p&gt;
&lt;h3&gt;附加例子&lt;/h3&gt;
&lt;p&gt;为什么需要&lt;code&gt;dp[i] + 1 &amp;gt; dp[k]&lt;/code&gt;? &lt;code&gt;lis([1,3,6,7,9,4,10,5,6])&lt;/code&gt;在处理10的时候，不应该由4来更新dp，也是应该由9所决定的dp来更新。&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.zhihu.com/question/23995189"&gt;什么是动态规划？动态规划的意义是什么？&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>深度搜索，回溯法和递归</title><link href="/pages/2018/01/07/shen-du-sou-suo-hui-su-fa-he-di-gui/" rel="alternate"></link><published>2018-01-07T00:00:00-06:00</published><updated>2018-01-07T00:00:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2018-01-07:/pages/2018/01/07/shen-du-sou-suo-hui-su-fa-he-di-gui/</id><summary type="html">&lt;p&gt;总结了一些网上的资料，比较深度搜索，回溯法和递归，以及怎么用来解决问题。&lt;/p&gt;</summary><content type="html">&lt;p&gt;通常如果需要列出所有的结果的时候，就需要使用回溯法(backtracking)。当然通过暴力解法(brute force)也是可以，但是暴力解法会浪费太多的时间和空间。两点区别&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;回溯法会回头，把路径pop出来，而暴力解法是走所有的解&lt;/li&gt;
&lt;li&gt;回溯法会剪枝，比如通过某些if条件滑过某些路径，减少很多不必要走的路&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;识别回溯&lt;/h2&gt;
&lt;p&gt;判断回溯很简单，拿到一个问题，你感觉如果不穷举一下就没法知道答案，那就可以开始回溯了。
一般回溯的问题有三种：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有没有解：Find a path to success &lt;/li&gt;
&lt;li&gt;求所有解：Find all paths to success 
    求所有解的个数
    求所有解的具体信息&lt;/li&gt;
&lt;li&gt;求最优解：Find the best path to success &lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;backtracking代码模板&lt;/h2&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;'''
backtracking使用dfs的模板，基本跟dfs的模板一模一样
'''
class Backtracking(object):

    def backtracking(self, input):

        self.res = []

        def dfs(input, temp, [index]):
            # 边界
            if 非法数据：
                return

            # 终止条件
            if len(input) == len(temp)：
                self.res.append(temp[:])
                return

            # for循环
            for i range(len(input)):
                ##1. 修改path
                temp.append(input[i])
                ##2. backtracking
                dfs(input, temp, [index])
                ##3. 退回原来状态，恢复path
                temp.pop()
        # 执行
        dfs(input, [], 0)
        return self.res
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;深度搜索与回溯法的区别&lt;/h2&gt;
&lt;p&gt;简单一点可以认为　回溯法 = 深度搜索 + 剪枝。一般大家用深度搜索时，或多或少会剪枝，因此深度搜索与回溯法没有什么不同，可以在它们之间画上一个等号。简单的可以认为二者等价。&lt;/p&gt;
&lt;p&gt;深度搜索一般用递归(recursion)来实现，这样比较简洁。&lt;/p&gt;
&lt;p&gt;深度搜索能够在候选答案生成到一半时，就进行判断，抛弃不满足要求的答案，所以深度搜索比暴力搜索法要快。&lt;/p&gt;
&lt;h2&gt;深度搜索与递归的区别&lt;/h2&gt;
&lt;p&gt;深度搜索经常用递归(recursion)来实现，二者常常同时出现。深度搜索，是逻辑意义上的算法，递归，是一种物理意义上的实现，它和迭代(iteration)是对应的。深度搜索，可以用递归来实现，也可以用栈来实现；而递归，一般总是用来实现深度搜索。可以说，递归一定是深度搜索，深度搜索不一定用递归。&lt;/p&gt;</content><category term="python"></category><category term="python"></category><category term="leetcode"></category></entry><entry><title>An interesting random walk question and simulation 03</title><link href="/pages/2017/10/22/an-interesting-random-walk-question-and-simulation-03/" rel="alternate"></link><published>2017-10-22T16:08:00-05:00</published><updated>2017-10-22T16:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-10-22:/pages/2017/10/22/an-interesting-random-walk-question-and-simulation-03/</id><summary type="html">&lt;p&gt;Modification of the &lt;a href="http://songhuiming.github.io/pages/2017/10/08/an-interesting-random-walk-question-and-simulation-02/"&gt;previous question&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is to modify the answer to the previous &lt;a href="http://songhuiming.github.io/pages/2017/10/08/an-interesting-random-walk-question-and-simulation-02/"&gt;blog&lt;/a&gt; about the expectation number of flips to get the pattern of the coin. For example: if flip a fair coin, what is the expected flips to get 3 consecutive Heads (HHH)? What is the expectation of flips to get Head, Tail and Head(HTH) or more status like HHHH?&lt;/p&gt;
&lt;p&gt;Take the question of HHH as an example: this is a stochastic question about Markov Chain. The possible status will be {0, H, HH, HHH}. The corresponding status transition matrix is &lt;/p&gt;
&lt;table border="1"&gt;
    &lt;tr&gt;
        &lt;td align="center" valign="middle"&gt;&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;H&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;HH&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;HHH&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center" valign="middle"&gt;H&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center" valign="middle"&gt;HH&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td align="center" valign="middle"&gt;HHH&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
        &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Denote the expected flips from 0 to HHH as f(0), the expected flips from H to HHH as f(H), the expected flips from HH to HHH as f(HH) and the expected flips from HHH to HHH as f(HHH). It is clear that f(HHH) = 0 since it is already reaching the status.
Title: An interesting random walk question and simulation 03
Date: 2017-10-08 16:08
Modified: 2017-10-08 16:08
Category: Python
Tags: python, random walk
Slug:
Author: Huiming Song
Summary: Got some random walk questions from my friend. We may see lots of similar questions in stochastic process. Here is the question.&lt;/p&gt;
&lt;p&gt;Based on the status transition matrix, we have&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
    \begin{cases}
      f(0) = \frac{1}{2} f(0) + \frac{1}{2} f(H)  + 1 \\
      f(H) = \frac{1}{2} f(0) + \frac{1}{2} f(HH)  + 1 \\
      f(HH) = \frac{1}{2} f(0) + \frac{1}{2} f(HHH)  + 1 \\
      f(HHH) = 0
    \end{cases}
\end{equation}&lt;/div&gt;
&lt;p&gt;Based on these, we can solve f(0) = 14. So the expectation number of flips to get HHH is 14 times.&lt;/p&gt;
&lt;p&gt;Similarly, we can calculate the expected number of flips to get HOH as below:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;H&lt;/th&gt;
&lt;th&gt;HT&lt;/th&gt;
&lt;th&gt;HTH&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HT&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HTH&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;div class="math"&gt;\begin{equation}
    \begin{cases}
      f(0) = \frac{1}{2} f(0) + \frac{1}{2} f(H)  + 1 \\
      f(H) = \frac{1}{2} f(H) + \frac{1}{2} f(HO)  + 1 \\
      f(HO) = \frac{1}{2} f(0) + \frac{1}{2} f(HOH)  + 1 \\
      f(HOH) = 0
    \end{cases}
\end{equation}&lt;/div&gt;
&lt;p&gt;By solving these equations, we will get f(0) = 10.&lt;/p&gt;
&lt;p&gt;Similarly, we can calculate the expectation number of flips for HHHH or the other more complicated cases.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;H&lt;/th&gt;
&lt;th&gt;HH&lt;/th&gt;
&lt;th&gt;HHH&lt;/th&gt;
&lt;th&gt;HHHH&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;H&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HH&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HHH&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0.5&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;HHHH&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following is to simulate the results in python. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def calcpattern(patstr):
    res = []
    t = 0
    while True:
        i = 0
        pos = []
        while True:
            x = np.random.randint(0, 2, 100)
            x = [str(s) for s in x]
            joinx = ''.join(x)
            try:
                p = joinx.index(patstr) + 3
                pos.append(p)
            except Exception as e:
                continue
            i += 1
            if i &amp;gt; 500:
                print('jump out i ' + str(i))
                break

        pos = np.array(pos)
        res.append(np.mean(pos))
        t += 1
        print('jump out t ' + str(t))
        if t &amp;gt; 500:
            break

    res = np.array(res)
    print(&amp;quot;The expected number of tosses for pattern %s is %f&amp;quot; %(patstr, np.mean(res)))

calcpattern('101')
calcpattern('010')
calcpattern('110')
calcpattern('1111')
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="random walk"></category></entry><entry><title>pickle error in multiprocssing</title><link href="/pages/2017/10/15/pickle-error-in-multiprocssing/" rel="alternate"></link><published>2017-10-15T16:08:00-05:00</published><updated>2017-10-15T16:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-10-15:/pages/2017/10/15/pickle-error-in-multiprocssing/</id><summary type="html">&lt;p&gt;Useful links from stackoverflow to solve the errors when pickle for multiprocessing&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the useful links I used to solve the issue I got when using multiprocssing in python.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;python Multithreading does not work well on windows. multiprocess sometimes have issues too. One simple example is about importing the self-defined module with multiprocessing in another code in windows NT OS. It is necessary to distinguish between the parent and child process with &lt;code&gt;__main__&lt;/code&gt;. Refer to &lt;a href="https://stackoverflow.com/questions/14175348/why-does-pythons-multiprocessing-module-import-main-when-starting-a-new-pro"&gt;This&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;what can be pickled in python? Functions are only picklable if they are defined at the &lt;strong&gt;top level&lt;/strong&gt; of the module. Refer to &lt;a href="https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error"&gt;this&lt;/a&gt; &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When define the multiprocessing funtion inside the &lt;code&gt;class&lt;/code&gt;, I got the error like &lt;em&gt;Can't pickle &lt;type 'instancemethod'&gt; when using multiprocessing Pool.map()&lt;/em&gt;. I try to solve it with &lt;code&gt;copy_reg&lt;/code&gt; as mentioned in &lt;a href="https://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-multiprocessing-pool-map"&gt;2&lt;/a&gt;. Another way to solve it is to use &lt;code&gt;pathos.multiprocessing&lt;/code&gt;. But finally I find &lt;a href="https://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-multiprocessing-pool-map/41959862#41959862"&gt;this&lt;/a&gt; is the easiest way to do it: define the &lt;code&gt;__call__&lt;/code&gt; method in the same class to call the function. This is the easiest way for me to solve it.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/8804830/python-multiprocessing-pickling-error"&gt;Python multiprocessing pickling error&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/1816958/cant-pickle-type-instancemethod-when-using-multiprocessing-pool-map/41959862#41959862"&gt;Can't pickle &lt;type 'instancemethod'&gt; when using multiprocessing Pool.map()&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/14175348/why-does-pythons-multiprocessing-module-import-main-when-starting-a-new-pro"&gt;Why does Python's multiprocessing module import &lt;strong&gt;main&lt;/strong&gt; when starting a new process on Windows?&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="multiprocessing"></category></entry><entry><title>An interesting random walk question and simulation 02</title><link href="/pages/2017/10/08/an-interesting-random-walk-question-and-simulation-02/" rel="alternate"></link><published>2017-10-08T16:08:00-05:00</published><updated>2017-10-08T16:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-10-08:/pages/2017/10/08/an-interesting-random-walk-question-and-simulation-02/</id><summary type="html">&lt;p&gt;Got some random walk questions from my friend. We may see lots of similar questions in stochastic process. Here is the question.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20171008_random_walk_02_01.jpg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;If we flip a coin, we will have 50% of chance getting head and 50% of chance getting tail. The question is: how many numbers of time shall we flip to get 3 consecutive heads?&lt;/p&gt;
&lt;p&gt;The question should also be asked with confidence. Before that, let's change it to a math question.&lt;/p&gt;
&lt;p&gt;We know &lt;span class="math"&gt;\(P(H) = 0.5\)&lt;/span&gt; and &lt;span class="math"&gt;\(P(T) = 0.5\)&lt;/span&gt;. Each flip is &lt;span class="math"&gt;\(i.i.d.\)&lt;/span&gt;. So it is easy to know that the probability that we get 3 consecutive Head is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
  &amp;amp; P(\mbox{3 consecutive Head}) \\
  &amp;amp; = P(H) \times P(H) \times P(H) \\
  &amp;amp; = \frac{1}{2} \times \frac{1}{2} \times \frac{1}{2} \\
  &amp;amp; = \frac{1}{8}
\end{aligned}&lt;/div&gt;
&lt;p&gt;But the question is not asking about the probability. It is asking about how many times we need to flip to get 3 consecutive heads.&lt;/p&gt;
&lt;p&gt;I try to write the distribution of the random variable but I cannot right now. So I did a simple simulation to estimate it. In the same way, I will ask the question as how many times shall I flip to get 3 consecutive heads with 90% confidence? The answer I got is 18.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np

def test():
    i = 1
    while True:
        x = np.random.randint(0, 2, 3)
        xsum = x.sum()
        if xsum == 3:
            #print(x)
            return i
        i += 1

res = []
for j in range(10000):
    res.append(test())

res = np.array(res)
np.percentile(res, 90)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;18.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And the average number of flips to get 3 consecutive heads is 8, which is consistent with our feeling because the probability of get this event is &lt;span class="math"&gt;\(\frac{1}{8}\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.mean(res)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;8.0066000000000006
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="random walk"></category></entry><entry><title>An interesting question and simulation 01</title><link href="/pages/2017/09/30/an-interesting-question-and-simulation-01/" rel="alternate"></link><published>2017-09-30T16:08:00-05:00</published><updated>2017-09-30T16:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-09-30:/pages/2017/09/30/an-interesting-question-and-simulation-01/</id><summary type="html">&lt;p&gt;An interesting question from my friend. We may see lots of similar questions in stochastic process. Here is one question.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="alt text" src="/figures/20170930_random_walk_01.jpeg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;Question: A man is standing on an infinite ladder. In one move, he can either go up (with probability 0.5), or stay where he is (with probability 0.5). The question is: &lt;font color="red"&gt;What is the number of moves the man need to take to move 100 steps up from his initial position&lt;/font&gt;?&lt;/p&gt;
&lt;p&gt;Let's transform this question to a math question: &lt;/p&gt;
&lt;p&gt;For any &lt;span class="math"&gt;\(t&amp;gt;0\)&lt;/span&gt;, &lt;span class="math"&gt;\(X_t\)&lt;/span&gt; is &lt;span class="math"&gt;\(i.i.d.\)&lt;/span&gt; Bernolli random variable with distribution&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
  X_{t} =
    \begin{cases}
      1, &amp;amp; \text{ prob = 0.5}\\
      0, &amp;amp; \text{ prob = 0.5}
    \end{cases}       
\end{equation}&lt;/div&gt;
&lt;p&gt;So we have &lt;span class="math"&gt;\(E(X) = \frac{1}{2}\)&lt;/span&gt; and &lt;span class="math"&gt;\(V(x) = \frac{1}{4}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Let us denote &lt;span class="math"&gt;\(Y_n = \sum_{t=1}^{n}X_t\)&lt;/span&gt;. Then &lt;span class="math"&gt;\(Y_n\)&lt;/span&gt; has binomial distribution &lt;span class="math"&gt;\(Bin \left(n, \frac{1}{2} \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Also, from Central Limit Theorem, we have &lt;span class="math"&gt;\(Y_n \sim \left(\frac{n}{2}, \frac{n}{4} \right)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;So for the queston that the man will be 100 steps from his initial position means &lt;span class="math"&gt;\(Y_n \ge 100\)&lt;/span&gt;. &lt;/p&gt;
&lt;p&gt;Since &lt;span class="math"&gt;\(Y_n\)&lt;/span&gt; is random, I think the question should be changed to &lt;font color="red"&gt;What is the number of moves the man need to take to move 100 steps up from his initial position with 95% confidence&lt;/font&gt;?&lt;/p&gt;
&lt;p&gt;Then the question becomes a math question&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
  &amp;amp;P(Y_n  \ge  100) = 0.95 \\    
  &amp;amp; \Rightarrow P(Y_n  \le  100) = 0.05  \\
  &amp;amp; \Rightarrow P\left( \frac{Y_n - \frac{n}{2}}{\sqrt{\frac{n}{4}}}   \le  \frac{100 - \frac{n}{2}}{\sqrt{\frac{n}{4}}} \right) = 0.05 \\
  &amp;amp; \Rightarrow P\left( Z  \le  \frac{100 - \frac{n}{2}}{\sqrt{\frac{n}{4}}} \right) = 0.05 \\
  &amp;amp; \Rightarrow \frac{100 - \frac{n}{2}}{\sqrt{\frac{n}{4}}} = Z_{0.05} = -1.645 \\
  &amp;amp; \Rightarrow n = 224.66
\end{aligned}&lt;/div&gt;
&lt;p&gt;That is, we have &lt;font color="red"&gt;95% confidence&lt;/font&gt; that &lt;font color="red"&gt;after 224 moves&lt;/font&gt; the man will be 100 steps up from his initial position.&lt;/p&gt;
&lt;p&gt;We can simulate this in python with these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;simulate 10000000 randon variables with 0.5 prob equal to 1, and 0.5 prob equal to 0&lt;/li&gt;
&lt;li&gt;calculate cumulative sum of these 10000000 simulated values&lt;/li&gt;
&lt;li&gt;cut them by 100, 200, 300, 400, 500, ... and so on&lt;/li&gt;
&lt;li&gt;count how many data points are there in each cut&lt;/li&gt;
&lt;li&gt;this count is the number of steps for random walk reaching 100&lt;/li&gt;
&lt;li&gt;calculate its percentile&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import pandas as pd

x = np.random.randint(0, 2, 10000000)
y = np.cumsum(x)
z = np.digitize(y, bins = (np.arange(10000000))*100)
w = pd.Series(z).groupby(z).size()
np.percentile(w, 95)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;224.0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Most of the time we saw the question asked in this way: if that man moves 200, what is the expected steps he is away from the original position. This question is very easy and we know the answer is 100. It is the expection of &lt;span class="math"&gt;\(Y_{200}\)&lt;/span&gt;, &lt;span class="math"&gt;\(E(Y_{200}) = 100\)&lt;/span&gt; .&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="random walk"></category></entry><entry><title>Data Engineering and Modeling 01: predict defaults with imbalanced data</title><link href="/pages/2017/09/23/data-engineering-and-modeling-01-predict-defaults-with-imbalanced-data/" rel="alternate"></link><published>2017-09-23T18:08:00-05:00</published><updated>2017-09-23T18:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-09-23:/pages/2017/09/23/data-engineering-and-modeling-01-predict-defaults-with-imbalanced-data/</id><summary type="html">&lt;p&gt;This is an real question with a sample data from internet. We want to predict the defaults from the imbalanced data(default rate is about 0.08%). All the variables are hidden so we need to explore the variables attributes to find some realtion. Also it is imbalanced so we need to apply some sampling methods to balance the data.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;0. Description&lt;/h2&gt;
&lt;p&gt;This data contains the recordings of a series of borrowers(based on id) from 2015-01-01 to 2015-11-02. The target variable is a categorical variable with value 0 or 1. target=1 means the borrower is default, otherwise it means the borrower is active. Totally there are 1168 unique borrower id.&lt;/p&gt;
&lt;p&gt;Each borrower will have one recording or no recording everyday. The minimum number of recording is 3 and the maximum number of recording is 304.&lt;/p&gt;
&lt;p&gt;For each recording, there are 9 independent variables named as x1 to x9.&lt;/p&gt;
&lt;p&gt;The job is to predict the target variable based on the independent variables.&lt;/p&gt;
&lt;h2&gt;1. Data View&lt;/h2&gt;
&lt;p&gt;The data looks like&lt;/p&gt;
&lt;table border="1" class="dataframe"&gt;  &lt;thead&gt;    &lt;tr style="text-align: right;"&gt;      &lt;th&gt;&lt;/th&gt;      &lt;th&gt;date&lt;/th&gt;      &lt;th&gt;id&lt;/th&gt;      &lt;th&gt;target&lt;/th&gt;      &lt;th&gt;x1&lt;/th&gt;      &lt;th&gt;x2&lt;/th&gt;      &lt;th&gt;x3&lt;/th&gt;      &lt;th&gt;x4&lt;/th&gt;      &lt;th&gt;x5&lt;/th&gt;      &lt;th&gt;x6&lt;/th&gt;      &lt;th&gt;x7&lt;/th&gt;      &lt;th&gt;x8&lt;/th&gt;      &lt;th&gt;x9&lt;/th&gt;    &lt;/tr&gt;  &lt;/thead&gt;  &lt;tbody&gt;    &lt;tr&gt;      &lt;th&gt;0&lt;/th&gt;      &lt;td&gt;2015-01-01&lt;/td&gt;      &lt;td&gt;S1F01085&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;215630672&lt;/td&gt;      &lt;td&gt;56&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;52&lt;/td&gt;      &lt;td&gt;6&lt;/td&gt;      &lt;td&gt;407438&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;7&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;1&lt;/th&gt;      &lt;td&gt;2015-01-01&lt;/td&gt;      &lt;td&gt;S1F0166B&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;61370680&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;3&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;6&lt;/td&gt;      &lt;td&gt;403174&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;2&lt;/th&gt;      &lt;td&gt;2015-01-01&lt;/td&gt;      &lt;td&gt;S1F01E6Y&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;173295968&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;12&lt;/td&gt;      &lt;td&gt;237394&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;3&lt;/th&gt;      &lt;td&gt;2015-01-01&lt;/td&gt;      &lt;td&gt;S1F01JE0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;79694024&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;6&lt;/td&gt;      &lt;td&gt;410186&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;4&lt;/th&gt;      &lt;td&gt;2015-01-01&lt;/td&gt;      &lt;td&gt;S1F01R2B&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;135970480&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;15&lt;/td&gt;      &lt;td&gt;313173&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;3&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;5&lt;/th&gt;      &lt;td&gt;2015-11-02&lt;/td&gt;      &lt;td&gt;Z1F0MA1S&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;18310224&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;10&lt;/td&gt;      &lt;td&gt;353705&lt;/td&gt;      &lt;td&gt;8&lt;/td&gt;      &lt;td&gt;8&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;6&lt;/th&gt;      &lt;td&gt;2015-11-02&lt;/td&gt;      &lt;td&gt;Z1F0Q8RT&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;172556680&lt;/td&gt;      &lt;td&gt;96&lt;/td&gt;      &lt;td&gt;107&lt;/td&gt;      &lt;td&gt;4&lt;/td&gt;      &lt;td&gt;11&lt;/td&gt;      &lt;td&gt;332792&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;13&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;7&lt;/th&gt;      &lt;td&gt;2015-11-02&lt;/td&gt;      &lt;td&gt;Z1F0QK05&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;19029120&lt;/td&gt;      &lt;td&gt;4832&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;11&lt;/td&gt;      &lt;td&gt;350410&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;8&lt;/th&gt;      &lt;td&gt;2015-11-02&lt;/td&gt;      &lt;td&gt;Z1F0QL3N&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;226953408&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;12&lt;/td&gt;      &lt;td&gt;358980&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;    &lt;tr&gt;      &lt;th&gt;9&lt;/th&gt;      &lt;td&gt;2015-11-02&lt;/td&gt;      &lt;td&gt;Z1F0QLC1&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;17572840&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;10&lt;/td&gt;      &lt;td&gt;351431&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;      &lt;td&gt;0&lt;/td&gt;    &lt;/tr&gt;  &lt;/tbody&gt;&lt;/table&gt;

&lt;ul&gt;
&lt;li&gt;Target Variable: target is the categorical variable with values 0 and 1&lt;/li&gt;
&lt;li&gt;Independent Variables: we can use all the other variables as the independent variables, including x1 to x9, even date and id if with proper transformation.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Some Observations&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;From the data we can see &lt;code&gt;x1&lt;/code&gt; and &lt;code&gt;x6&lt;/code&gt; are like numeric variable while the others are like categorical variable. We will do some analysis later.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;x2&lt;/code&gt;, &lt;code&gt;x3&lt;/code&gt;, &lt;code&gt;x7&lt;/code&gt;, &lt;code&gt;x8&lt;/code&gt;, &lt;code&gt;x9&lt;/code&gt; has lots of zeros &lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The first 3 characters of id might be useful to distinguish the target variable&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The variable date might be useful: we can get month information which might be useful, or we can get the time duration from first recording time to positive target time&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Some Extras&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Totally there are 124494 rows in the data. But there are only 1168 unique ids.&lt;/li&gt;
&lt;li&gt;Overall there are only 106 target=1. That is, the rate is about 0.08%.&lt;/li&gt;
&lt;li&gt;Each id only has at most one time target=1.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;1.1. Data summary and target info&lt;/h3&gt;
&lt;p&gt;There are 124494 rows of data in all but there is only 106 target=1. So the data is very imbalanced.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix
from sklearn.cross_validation import KFold
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import log_loss

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

%matplotlib inline
#get_ipython().magic('matplotlib inline')
plt.rcParams['figure.figsize'] = (20, 16)

mypath = r'/home/shm/projects/blog_study_draft/data//'
indata = pd.read_csv(mypath + r'defaults.csv')
&lt;/code&gt;&lt;/pre&gt;
&lt;script&gt;requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}&lt;/script&gt;

&lt;pre&gt;&lt;code class="language-python"&gt;print(indata.shape)
print(indata.target.sum())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(124494, 12)
106
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;1.2. Borrowers defaulted&lt;/h3&gt;
&lt;p&gt;Since there are many target=0. We will check how many borrowers defaulted and how many not.&lt;/p&gt;
&lt;p&gt;It shows there are 106 borrowers defaulted. The rest 1062 borrowers did not have any default during this observation time period.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;defaultid = indata[indata.target == 1].id.unique()

print(indata[indata.id.isin(defaultid)].id.unique().shape)
print(indata[~indata.id.isin(defaultid)].id.unique().shape)
print(indata.id.unique().shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(106,)
(1062,)
(1168,)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For furture use, I will pick out the defaulted borrowers to a standalone data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;defaults = indata[indata.id.isin(defaultid)]
print(defaults.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(10713, 12)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. Data Engineering&lt;/h2&gt;
&lt;h3&gt;2.1. Unique values for each variable&lt;/h3&gt;
&lt;p&gt;The data does not have a dictionary to indicate clearly what does the x mean. I will determine them based on their values information. &lt;/p&gt;
&lt;p&gt;x1 and x6 have lots of unique values, they are more like continuous variables. The other variables x are more like categorical variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# pd.concat([indata[:5], indata[-5:]], axis = 0).reset_index(drop=True).to_html()
for i in indata.columns:
    print(&amp;quot;For variable &amp;quot; + str(i) + &amp;quot;, there are &amp;quot; + str(len(indata[i].unique())) + &amp;quot; unique values&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;For variable date, there are 304 unique values
For variable id, there are 1168 unique values
For variable target, there are 2 unique values
For variable x1, there are 123878 unique values
For variable x2, there are 558 unique values
For variable x3, there are 47 unique values
For variable x4, there are 115 unique values
For variable x5, there are 60 unique values
For variable x6, there are 44838 unique values
For variable x7, there are 28 unique values
For variable x8, there are 28 unique values
For variable x9, there are 65 unique values
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;2.2. process variable date and id&lt;/h3&gt;
&lt;p&gt;First I will grab the month informtion to have a look if the defaults have any relation with month. Also I will do the same thing for the id.&lt;/p&gt;
&lt;p&gt;There are only 3 different values for the first 2 strings in id. I guess it is like geo info. I will check if will help to explain y.&lt;/p&gt;
&lt;p&gt;From the plot it shows month 5 and month 7 are higher than the ohter months. There is also period default trend by month. &lt;/p&gt;
&lt;p&gt;By id it shows w1 has higher default rate then z1. And z1 is higher than s1. Since extracted month and id0 are characters. I will replace them by the log(odds)&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;indata['month'] = indata.date.map(lambda x: x.split('-')[1])
indata['id0'] = indata.id.map(lambda x: x[:2])
defaults['month'] = defaults.date.map(lambda x: x.split('-')[1])
defaults['id0'] = defaults.id.map(lambda x: x[:2])

def attr_f1(df, x, name):
    print(&amp;quot;-&amp;quot;*20 + &amp;quot; this is for x &amp;quot; + x + &amp;quot;-&amp;quot;*20)
    a = df.groupby(x).target.mean().reset_index() 
    a.columns = [x, name]
    df = pd.merge(df, a, how = &amp;quot;left&amp;quot;, on = x)
    #df.loc[:, name] = np.log((df.loc[:, name] + 1e-8)/(1 - df.loc[:, name] + 1e-8))
    return df

indata = attr_f1(indata, &amp;quot;x2&amp;quot;, &amp;quot;a2&amp;quot;)
indata = attr_f1(indata, &amp;quot;x3&amp;quot;, &amp;quot;a3&amp;quot;)
indata = attr_f1(indata, &amp;quot;x4&amp;quot;, &amp;quot;a4&amp;quot;)
indata = attr_f1(indata, &amp;quot;x5&amp;quot;, &amp;quot;a5&amp;quot;)
indata = attr_f1(indata, &amp;quot;x7&amp;quot;, &amp;quot;a7&amp;quot;)
indata = attr_f1(indata, &amp;quot;x8&amp;quot;, &amp;quot;a8&amp;quot;)
indata = attr_f1(indata, &amp;quot;x9&amp;quot;, &amp;quot;a9&amp;quot;)
indata = attr_f1(indata, &amp;quot;id0&amp;quot;, &amp;quot;id1&amp;quot;)
indata = attr_f1(indata, &amp;quot;month&amp;quot;, &amp;quot;month0&amp;quot;)


defaults = attr_f1(defaults, &amp;quot;x2&amp;quot;, &amp;quot;a2&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x3&amp;quot;, &amp;quot;a3&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x4&amp;quot;, &amp;quot;a4&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x5&amp;quot;, &amp;quot;a5&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x7&amp;quot;, &amp;quot;a7&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x8&amp;quot;, &amp;quot;a8&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;x9&amp;quot;, &amp;quot;a9&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;id0&amp;quot;, &amp;quot;id1&amp;quot;)
defaults = attr_f1(defaults, &amp;quot;month&amp;quot;, &amp;quot;month0&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;indata.groupby('month').target.mean().plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_01.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;indata.groupby('id0').target.mean().plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_02.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;However, when I only look at the defaults data, I will find that month 5 and month 7 are not the highest default month. The trend is not exactly the same as the trend from the full data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;defaults.groupby('month').target.mean().plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_03.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;The transformed variable from id still works:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;defaults.groupby('id0').target.mean().plot()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_04.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;sns.factorplot(x = &amp;quot;x3&amp;quot;, hue = &amp;quot;target&amp;quot;, col = &amp;quot;id0&amp;quot;, 
               data = defaults[defaults.x3 != 0], kind = &amp;quot;count&amp;quot;, size = 8, aspect=1)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_05.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h3&gt;2.3. x1 and x6&lt;/h3&gt;
&lt;p&gt;Let us look at the tow continuous variable x1 and x6. I will scatter plot the default rate based on the grids from x1 and x6.&lt;/p&gt;
&lt;p&gt;target=0 will be colored as blue and target=1 will be colored as red. It shows most of the x1 are between 200000 to 500000. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;fig, ax = plt.subplots()
colors = {0:'#9bc3f6', 1:'#ff0000'}
ax.scatter(indata.x1, indata.x6, c = indata.target.apply(lambda x: colors[x]))
plt.show() 
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_06.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;fig, ax = plt.subplots()
colors = {0:'#9bc3f6', 1:'#ff0000'}
ax.scatter(defaults.x1, defaults.x6, c = defaults.target.apply(lambda x: colors[x]))
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_07.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h3&gt;2.4. Correlation&lt;/h3&gt;
&lt;p&gt;It will be helpful to check the correlation between x and y. Also it is helpful to check the correlation between all the x so that we can know if there is any multicollinearity or not.&lt;/p&gt;
&lt;p&gt;We can see there is high correlation between x7 and x8. Alao x9 and 3 has a correlation around 0.53.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cols = [&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, &amp;quot;x3&amp;quot;, &amp;quot;x4&amp;quot;, &amp;quot;x5&amp;quot;, &amp;quot;x6&amp;quot;, &amp;quot;x7&amp;quot;, &amp;quot;x8&amp;quot;, &amp;quot;x9&amp;quot;, &amp;quot;id1&amp;quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(indata[[&amp;quot;target&amp;quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_08.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cols = [&amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, &amp;quot;x3&amp;quot;, &amp;quot;x4&amp;quot;, &amp;quot;x5&amp;quot;, &amp;quot;x6&amp;quot;, &amp;quot;x7&amp;quot;, &amp;quot;x8&amp;quot;, &amp;quot;x9&amp;quot;, &amp;quot;id1&amp;quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(defaults[[&amp;quot;target&amp;quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_09.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;Also, we can check the correlation between y and the transformed values a.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cols = [&amp;quot;x1&amp;quot;, &amp;quot;a2&amp;quot;, &amp;quot;a3&amp;quot;, &amp;quot;a4&amp;quot;, &amp;quot;a5&amp;quot;, &amp;quot;x6&amp;quot;, &amp;quot;a7&amp;quot;, &amp;quot;a8&amp;quot;, &amp;quot;a9&amp;quot;, &amp;quot;id1&amp;quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(indata[[&amp;quot;target&amp;quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_10.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;cols = [&amp;quot;x1&amp;quot;, &amp;quot;a2&amp;quot;, &amp;quot;a3&amp;quot;, &amp;quot;a4&amp;quot;, &amp;quot;a5&amp;quot;, &amp;quot;x6&amp;quot;, &amp;quot;a7&amp;quot;, &amp;quot;a8&amp;quot;, &amp;quot;a9&amp;quot;, &amp;quot;id1&amp;quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(defaults[[&amp;quot;target&amp;quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_11.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h2&gt;3. Model&lt;/h2&gt;
&lt;h3&gt;3.1. imbalanced Data&lt;/h3&gt;
&lt;p&gt;Since there are only 106 defaults comparing to 1168 borrowers and 124494 observations, the proportion of target=1 is  very low. That is, our data is imbalanced data.&lt;/p&gt;
&lt;p&gt;If we used imbalanced data directly to build the model, we might be in trouble because the model will be ignoring the target=1 data if we use accuracy to measure the model performance.&lt;/p&gt;
&lt;p&gt;There are some methods to deal with the imbalanced data:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;oversampling: we repeat the low proportion data to make the proportion of target=1 and target=0 to be close in the oversampled data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;downsampling: unlike oversampling to increase the low proportion data, downsampling will try to sample from high proportion(target=1 here) to make the data balanced&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjust the low proportion data weight in the algorithm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjust the decision threshold of the output probability to classify&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;adjust the lost function if we want to give more weights on the low proportion data&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;3.2. Oversampling&lt;/h3&gt;
&lt;p&gt;Here we will do oversampling: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;we pick all the 10713 observations of 106 borrowers who has been defaulted. There are 106 target=1 with all the rest 10607 observations having target=0&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Second we will repeat the 106 positive sample 10607/106 times. After this, in the oversampled data, the ratio of 1 v.s. 0 is about half half.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After the new oversampled data is created, I will split them to training part and validation part.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import log_loss, auc, roc_auc_score

defaults_1 = defaults.query('target == 1')
defaults_0 = defaults.query('target == 0')
defaults_1_rep = pd.concat([defaults_1]*int(np.ceil(len(defaults_0)/len(defaults_1))), ignore_index = True, axis = 0)
defaults_new = pd.concat([defaults_0, defaults_1_rep], ignore_index = True, axis = 0)

cols = [&amp;quot;x1&amp;quot;, &amp;quot;a2&amp;quot;, &amp;quot;a3&amp;quot;, &amp;quot;a4&amp;quot;, &amp;quot;a5&amp;quot;, &amp;quot;x6&amp;quot;, &amp;quot;a7&amp;quot;, &amp;quot;a9&amp;quot;, &amp;quot;id1&amp;quot;, 'month0']
ftrain = defaults_new[cols]
fx_train = defaults_new[cols].values
fy_train = defaults_new[[&amp;quot;target&amp;quot;]].values.ravel()
print(np.sum(fy_train))

xtrain, xtest, ytrain, ytest = train_test_split(fx_train, fy_train, test_size = 0.4, random_state = 999)
print(ytest.sum())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;10600
4168
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3.3. First Model: RandomForestClassifier with growing number of estimatiors&lt;/h3&gt;
&lt;p&gt;RandomForest is the decision tree based algorithm which builds several decision trees and then combine their output to improve the ability of the model. The method of combining trees is known as ensemble method. &lt;font color="red"&gt;Ensembing means combination of weak learners to produce a stronger learner.&lt;/font&gt; &lt;/p&gt;
&lt;h4&gt;3.3.1. Growing RandomForest&lt;/h4&gt;
&lt;p&gt;RandomForstClassifier is a combination of many decision trees. In the first example, I will show how the &lt;a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic"&gt;AUC&lt;/a&gt; chages based on different number of decision trees. &lt;code&gt;n_estimators&lt;/code&gt; is the hyperparameter indicating how many decision trees will be used. We will start from 100 and increase 10 in each loop. Then we will draw the graph of AUC v.s. n_estimators. &lt;/p&gt;
&lt;p&gt;Another benifits is we can get the variable importances from the RandomForest output. The variable importances are measured by the total decrease of node impurities from splitting on the variable.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;auc = []
growing_rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = 10, 
                                    min_samples_leaf = 2, max_features = 'auto', 
                                    verbose = 0, n_jobs = -1, warm_start = True, random_state = 168)

for i in range(50):
    growing_rf.fit(xtrain, ytrain)
    growing_rf.n_estimators += 10
    auc.append(roc_auc_score(ytest, growing_rf.predict(xtest)))

_ = plt.plot(auc, '-r')
plt.title(&amp;quot;Growing RandomForest AUC on validation data&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_12.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;Feature importance plot shows that a2 is the most important feature, and a4 is the next most important.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;rf_importance = growing_rf.feature_importances_

feature_df = pd.DataFrame({'features': cols, 'RandomForest feature importances': rf_importance})

trace = go.Scatter(y = feature_df['RandomForest feature importances'].values,
                  x = feature_df['features'].values,
                  mode = &amp;quot;markers&amp;quot;,
                  marker = dict(sizemode = &amp;quot;diameter&amp;quot;,
                               sizeref = 1,
                               size = 25,
                               color = feature_df['RandomForest feature importances'].values,
                               colorscale = &amp;quot;Portland&amp;quot;,
                               showscale = True),
                  text = feature_df['features'].values)
data = [trace]

layout = go.Layout(autosize = True,
                  title = &amp;quot;Growing RandomForest Feature Importances&amp;quot;,
                  hovermode = &amp;quot;closest&amp;quot;,
                  yaxis = dict(title = &amp;quot;Feature Importances&amp;quot;,
                              ticklen = 5, 
                              gridwidth = 2),
                  showlegend = False)

fig = go.Figure(data = data, layout = layout)

py.iplot(fig, filename = 'scatter2010')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_13.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;Finally we will apply the built RandomForestClassifier on the original data set. We will get 31 defaults predicted correctly. There are 75 defaults are wrongly predicted as non-defaults. There are also 10 non-defaults are predicted as defaults.&lt;/p&gt;
&lt;p&gt;This result also show accuracy is not a good measure here as mentioned above because there are 124378 non-defaults are predicted correctly. So the overall accuracy is very high.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(confusion_matrix(indata.target, growing_rf.predict(indata[cols])))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[124384      4]
 [    86     20]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3.4. GridSearch Hyperparameters&lt;/h3&gt;
&lt;p&gt;RandomForestClassifier have several hyperparameters to choose.  &lt;code&gt;n_estimator&lt;/code&gt; will enable you to choose how many decision trees will be used. &lt;code&gt;max_depth&lt;/code&gt; will decide how deep the decision tree will be. &lt;code&gt;min_samples_leaf&lt;/code&gt; is the minimum number of samples required to be at a leaf node. &lt;/p&gt;
&lt;p&gt;We will test the combination of these hyperparameters and let the data decide which hyperparameter is the best. To do that, we first need to define the scoring function which will be the rule to select hyperparameters. From above the main issue here is we are likely to make type 1 and type 2 errors. So I will define a function to minimize the type 1 and type 2 predictions.  &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer

def myscoring(ground_truth, predictions):
    cmatrix = confusion_matrix(ground_truth, predictions)
    fp = cmatrix[0, 1]
    fn = cmatrix[1, 0]
    return  fn + fp

my_score = make_scorer(myscoring, greater_is_better = False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;estimator = RandomForestClassifier(random_state=0)

rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                       'min_samples_leaf': [2, 4, 10, 20]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = my_score)
cv_grid.fit(xtrain, ytrain)

best_parameters = cv_grid.best_estimator_.get_params()

for param_name in sorted(rf_tuned_parameters.keys()):
    print(&amp;quot;\t%s: %r&amp;quot; % (param_name, best_parameters[param_name]))
pred1 = cv_grid.predict(xtest)
print(&amp;quot;confustion matrix on validation data: &amp;quot; + str(confusion_matrix(ytest, pred1)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    max_depth: 20
    min_samples_leaf: 2
    n_estimators: 100
confustion matrix on validation data: [[4259   56]
 [   0 4168]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pred_full = cv_grid.predict(indata[cols])
print(confusion_matrix(indata.target, pred_full))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[124384      4]
 [    88     18]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3.5. ExtraTreeClassifier, AdaboostClassifier, GradientBoostingClassifier, LinearSVC, Logistic Regression&lt;/h3&gt;
&lt;p&gt;Next I will build some other classification learners. Then these models will be combined together as input for another model. It is like a new ensembling model is created.&lt;/p&gt;
&lt;p&gt;Each classifier has its own hyperparameters. It is better to grad search for finetuning hyperparameters and find the best hyperparameters. But that will take longer time to do. To save time I assigned the hyperparameter directly.&lt;/p&gt;
&lt;p&gt;The modeling data(oversampled data) will be split into 5 folders. Each time 4 folders data will be used to train a model and the left 1 folder will be used for test data. &lt;/p&gt;
&lt;h4&gt;3.5.1. Build Model&lt;/h4&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ntrain = fx_train.shape[0]
nfolds = 5
seed = 888
kf = KFold(ntrain, n_folds=nfolds, random_state=seed)

class SklearnClassifier(object):
    def __init__(self, clf, seed = 999, params = None):
        params['random_state'] = seed
        self.clf = clf(**params)

    def fit(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)

    def feature_importances(self, x, y):
        return self.clf.fit(x, y).feature_importances_

def get_oof(clf, x_train, y_train):
    oof_train = np.zeros((ntrain, ))
    for i, (train_index, test_index) in enumerate(kf):
        x_tr = x_train[train_index]
        y_tr = y_train[train_index]
        x_te = x_train[test_index]
        y_te = x_train[test_index]
        clf.fit(x_tr, y_tr)
        oof_train[test_index] = clf.predict(x_te)
    full_predict = clf.predict(indata[cols]).reshape(-1, 1)
    return (oof_train.reshape(-1, 1), full_predict)

# random forest parameters
rf_params = {'n_jobs': -1, 
            'n_estimators': 50,
            'warm_start': True,
            'max_depth': 50,
            'min_samples_leaf':2,
            'max_features': 'sqrt',
            'verbose':0}

# extra tree parameters
et_params = {'n_jobs': -1,
            'n_estimators': 100,
            'max_depth': 50,
            'min_samples_leaf': 2,
            'verbose':0}

# adaboost parameters
ada_params = {'n_estimators': 100,
             'learning_rate': 0.75}

# gradient boosting parameters
gb_params = {'n_estimators': 100,
            'max_features': 0.5,
            'max_depth': 50,
            'min_samples_leaf':2,
            'verbose':0}

# LinearSVC parameters
svc_params = {'C': 0.25}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;rf = SklearnClassifier(clf = RandomForestClassifier, seed = seed, params = rf_params)
et = SklearnClassifier(clf = ExtraTreesClassifier, seed = seed, params = et_params)
ada = SklearnClassifier(clf = AdaBoostClassifier, seed = seed, params = ada_params)
gb = SklearnClassifier(clf = GradientBoostingClassifier, seed = seed, params = gb_params)
svc = SklearnClassifier(clf = LinearSVC, seed = seed, params = svc_params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
startt = datetime.now()

(rf_oof_train, rf_oof_full) = get_oof(rf, fx_train, fy_train)  # 0:00:02.194219
(et_oof_train, et_oof_full) = get_oof(et, fx_train, fy_train)  # 0:00:05.207521
(ada_oof_train,  ada_oof_full) = get_oof(ada, fx_train, fy_train) # 0:00:27.385738
(gb_oof_train, gb_oof_full) = get_oof(gb, fx_train, fy_train)  # 0:00:16.449645
# SVM usually takes too long time to run, use LinearSVC
#https://stackoverflow.com/questions/40077432/scikit-learn-svm-svc-is-extremely-slow
(svc_oof_train, svc_oof_full) = get_oof(svc, fx_train, fy_train)

endt = datetime.now()
print(endt - startt)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0:00:30.987350
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;3.5.2. Feature Importances&lt;/h4&gt;
&lt;p&gt;Next we will get the feature importances and plot them. SVM does not have feature importances because it depends on the boundary data points(supporting vectors) on the splitting surface.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;rf_features = rf.feature_importances(fx_train, fy_train)
et_features = et.feature_importances(fx_train, fy_train)
ada_features = ada.feature_importances(fx_train, fy_train)
gb_features = gb.feature_importances(fx_train, fy_train)

feature_dataframe = pd.DataFrame({'features':cols, 
                                 'Random Forest feature importances': rf_features,
                                 'AdaBoost feature importances': ada_features,
                                 'Gradient Boost feature importances': gb_features,
                                 'Extra trees feature importances': et_features})
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def scatter_plot(picked_feature_name = 'Random Forest feature importances'):
    trace = go.Scatter(y = feature_dataframe[picked_feature_name].values,
                      x = feature_dataframe['features'].values,
                      mode = &amp;quot;markers&amp;quot;,
                      marker = dict(sizemode = &amp;quot;diameter&amp;quot;,
                                   sizeref = 1,
                                   size = 25,
                                   color = feature_dataframe[picked_feature_name].values,
                                   colorscale = &amp;quot;Portland&amp;quot;,
                                   showscale = True),
                      text = feature_dataframe['features'].values)
    data = [trace]

    layout = go.Layout(autosize = True,
                      title = picked_feature_name,
                      hovermode = &amp;quot;closest&amp;quot;,
                      yaxis = dict(title = &amp;quot;Feature Importances&amp;quot;,
                                  ticklen = 5, 
                                  gridwidth = 2),
                      showlegend = False)

    fig = go.Figure(data = data, layout = layout)

    py.iplot(fig, filename = 'scatter2010')


&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;3.5.2.1. RandomForestClassifier Feature Importances&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scatter_plot(picked_feature_name = 'Random Forest feature importances')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_14.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h5&gt;3.5.2.2. AdaBoost Feature Importances&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scatter_plot(picked_feature_name = 'AdaBoost feature importances')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_15.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h5&gt;3.5.2.3. Gradient Boost Feature Importances&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scatter_plot(picked_feature_name = 'Gradient Boost feature importances')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_16.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h5&gt;3.5.2.4. Extra trees Feature Importances&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;scatter_plot(picked_feature_name = 'Extra trees feature importances')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_17.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h3&gt;3.6. XGBoost&lt;/h3&gt;
&lt;p&gt;Finally we will build another model with inputs from the output of the previous models. (This is a little like multi-layers neural network which use previous layers output as current layer input).&lt;/p&gt;
&lt;p&gt;This time we will XGBoost to build the model. XGBoost is known for boosted tree learners. It optimizes large scale boosted tree. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;base_pred = pd.DataFrame({'randomforest': rf_oof_train.ravel(),
                                'extratree': et_oof_train.ravel(),
                                'adaboost': ada_oof_train.ravel(),
                                'gradientboost': gb_oof_train.ravel(), 
                                'linearsvc': svc_oof_train.ravel()})
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The correlation of the output from the previous model output is given below. We can see the correlation is not very high so we can safely use these output as input for XGBoost model.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;data = [
    go.Heatmap(z = base_pred.astype(float).corr().values,
               x = base_pred.columns.values,
               y = base_pred.columns.values,
               colorscale = &amp;quot;Portland&amp;quot;,
               showscale = True,
               reversescale = True
              )
]
py.iplot(data, filename = &amp;quot;labelled-Heatmap&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="jpgpng" src="/figures/20170917_aws_device_failure_18.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;xgb_x_train = np.concatenate((et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis = 1)
xgb_y_train = fy_train

import xgboost as xgb

gbm = xgb.XGBClassifier(
    n_estimators = 200,
    max_depth = 50,
    min_child_weight = 2,
    gamma = 0.6,
    subsample = 0.8,
    colsample_bytree = 0.8,
    objective = &amp;quot;binary:logistic&amp;quot;,
    nthread = -1,
    scale_pos_weight = 1
).fit(xgb_x_train, xgb_y_train)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;confusion_matrix(xgb_y_train, gbm.predict(xgb_x_train))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[10267,   340],
       [    0, 10600]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;GridSearch for All&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def gscv(estimator, tuned_parameters):
    cv_grid = GridSearchCV(estimator, param_grid = tuned_parameters, scoring = my_score)
    cv_grid.fit(xtrain, ytrain)

    best_parameters = cv_grid.best_estimator_.get_params()

    for param_name in sorted(tuned_parameters.keys()):
        print(&amp;quot;\t%s: %r&amp;quot; % (param_name, best_parameters[param_name]))
    pred1 = cv_grid.predict(xtest)
    print(&amp;quot;confustion matrix on validation data: &amp;quot; + str(confusion_matrix(ytest, pred1)))
    pred_full = cv_grid.predict(indata[cols])
    print(&amp;quot;confustion matrix on validation data: &amp;quot; + str(confusion_matrix(indata.target, pred_full)))
    return pred_full

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;rf_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
rf_estimator = RandomForestClassifier(random_state=0)
rf_out = gscv(rf_estimator, rf_tuned_parameters)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;max_depth: 20
min_samples_leaf: 2
n_estimators: 100

confustion matrix on validation data: [[4259   56] [   0 4168]]
confustion matrix on validation data: [[124384      4] [    88     18]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;et_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
et_estimator = ExtraTreesClassifier(random_state=0)
et_out = gscv(et_estimator, et_tuned_parameters)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;max_depth: 50
min_samples_leaf: 2
n_estimators: 200

confustion matrix on validation data: [[4241   74] [   0 4168]]
confustion matrix on validation data: [[124382      6] [    82     24]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ada_tuned_parameters = {'n_estimators': [20, 50, 100, 200], 'learning_rate': [0.2, 0.5, 1, 2, 5]}
ada_estimator = AdaBoostClassifier(random_state=0)
ada_out = gscv(ada_estimator, ada_tuned_parameters)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;learning_rate: 1
n_estimators: 200

confustion matrix on validation data: [[4026  289] [ 151 4017]]
confustion matrix on validation data: [[124388      0] [   105      1]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;gb_tuned_parameters = {&amp;quot;max_depth&amp;quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
gb_estimator = GradientBoostingClassifier(random_state=0)
gb_out = gscv(gb_estimator, gb_tuned_parameters)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;max_depth: 10
min_samples_leaf: 20
n_estimators: 200

confustion matrix on validation data: [[4287   28] [   0 4168]]
confustion matrix on validation data: [[124386      2] [    77     29]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;svc_tuned_parameters = {&amp;quot;C&amp;quot;: [0.2, 0.5, 1, 2]}
svc_estimator = LinearSVC(random_state=0)
svc_out = gscv(svc_estimator, svc_tuned_parameters)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;C: 0.2

confustion matrix on validation data: [[4315    0] [4168    0]]
confustion matrix on validation data: [[124388      0] [   106      0]]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;xgb_x_train_ = np.concatenate((rf_out.reshape(-1, 1), et_out.reshape(-1, 1), 
                               ada_out.reshape(-1, 1), gb_out.reshape(-1, 1), 
                               svc_out.reshape(-1, 1)), axis = 1)
xgb_y_train_ = indata.target

gbm = xgb.XGBClassifier(
    n_estimators = 200,
    max_depth = 50,
    min_child_weight = 2,
    gamma = 0.6,
    subsample = 0.8,
    colsample_bytree = 0.8,
    objective = &amp;quot;binary:logistic&amp;quot;,
    nthread = -1,
    scale_pos_weight = 1
).fit(xgb_x_train_, xgb_y_train_)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(confusion_matrix(indata.target, gbm.predict(xgb_x_train_)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[[124386     2]
 [    60     46]]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://scikit-learn.org/stable/modules/model_evaluation.html"&gt;sklearn score function&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.hackerearth.com/zh/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/"&gt;Practical Tutorial on Random Forest and Parameter Tuning in R&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="data mining"></category><category term="sklearn"></category><category term="data visualization"></category></entry><entry><title>Numpy Introduction 03</title><link href="/pages/2017/09/10/numpy-introduction-03/" rel="alternate"></link><published>2017-09-10T15:08:00-05:00</published><updated>2017-09-10T15:28:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-09-10:/pages/2017/09/10/numpy-introduction-03/</id><summary type="html">&lt;p&gt;continue to introduction some basic knowledge of numpy.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;0. Introduction&lt;/h2&gt;
&lt;p&gt;Continue to introduce a little about numpy following &lt;a href="http://songhuiming.github.io/pages/2015/09/26/numpy-introduction-01/"&gt;Numpy Introduction 01&lt;/a&gt; and &lt;a href="http://songhuiming.github.io/pages/2017/04/21/numpy-introduction-02/"&gt;Numpy Introduction 02&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;np.ndarray basics&lt;/li&gt;
&lt;li&gt;np.ndarray structure&lt;/li&gt;
&lt;li&gt;slicing, masking, fancy indexing and broadcasting&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;1. 矩阵的元素，维度和内存大小&lt;/h2&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170910_numpy_introduction_03_fig00.png" title="n"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np

x=  np.arange(2*3*2*4).reshape(2,3,2,4).astype('int32')
print(&amp;quot;type of x is : &amp;quot; + str(type(x)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;type of x is : &amp;lt;type 'numpy.ndarray'&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 不同的的dtype占用的内存大小不一样
print(&amp;quot;dtype of x is : &amp;quot;+ str(x.dtype))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dtype of x is : int32
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# x的维数,是一个4维的矩阵
print(&amp;quot;x是一个4维矩阵,所以维度是: &amp;quot; + str(x.ndim))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x是一个4维矩阵,所以维度是: 4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;#shape表示在每个维度上有多少个元素(item)
print(&amp;quot;x是一个4维矩阵,每个维度上item的个数为: &amp;quot; + str(x.shape))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x是一个4维矩阵,每个维度上item的个数为: (2, 3, 2, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# size表示总共有多少个元素(所有维度上shape的和)
print(&amp;quot;x总共有多少个item: &amp;quot; + str(x.size) + &amp;quot;: = 2x3x2x4&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;x总共有多少个item: 48: = 2x3x2x4
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# 每一个item占用的内存的大小，int16是2bytes，int32是4bytes，float64是8bytes
print(&amp;quot;每一个item占用的内存的大小: &amp;quot; + str(x.itemsize) + &amp;quot; bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;每一个item占用的内存的大小: 4 bytes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# np.ndarray所有item占用的总共内存
print(&amp;quot;所有item占用的总共内存: &amp;quot; + str(x.nbytes) + &amp;quot; bytes&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;所有item占用的总共内存: 192 bytes
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(x.strides)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(96, 32, 16, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. 矩阵的排列&lt;/h2&gt;
&lt;p&gt;numpy采用和C, java相似的方法：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先横着排(列优先，所以任何时候列总是最后一个维度，即-1维总是列)，np.arange(4)是一个行向量，占据4个列，shape为(4, ), shape[-1] = 4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;增加一个维度以后变成2维的，增加的维度在shape上的表现是加在左边，在矩阵上的表现是增加了行. &lt;code&gt;np.arange(2*4).reshape(2,4)&lt;/code&gt;是一个2行4列的矩阵。每一行在内存里面是相连的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;再增加一个维度以后变长3维的，增加的维度在shape上的表现是加在最左边，在矩阵上的表现是增加了depth. &lt;code&gt;np.arange(3*2*4).reshape(3, 2, 4)&lt;/code&gt;是depth为3，每个depth上是一个2行4列的矩阵&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;4d的时候，可以假想basic unit是3d的cube，然后把它们连成一行排起来，变成4d&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;同样，5d可以认为是在4d的基础上向下增加行，6d可以认为再增加depth，等等&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总之：np.ndarray 的-1维度(最后一个维度)总是column，倒数第二个维度是行，倒数第三个维度是depth，&lt;/p&gt;
&lt;h5&gt;2.1. First will occupy columns(one dimension)&lt;/h5&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170910_numpy_introduction_03_fig01.png" title="n"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.arange(4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([0, 1, 2, 3])
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;2.2. second will be row (down, two dimensions)&lt;/h5&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170910_numpy_introduction_03_fig02.png" title="n"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.arange(8).reshape(2, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[0, 1, 2, 3],
       [4, 5, 6, 7]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;2.3. third will be depth direction (3 dimensions, 3 arrays each shape is 2x4)&lt;/h5&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170910_numpy_introduction_03_fig03.png" title="n"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.arange(24).reshape(3, 2, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7]],

       [[ 8,  9, 10, 11],
        [12, 13, 14, 15]],

       [[16, 17, 18, 19],
        [20, 21, 22, 23]]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;2. 4. forth will be column blocks, each block is 3 dimension arrays&lt;/h5&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170910_numpy_introduction_03_fig04.png" title="n"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.arange(48).reshape(2, 3, 2, 4)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7]],

        [[ 8,  9, 10, 11],
         [12, 13, 14, 15]],

        [[16, 17, 18, 19],
         [20, 21, 22, 23]]],


       [[[24, 25, 26, 27],
         [28, 29, 30, 31]],

        [[32, 33, 34, 35],
         [36, 37, 38, 39]],

        [[40, 41, 42, 43],
         [44, 45, 46, 47]]]])
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;3. Slicing, Masking, Fancy Indexing and Broadcasting&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;slicing: like slicing in python list, e.g. &lt;code&gt;x[3:6]&lt;/code&gt;. Slicing is a reference, not a copy.&lt;/li&gt;
&lt;li&gt;masking: use boolean array indexing, e.g. &lt;code&gt;x[x &amp;gt; 2]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;fancy indexing: pass a list/array of indices, &lt;code&gt;x[[1,3,5]]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;broadcasting: a set of rules for array calculation with different dimension or shape size. broadcasting可以避免某些for循环，从而大大增加计算速度。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Broadcasting is a set of rules by which ufuncs operate on arrays of different sizes and/or dimensions.&lt;/p&gt;
&lt;p&gt;Broadcasting rules:
1. 如果两个array的ndim不一样，那么会在ndim比较小的array左边垫上1(类似newaxis)直到两个array的ndim一样
2. 两个array在所有的dim上必须满足：要么在每一维度上size相同，要么size为1(size=1的array在其维度上会类似copy的方式把它展开到跟另一个矩阵相同size)
3. broadcasting得到的array的shape是原来两个矩阵在各个维度上的shape的最大值&lt;/p&gt;
&lt;h4&gt;3.1. An example of Broadcasting&lt;/h4&gt;
&lt;p&gt;使用broadcasting的一个例子，nearest neighbour: x0是一个&lt;code&gt;100x3&lt;/code&gt;的矩阵，表示100个点，现在要求对这100个点里面的每一个给定的点，另外99个点里面哪个点跟这个给定的点欧氏距离最近。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9)
x0 = np.random.normal(0, 1, 300).reshape(100, 3)
print(x0.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(100, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面使用broadcasting来计算距离，这样可以避免for循环。&lt;/p&gt;
&lt;p&gt;首先把100x3的矩阵reshape成100x1x3(e.g., &lt;code&gt;x0.reshape(100, 1, 3)&lt;/code&gt;)，变成一个depth为100，一行3列的矩阵。这个矩阵减去原来100行3列的矩阵，根据broadcasting的规则，它会这么做：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;首先把ndim较小的矩阵在最左边增加一维，变成1x100x3，然后相减。这时候第一个矩阵在行上的size只有1，会被copy层成100(不是真正的copy，这儿只是借用copy这个说法)，所以第一个矩阵变成100x100x3。同样第一个矩阵1x100x3，也会在它的depth上被copy100次变成100x100x3。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;这样的结果就是初始的100x3的矩阵每一行都会跟这100x3的矩阵相减&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;最后得到的结果矩阵是100x100x3。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;diff = x0.reshape(100, 1, 3) - x0
print(diff.shape)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(100, 100, 3)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;distance = np.square(diff).sum(axis = -1)
i = np.arange(100)
distance[i, i] = np.inf

ind = np.argmin(distance, axis = 1)
print(ind[:10])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[73 22 11 21 21 71 28 28 41 31]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样的结果跟sklearn的NearestNeighbors得到的结果相同:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.neighbors import NearestNeighbors
dist_sk, ind_sk = NearestNeighbors().fit(x0).kneighbors(x0, 2)
print(ind_sk[:10, 1])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[73 22 11 21 21 71 28 28 41 31]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同样由上面我们也看到broadcasting的一个问题：大大增加了内存的使用。原来一个100x3的矩阵因为broadcasting，最后大小增加了100倍。如果矩阵的shape很大的时候，这样会消耗很多的内存。最后导致速度可能更慢而不是更快。&lt;/p&gt;
&lt;h3&gt;References&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html"&gt;Broadcasting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc"&gt;Array Broadcasting in numpy&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="numpy"></category></entry><entry><title>Build Recurrent Neural Network from Scratch</title><link href="/pages/2017/08/20/build-recurrent-neural-network-from-scratch/" rel="alternate"></link><published>2017-08-20T16:08:00-05:00</published><updated>2017-08-20T16:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-08-20:/pages/2017/08/20/build-recurrent-neural-network-from-scratch/</id><summary type="html">&lt;p&gt;The &lt;a href="http://songhuiming.github.io/pages/2017/08/12/build-neural-network-from-scratch/"&gt;previous blog&lt;/a&gt;  shows how to build a neural network manualy from scratch in numpy with matrix/vector multiply and add. Although there are many packages can do this easily and quickly with a few lines of scripts, it is still a good idea to understand the logic behind the packages. This part is from &lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;a good blog&lt;/a&gt; which use an example predicitng the words in the sentence to explain how to build RNN manually. RNN is a little more complicated than the neural network in the previous blog because the current time status and ourput in RNN will depends on the status in the previous time. So the Backpropagation part will be more complicated. I try to give the details in mathematic formula about how to get the gradients recursively in the partial derivatives.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;0. Introduction&lt;/h2&gt;
&lt;p&gt;The &lt;a href="http://songhuiming.github.io/pages/2017/08/12/build-neural-network-from-scratch/"&gt;previous blog&lt;/a&gt;  shows how to build a neural network manualy from scratch in numpy with matrix/vector multiply and add. Although there are many packages can do this easily and quickly with a few lines of scripts, it is still a good idea to understand the logic behind the packages. This part is from &lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;a good blog&lt;/a&gt; which use an example predicitng the words in the sentence to explain how to build RNN manually. RNN is a little more complicated than the neural network in the previous blog because the current time status and ourput in RNN will depends on the status in the previous time. So the Backpropagation part will be more complicated. I try to give the details in mathematic formula about how to get the gradients recursively in the partial derivatives.&lt;/p&gt;
&lt;p&gt;Recurrent neural network is one of the most popular neural networks for language modeling(based on existed words to predict next word) or automatic input like the automatic complete in the mobile input(based on existed character to predict next character).&lt;/p&gt;
&lt;p&gt;For example, when we build a RNN for language, that means: the training data is a list of sentences. Each sentence is a seris of words(tokenized words). For each sentence, from the first word, we will predict the second word. From the first and the second word, we will predict the third word, etc. Recurrent neural network means when it predict time order t, it will remember the information from time order 0 to time order t.&lt;/p&gt;
&lt;p&gt;Let's denote the sentence having &lt;span class="math"&gt;\(t+1\)&lt;/span&gt; words as &lt;span class="math"&gt;\(x=[x_0, x_1, \cdots, x_t]\)&lt;/span&gt;. We start from &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; to status &lt;span class="math"&gt;\(s_0 = \tanh(Ux_0 + Ws_{-1})\)&lt;/span&gt;, where &lt;span class="math"&gt;\(s_{-1}\)&lt;/span&gt; is the initialization of status initialized as 0. The output &lt;span class="math"&gt;\(o_0 = \mathrm{softmax}(Vs_0)\)&lt;/span&gt;. Then when we go to next word &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; we will have updated status &lt;span class="math"&gt;\(s_1 = \tanh(Ux_1 + Ws_0)\)&lt;/span&gt; and the corresponding output &lt;span class="math"&gt;\(o_1 = \mathrm{softmax}(Vs_1)\)&lt;/span&gt;. You will see at time order &lt;span class="math"&gt;\(t=1\)&lt;/span&gt; it not only depends on input &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; but also depends on the previous status &lt;span class="math"&gt;\(s_0\)&lt;/span&gt;. The equation for the RNN used in this tutorial is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
s_t &amp;amp;= \tanh(Ux_t + Ws_{t-1}) \\
o_t &amp;amp;= \mathrm{softmax}(Vs_t)
\end{aligned}&lt;/div&gt;
&lt;p&gt;If we plot the logic of RNN and the corresponding forward propagation, it is like
&lt;img alt="alt text" src="/figures/20170826_rnn_scratch_01_rnn.jpg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;The training data is 79,170 sentences coming from 15,000 reddit comments(one comment may has multiple sentences). The vocabulary consists of the 8,000 most common words. For the words not included in the vocabulary list are replaced by UNKNOWN_TOKEN.&lt;/p&gt;
&lt;p&gt;The words in each sentences are mapped to the index of the order of these words in the vocabulary list. So the training data &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(Y\)&lt;/span&gt; are both integers rather than strings. Since the purpose here is to predict the next word, so &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is just &lt;span class="math"&gt;\(X\)&lt;/span&gt; shifted by one leading position.&lt;/p&gt;
&lt;h2&gt;1. Build RNN&lt;/h2&gt;
&lt;h3&gt;1.0. Model and Data Structure, Initialization&lt;/h3&gt;
&lt;p&gt;As introduced before, the model structure of RNN used here is:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
s_t &amp;amp;= \tanh(Ux_t + Ws_{t-1}) \\
o_t &amp;amp;= \mathrm{softmax}(Vs_t)
\end{aligned}&lt;/div&gt;
&lt;p&gt;The vocabulary size &lt;span class="math"&gt;\(C=8,000\)&lt;/span&gt; and the hidden layer size &lt;span class="math"&gt;\(H=100\)&lt;/span&gt;. So the size of W is &lt;span class="math"&gt;\(100 \times 100\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let's assume one sentence has 10 words, for the corresponding mapped &lt;span class="math"&gt;\(x\)&lt;/span&gt;, we can treat it in two equal ways: 1. it is a python list by index of the words in the sentence. Then its length is the same as the number of words in that sentence, which is 10. we call it x_expression_1;   2. it is the one-hot transformation of these 10 words in the 8,000 vocabulary list. so its shape is [10, 8000]. For each row, there is one 1 in the word index position and the other 7999 positions will be all 0. we call it x_expression_2. So for each word of these 10 words, the matrix dot multiply &lt;span class="math"&gt;\(U.\)&lt;/span&gt;dot&lt;span class="math"&gt;\((x\_expression\_2[t])\)&lt;/span&gt; is the same as &lt;span class="math"&gt;\(U[:, x\_expression\_1[t]]\)&lt;/span&gt; for &lt;span class="math"&gt;\(t = \mathrm{range(10)}\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If use numpy to explain above, it is &lt;code&gt;np.arange(1, 21).reshape(4, 5).dot([0,1,0,0,0])&lt;/code&gt; == &lt;code&gt;np.arange(1, 21).reshape(4, 5)[:, 1]&lt;/code&gt;. This will save a lot of time because numpy indexing is much faster than matrix dot multiply.&lt;/p&gt;
&lt;p&gt;The dimension of all the data used is(below &lt;span class="math"&gt;\(x_t\)&lt;/span&gt; is index position in the vocabulary list for the &lt;span class="math"&gt;\(t_{th}\)&lt;/span&gt; word in the sentence, and &lt;span class="math"&gt;\(x\)&lt;/span&gt; is a sentence with &lt;span class="math"&gt;\(l\)&lt;/span&gt; words):&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
x_t &amp;amp;\in \mathbb{R}^{8000}  \Longleftrightarrow x \in \mathbb{R}^{l \times 8000} \\
o_t &amp;amp;\in \mathbb{R}^{8000}  \Longleftrightarrow o \in \mathbb{R}^{l \times 8000} \\
s_t &amp;amp;\in \mathbb{R}^{100}   \Longleftrightarrow s \in \mathbb{R}^{l \times 100} \\
U &amp;amp;\in \mathbb{R}^{100 \times 8000} \\
V &amp;amp;\in \mathbb{R}^{8000 \times 100} \\
W &amp;amp;\in \mathbb{R}^{100 \times 100} \\
\end{aligned}&lt;/div&gt;
&lt;p&gt;When we iterate to update the parameters, we need to set up their initial values. It is very important to set up suitable initializations to make RNN gradients work well. The initizalition of the parameters is initialized as from random uniform distribution &lt;span class="math"&gt;\(U\left(-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right)\)&lt;/span&gt; :&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import itertools
import operator
from datetime import datetime
import sys

vocabulary_size = 8000

X_train = np.load('X_train.npy')
y_train = np.load('y_train.npy')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## initialize parameters
class RNNNumpy():
    def __init__(self, word_dim, hidden_dim = 100, bptt_truncate = 4):
        # assign instance variable
        self.word_dim = word_dim
        self.hidden_dim = hidden_dim
        self.bptt_truncate = bptt_truncate
        # random initiate the parameters
        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))
        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))
        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Pay attention: the matrix &lt;span class="math"&gt;\(U, V, W\)&lt;/span&gt; are not initizalized as matrix with value 0 but random numbers. If you initilize them as 0, then you will get everythin 0 and they will not change in the loop. &lt;/p&gt;
&lt;h3&gt;1.1. Forward-Propagation&lt;/h3&gt;
&lt;p&gt;For a given sentence &lt;span class="math"&gt;\(x = (x_0, \cdots, x_{T-1})\)&lt;/span&gt; having &lt;span class="math"&gt;\(T\)&lt;/span&gt; words, we will start from &lt;span class="math"&gt;\(x_0\)&lt;/span&gt; with initialized &lt;span class="math"&gt;\(s_{-1} = 0\)&lt;/span&gt; to calculate &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; and &lt;span class="math"&gt;\(o_0\)&lt;/span&gt;, then from &lt;span class="math"&gt;\(s_0\)&lt;/span&gt; together with &lt;span class="math"&gt;\(x_1\)&lt;/span&gt; to get &lt;span class="math"&gt;\(s_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(o_1\)&lt;/span&gt;, and so on.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## 1. forward propagation

def softmax(x):
    xt = np.exp(x - np.max(x))
    return xt / np.sum(xt)

def forward_propagation(self, x):
    # total num of time steps, len of vector x
    T = len(x)
    # during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}
    # we also need the initial state of s, which is set to 0
    # each time step is saved in one row in s，each row in s is s[t] which corresponding to an rnn internal loop time
    s = np.zeros((T+1, self.hidden_dim))
    s[-1] = np.zeros(self.hidden_dim)
    # output at each time step saved as o, save them for later use
    o = np.zeros((T, self.word_dim))
    for t in np.arange(T):
        # we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector
        s[t] = np.tanh(self.U[:, x[t]] + self.W.dot(s[t-1]))
        o[t] = softmax(self.V.dot(s[t]))
    return [o, s]

RNNNumpy.forward_propagation = forward_propagation
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each &lt;span class="math"&gt;\(o_t\)&lt;/span&gt; here is a vector of prob representing the word in the vocabulary list. All we want is the next word with the predicted prob, we call it predict&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def predict(self, x):
    o, s = self.forward_propagation(x)
    return np.argmax(o, axis = 1)

RNNNumpy.predict = predict

np.random.seed(10)
model = RNNNumpy(vocabulary_size)
o, s = model.forward_propagation(X_train[10])
print(o.shape)
print(o)

predictions = model.predict(X_train[10])
print(predictions.shape)
print(predictions) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(45, 8000)
[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488
   0.00012508]
 [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456
   0.00012451]
 [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588
   0.00012551]
 ..., 
 [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494
   0.0001263 ]
 [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578
   0.00012502]
 [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536
   0.00012665]]
(45,)
[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539
   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21
 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;1.2. Lost Function&lt;/h3&gt;
&lt;p&gt;We will use &lt;a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"&gt;cross entropy&lt;/a&gt; loss function here. If we have &lt;span class="math"&gt;\(N\)&lt;/span&gt; training examples(words in the text) and &lt;span class="math"&gt;\(C\)&lt;/span&gt; classes(the size of the vocabulary list), then the lost function with respect to the prediction &lt;span class="math"&gt;\(o\)&lt;/span&gt; and the true lable &lt;span class="math"&gt;\(y\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
L(y, o) = - \frac{1}{N}\sum_{n \in N}y_n \log o_n
\end{aligned}&lt;/div&gt;
&lt;p&gt;How do we know this loss function makes sense? &lt;/p&gt;
&lt;p&gt;If everything is predicted correctly, that is: for the &lt;span class="math"&gt;\(y_i = 1\)&lt;/span&gt;, the corresponding &lt;span class="math"&gt;\(o_i = 1\)&lt;/span&gt;, then &lt;span class="math"&gt;\(1 \times \log(1) = 1 \times 0 = 0\)&lt;/span&gt;. For the rest &lt;span class="math"&gt;\(y_i = 0\)&lt;/span&gt;, the multiplied value &lt;span class="math"&gt;\(0 \times \log(o_i) = 0\)&lt;/span&gt;. So the loss will be 0 for the perfect function.&lt;/p&gt;
&lt;p&gt;On the other way, if the model have no prediction power, then all &lt;span class="math"&gt;\(o_i = 1/C\)&lt;/span&gt; then we will have &lt;span class="math"&gt;\(L(y, o) = - \frac{1}{N}\sum_{n \in N} \log \frac{1}{C} = \log(C)\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## 2. calculate the loss
'''
the loss is defined as
L(y, o) = -\frac{1}{N} \sum_{n \in N} y_n log(o_n)
'''
def calculate_total_loss(self, x, y):
    L = 0
    # for each sentence ...
    for i in np.arange(len(y)):
        o, s = self.forward_propagation(x[i])
        # we only care about our prediction of the &amp;quot;correct&amp;quot; words
        correct_word_predictions = o[np.arange(len(y[i])), y[i]]
        # add to the loss based on how off we were
        L += -1 * np.sum(np.log(correct_word_predictions))
    return L

def calculate_loss(self, x, y):
    # divide the total loss by the number of training examples
    N = np.sum((len(y_i) for y_i in y))
    return self.calculate_total_loss(x, y)/N

RNNNumpy.calculate_total_loss = calculate_total_loss
RNNNumpy.calculate_loss = calculate_loss

print(&amp;quot;Expected Loss for random prediction: %f&amp;quot; % np.log(vocabulary_size))
print(&amp;quot;Actual loss: %f&amp;quot; % model.calculate_loss(X_train[:1000], y_train[:1000]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Expected Loss for random prediction: 8.987197
Actual loss: 8.987440
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;1.3. Model Training with Backward-Propagation&lt;/h3&gt;
&lt;p&gt;Next we need to &lt;strong&gt;find the value of &lt;span class="math"&gt;\(U, V, W\)&lt;/span&gt; to minimize the loss function&lt;/strong&gt;. SGD is the common way to do this. The idad behind SGD is: we iterate over all the training examples and during each iteration &lt;strong&gt;we nudge the parameters into a direction that reduces the error&lt;/strong&gt;. &lt;strong&gt;The direction is given by the gradient of th loss &lt;span class="math"&gt;\(\frac{\partial{L}}{\partial{U}}, \frac{\partial{L}}{\partial{V}}, \frac{\partial{L}}{\partial{W}}\)&lt;/span&gt;&lt;/strong&gt;. SGD also needs &lt;strong&gt;learning rate&lt;/strong&gt;, which defines how big of a step we want to make in each iteration. You can refer to &lt;a href="http://cs231n.github.io/optimization-1/"&gt;here&lt;/a&gt; for SGD introduction. SGD is a common method used in many numeric solutions to optimize the function.&lt;/p&gt;
&lt;p&gt;Now the question becomes how to calculate the gradients &lt;span class="math"&gt;\(\frac{\partial{L}}{\partial{U}}, \frac{\partial{L}}{\partial{V}}, \frac{\partial{L}}{\partial{W}}\)&lt;/span&gt;. In the regular fully connected neural network, we use backpropagation to calculate it. In RNN it is a little more complicated because of the hidden status which links the current time step with the historical time step. So we need to calculate the gradients through the time. Thus we call this algorithm &lt;strong&gt;backpropagation through time&lt;/strong&gt;(BPTT). The parameters are shared in all the time steps, the gradients at each output will not only depends on the current time step, but also the previous time steps. For general introduction of backpropagation, please read &lt;a href="http://colah.github.io/posts/2015-08-Backprop/"&gt;this&lt;/a&gt; and &lt;a href="http://cs231n.github.io/optimization-2/"&gt;this&lt;/a&gt;. Here I will give some detailed math formula about the BPTT used in this post. &lt;/p&gt;
&lt;p&gt;Let's look at the graph of RNN below. Suppose now we are at time step &lt;span class="math"&gt;\(t=3\)&lt;/span&gt;, we want to calculate the gradients.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170826_rnn_scratch_02_bptt_with_gradients.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;To make it clear, I will write the forward propagation explitly:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
&amp;amp; s_0 = tanh(U x_0 + W s_{-1}) \\
&amp;amp; z_0 = V s_0   \\
&amp;amp; o_0 \triangleq \hat{y}_{0} = sigmoid(z_0) \\\\
&amp;amp; s_1 = tanh(U x_1 + W s_0) \\
&amp;amp; z_1 = V s_1   \\
&amp;amp; o_1 \triangleq \hat{y}_{1} = sigmoid(z_1) \\\\
&amp;amp; s_2 = tanh(U x_2 + W s_1) \\
&amp;amp; z_2 = V s_2   \\
&amp;amp; o_2 \triangleq \hat{y}_{2} = sigmoid(z_2) \\\\
&amp;amp; s_3 = tanh(U x_3 + W s_2) \\
&amp;amp; z_3 = V s_3   \\
&amp;amp; o_3 \triangleq \hat{y}_{3} = sigmoid(z_3) \\
\end{aligned}&lt;/div&gt;
&lt;p&gt;Also, for simplification, I will treat everything as scale rather then vectors or matrix. This will make the partial derivative easy to understand. After we understand this, we only need tiny modification to replace some multiplication by the array dot multiply.&lt;/p&gt;
&lt;p&gt;First, let's denote
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
&amp;amp; d_3 \triangleq \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \\
&amp;amp; d_2 \triangleq d_3 \cdot W \cdot \big(1 - s_2 ^ 2 \big) \\
&amp;amp; d_1 \triangleq d_2 \cdot W \cdot \big(1 - s_1 ^ 2 \big) \\
&amp;amp; d_0 \triangleq d_1 \cdot W \cdot \big(1 - s_0 ^ 2 \big) \\
\end{aligned}&lt;/div&gt;
&lt;h4&gt;1.3.1. Calculate of partial derivative of error &lt;span class="math"&gt;\(E_3\)&lt;/span&gt; to &lt;span class="math"&gt;\(U\)&lt;/span&gt;: &lt;span class="math"&gt;\(\frac{\partial{L}}{\partial{U}}\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;We have already know each historical hidden status will be used to calculate current status. The parameters are used in each status. So we need to calculate all the hidden status to the parameter &lt;span class="math"&gt;\(U\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{s_0}}{\partial{U}} &amp;amp;= \big(1 - s_0 ^ 2 \big) \left(x_0 + \frac{\partial{s_{-1}}}{\partial{U}} \right) \\
&amp;amp;= \big(1 - s_0 ^ 2 \big) \cdot x_0 \\\\
\frac{\partial{s_1}}{\partial{U}} &amp;amp;= \big(1 - s_1 ^ 2 \big) \left(x_1 + W \cdot \frac{\partial{s_{0}}}{\partial{U}} \right) \\
&amp;amp;= \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\\\
\frac{\partial{s_2}}{\partial{U}} &amp;amp;= \big(1 - s_2 ^ 2 \big) \left(x_2 + W \cdot \frac{\partial{s_{1}}}{\partial{U}} \right) \\
&amp;amp;= \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)\\\\
\frac{\partial{s_3}}{\partial{U}} &amp;amp;= \big(1 - s_3 ^ 2 \big) \left(x_3 + W \cdot \frac{\partial{s_{2}}}{\partial{U}} \right) \\
&amp;amp;= \big(1 - s_3 ^ 2 \big) \\
&amp;amp; \bigg(x_3 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)  \bigg)\\\\
\end{aligned}&lt;/div&gt;
&lt;p&gt;After we get this, we can calculate the partial derivative of error &lt;span class="math"&gt;\(E3\)&lt;/span&gt; to &lt;span class="math"&gt;\(U\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{E_3}}{\partial{U}} &amp;amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{s_3}} \frac{\partial{s_3}}{\partial{U}}  \\
&amp;amp;= \left(\frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \right) \cdot \frac{\partial{z_3}}{\partial{s_3}} \cdot \frac{\partial{s_3}}{\partial{U}}  \\
&amp;amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \frac{\partial{s_3}}{\partial{U}}  \\
&amp;amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \bigg(x_3 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)  \bigg)\\
&amp;amp; \triangleq d_3 \big[x_3 + W \cdot  \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)   \big]\\
&amp;amp;= d_3 x_3 + d_3 W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big) \\
&amp;amp; \triangleq d_3 x_3 + d_2 \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big)\Big) \\
&amp;amp;= d_3 x_3 + d_2 x_2 + d_2 W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\
&amp;amp; \triangleq d_3 x_3 + d_2 x_2 + d_1 \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\
&amp;amp;= d_3 x_3 + d_2 x_2 + d_1 x_1 + d_1 W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0  \\
&amp;amp; \triangleq d_3 x_3 + d_2 x_2 + d_1 x_1 + d_0 \cdot x_0  \\
\end{aligned}&lt;/div&gt;
&lt;h4&gt;1.3.2. Calculate of partial derivative of error &lt;span class="math"&gt;\(E_3\)&lt;/span&gt; to &lt;span class="math"&gt;\(W\)&lt;/span&gt;: &lt;span class="math"&gt;\(\frac{\partial{L}}{\partial{W}}\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;First is each hidden status to &lt;span class="math"&gt;\(W\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{s_0}}{\partial{W}} &amp;amp;= \big(1 - s_0 ^ 2 \big) \left(s_{-1} + \frac{\partial{s_{-1}}}{\partial{W}} \right) \\
&amp;amp;= \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \\\\
\frac{\partial{s_1}}{\partial{W}} &amp;amp;= \big(1 - s_1 ^ 2 \big) \left(s_0 + W \cdot \frac{\partial{s_{0}}}{\partial{W}} \right) \\
&amp;amp;= \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\\\
\frac{\partial{s_2}}{\partial{W}} &amp;amp;= \big(1 - s_2 ^ 2 \big) \left(s_1 + W \cdot \frac{\partial{s_{1}}}{\partial{W}} \right) \\
&amp;amp;= \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)\\\\
\frac{\partial{s_3}}{\partial{W}} &amp;amp;= \big(1 - s_3 ^ 2 \big) \left(s_2 + W \cdot \frac{\partial{s_{2}}}{\partial{W}} \right) \\
&amp;amp;= \big(1 - s_3 ^ 2 \big) \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)\\\\
\end{aligned}&lt;/div&gt;
&lt;p&gt;Then we can write the partial derivative of error &lt;span class="math"&gt;\(E3\)&lt;/span&gt; to &lt;span class="math"&gt;\(W\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{E_3}}{\partial{W}} &amp;amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{s_3}} \frac{\partial{s_3}}{\partial{W}}  \\
&amp;amp;= \left(\frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \right) \cdot \frac{\partial{z_3}}{\partial{s_3}} \cdot \frac{\partial{s_3}}{\partial{W}}  \\
&amp;amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \frac{\partial{s_3}}{\partial{W}}  \\
&amp;amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)\\
&amp;amp; \triangleq d_3    \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)   \\
&amp;amp;= d_3 s_2 + d_3 W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big) \\
&amp;amp; \triangleq d_3 s_2 + d_2 \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big) \\
&amp;amp;= d_3 s_2 + d_2 s_1 + d_2 W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\
&amp;amp; \triangleq d_3 s_2 + d_2 s_1 + d_1 \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\
&amp;amp;= d_3 s_2 + d_2 s_1 + d_1 s_0 + d_1 W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1}  \\
&amp;amp; \triangleq d_3 s_2 + d_2 s_1 + d_1 s_0 + d_0 \cdot s_{-1}  \\
\end{aligned}&lt;/div&gt;
&lt;h4&gt;1.3.3. Calculate of partial derivative of error &lt;span class="math"&gt;\(E_3\)&lt;/span&gt; to &lt;span class="math"&gt;\(V\)&lt;/span&gt;: &lt;span class="math"&gt;\(\frac{\partial{L}}{\partial{V}}\)&lt;/span&gt;&lt;/h4&gt;
&lt;p&gt;This will be easier than the two above:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{E_3}}{\partial{V}} &amp;amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{V}} \\
&amp;amp;= (\hat{y}_{3} - y_3)  s_3
\end{aligned}&lt;/div&gt;
&lt;p&gt;This simplifies the equations from vector and matrix multiply to scale multiply. But with this it can easily revert back to vertor and matrix, just need to pay attention of the orders, shape, transformation and dot multiply.&lt;/p&gt;
&lt;p&gt;From the derivatives formula above, we can easily write the BPTT in python:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## 3. BPTT
'''
1. we nudge the parameters into a direction that reduces the error. the direction is given by the gradient of the loss: \frac{\partial L}{\partial U}, 
\frac{\partial L}{\partial V}, \frac{\partial L}{\partial W}
2. we also need learning rate: which indicated how big of a step we want to make in each direction
Q: how to optimize SGD using batching, parallelism and adaptive learning rates.

RNN BPTT: because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the
current time step, but also the previous time steps.
'''

def bptt(self, x, y):
    T = len(y)
    # perform forward propagation
    o, s = self.forward_propagation(x)
    # we will accumulate the gradients in these variables
    dLdU = np.zeros(self.U.shape)
    dLdV = np.zeros(self.V.shape)
    dLdW = np.zeros(self.W.shape)
    delta_o = o
    delta_o[np.arange(len(y)), y] -= 1   # it is y_hat - y
    # for each output backwards ...
    for t in np.arange(T):
        dLdV += np.outer(delta_o[t], s[t].T)    # at time step t, shape is word_dim * hidden_dim
        # initial delta calculation
        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ^ 2))
        # backpropagation through time (for at most self.bptt_truncate steps)
        # given time step t, go back from time step t, to t-1, t-2, ...
        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:
            # print(&amp;quot;Backprogation step t=%d bptt step=%d&amp;quot; %(t, bptt_step))
            dLdW += np.outer(delta_t, s[bptt_step - 1])
            dLdU[:, x[bptt_step]] += delta_t
            # update delta for next step
            dleta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1]^2)
    return [dLdU, dLdV, dLdW]

RNNNumpy.bptt = bptt
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;When we implement the backpropagaton it is good idea to also implement gradient checking. The idad behind gradient checking is that derivative of a parameter is equal to the slope at that point, which we can approximate by slighyly changing the parameter and then dividing by the change:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
\frac{\partial{L}}{\partial{\theta}} = \lim_{h \rightarrow 0} \frac{L(\theta + h) - L(\theta - h)}{2h}
\end{aligned}&lt;/div&gt;
&lt;p&gt;We will compare the calculate gradient using the limitation approaching formula above with the gradients from the derivatives. They should be very close. The approximation needs to calculate the total loss for every parameter, so the gradient checking is very expensive. So it is a good idea to perform it on a model with a smaller vocabulary.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;### 3.1 gradient checking
'''
verify the gradient by its definition:
\frac{\partial{L}}{\partial{\theta}} = \lim_{h \propto 0} \frac{J(\theta + h) - J(\theta - h)}{2h}
'''
def gradient_check(self, x, y, h = 0.001, error_threshold = 0.01):
    # calculate the gradient using backpropagation
    bptt_gradients = self.bptt(x, y)
    # list of all params we want to check
    model_parameters = [&amp;quot;U&amp;quot;, &amp;quot;V&amp;quot;, &amp;quot;W&amp;quot;]
    # gradient check for each parameter
    for pidx, pname in enumerate(model_parameters):
        # get the actual parameter value from model, e.g. model.W
        parameter = operator.attrgetter(pname)(self)
        print(&amp;quot;performing gradient check for parameter %s with size %d. &amp;quot; %(pname, np.prod(parameter.shape)))
        # iterate over each element of the parameter matrix, e.g. (0,0), (0,1)...
        it = np.nditer(parameter, flags = ['multi_index'], op_flags=['readwrite'])
        while not it.finished:
            ix = it.multi_index
            # save the original value so we can reset it later
            original_value = parameter[ix]
            # estimate the gradient using (f(x+h) - f(x-h))/2h
            parameter[ix] = original_value + h
            gradplus = self.calculate_total_loss([x], [y])
            parameter[ix] = original_value - h
            gradminus = self.calculate_total_loss([x], [y])
            estimated_gradient = (gradplus - gradminus)/(2*h)
            # reset parameter to the original value
            parameter[ix] = original_value
            # the gradient for this parameter calculated using backpropagation
            backprop_gradient = bptt_gradients[pidx][ix]
            # calculate the relative error (|x - y|)/(|x|+|y|)
            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))
            # if the error is too large fail the gradient check
            if relative_error &amp;lt; error_threshold:
                print(&amp;quot;Gradient check error: parameter = %s ix = %s&amp;quot; %(pname, ix))
                print(&amp;quot;+h Loss: %f&amp;quot; % gradplus)
                print(&amp;quot;-h Loss: %f&amp;quot; % gradminus)
                print(&amp;quot;Estimated gradient: %f&amp;quot; % estimated_gradient)
                print(&amp;quot;Backpropagation gradient: %f&amp;quot; % backprop_gradient)
                print(&amp;quot;Relative error: %f&amp;quot; % relative_error)
                return
            it.iternext()
        print(&amp;quot;Gradient check for parameter %s passed. &amp;quot; %(pname))

RNNNumpy.gradient_check = gradient_check

grad_check_vocab_size = 100
np.random.seed(10)
model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate = 1000)
model.gradient_check([0,1,2,3], [1,2,3,4])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;performing gradient check for parameter U with size 1000. 
Gradient check error: parameter = U ix = (0, 3)
+h Loss: 18.307080
-h Loss: 18.307296
Estimated gradient: -0.108091
Backpropagation gradient: -0.108091
Relative error: 0.000000
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. SGD Implementation&lt;/h2&gt;
&lt;p&gt;Now that we can calculate the gradients for our parameters we can implement SGD in two steps: 1. A function &lt;em&gt;sgd_step&lt;/em&gt; that calculate the gradients and performs the updates for one batch(here one batch is one sentence). 2. An output loop that iterates through the training data and adjust the learning rate.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## 4. SGD implementation
'''
two step:
1. calculate the gradients and perform the updates for one batch
2. loop through the training set and adjust the learning rate
'''
### 4.1. perform one step of SGD
def numpy_sgd_step(self, x, y, learning_rate):
    dLdU, dLdV, dLdW = self.bptt(x, y)
    self.U -= learning_rate * dLdU
    self.V -= learning_rate * dLdV
    self.W -= learning_rate * dLdW
RNNNumpy.sgd_step = numpy_sgd_step

### 4.2. outer SGD loop
'''
 - model: 
 - X_train:
 - y_train:
 - learning_rate:
 - nepoch:
 - evaluate loss_after:
'''
def train_with_sgd(model, X_train, y_train, learning_rate = 0.005, nepoch = 100, evaluate_loss_after = 5):
    # keep track of the losses so that we can plot them later
    losses = []
    num_examples_seen = 0
    for epoch in range(nepoch):
        # optionally evaluate the loss
        if (epoch % evaluate_loss_after == 0):
            loss = model.calculate_loss(X_train, y_train)
            losses.append((num_examples_seen, loss))
            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            print(&amp;quot;%s: loss after num_examples_seen=%d epoch=%d: %f&amp;quot; %(time, num_examples_seen, epoch, loss))
            # adjust the learning rate if loss increases
            if (len(losses) &amp;gt; 1 and losses[-1][1] &amp;gt; losses[-2][1]):
                learning_rate = learning_rate * 0.5
                print(&amp;quot;setting learning rate to %f&amp;quot; %(learning_rate))
            sys.stdout.flush()
        # for each training example...
        for i in range(len(y_train)):
            # one sgd step
            model.sgd_step(X_train[i], y_train[i], learning_rate)
            num_examples_seen += 1

np.random.seed(10)
model = RNNNumpy(vocabulary_size)
%timeit model.sgd_step(X_train[10], y_train[10], 0.005)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1 loop, best of 3: 175 ms per loop
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(10)
model = RNNNumpy(vocabulary_size)
losses = train_with_sgd(model, X_train[:100], y_train[:100], nepoch = 10, evaluate_loss_after = 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;2017-09-21 02:59:52: loss after num_examples_seen=0 epoch=0: 8.987425
2017-09-21 03:00:04: loss after num_examples_seen=100 epoch=1: 8.974076
2017-09-21 03:00:18: loss after num_examples_seen=200 epoch=2: 8.943971
2017-09-21 03:00:30: loss after num_examples_seen=300 epoch=3: 6.892136
2017-09-21 03:00:40: loss after num_examples_seen=400 epoch=4: 6.351962
2017-09-21 03:00:50: loss after num_examples_seen=500 epoch=5: 6.107587
2017-09-21 03:01:00: loss after num_examples_seen=600 epoch=6: 5.960636
2017-09-21 03:01:11: loss after num_examples_seen=700 epoch=7: 5.858011
2017-09-21 03:01:22: loss after num_examples_seen=800 epoch=8: 5.781543
2017-09-21 03:01:32: loss after num_examples_seen=900 epoch=9: 5.722384
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Since this RNN is implemented in python without code optimization, the running time is pretty long for our 79,170 words in each epoch. But we can try a small sample data and check if the loss actually decreases:&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/"&gt;Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="deep learning"></category></entry><entry><title>Build Neural Network from Scratch</title><link href="/pages/2017/08/12/build-neural-network-from-scratch/" rel="alternate"></link><published>2017-08-12T15:08:00-05:00</published><updated>2017-08-12T15:08:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-08-12:/pages/2017/08/12/build-neural-network-from-scratch/</id><summary type="html">&lt;p&gt;Use two examples to show how to build the neural network from scratch: define the activation function on each layer, define the lost function, and calculate the partial derivatives using chain rules.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;How to build Neural Network from scratch&lt;/h2&gt;
&lt;p&gt;This post will introduce how to build a neural network from stratch: &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the forward-propagation: from input data to activations on each layer&lt;/li&gt;
&lt;li&gt;the backpropagation: using chain rules to calculate the deravatives of cost function to weights(parameters) on each layer&lt;/li&gt;
&lt;li&gt;minimize the cost function by SGD with the derivatives from step 2.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Example 1.&lt;/h3&gt;
&lt;p&gt;First I will introduce a very simple example. Suppose we have data like the following. &lt;span class="math"&gt;\(X\)&lt;/span&gt; is a 4x3 array. &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is a 4x1 vector. The task is to predict the output &lt;span class="math"&gt;\(Y\)&lt;/span&gt; based on the input &lt;span class="math"&gt;\(X\)&lt;/span&gt;. We will show how to build a neural network to do this job.&lt;/p&gt;
&lt;table cellspacing="10" width="400"&gt;
  &lt;tr&gt;
    &lt;td colspan="3"  align="center" valign="middle"&gt;Inputs: X&lt;/td&gt;
    &lt;td  align="center" valign="middle"&gt;Output: Y&lt;/td&gt;
  &lt;/tr&gt;

  &lt;tr&gt;
    &lt;td align="center" valign="middle"&gt;0 &lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;0 &lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1 &lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
  &lt;/tr&gt; 
  &lt;tr&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;1&lt;/td&gt;
    &lt;td align="center" valign="middle"&gt;0&lt;/td&gt;
  &lt;/tr&gt; 
&lt;/table&gt;

&lt;p&gt;Our NN will have 2 layers. The first layer is from input &lt;span class="math"&gt;\(X\)&lt;/span&gt; (we will call it &lt;span class="math"&gt;\(a_1\)&lt;/span&gt;) to activations &lt;span class="math"&gt;\(a_2\)&lt;/span&gt; with the sigmoid activation function. The shape of the weights for the first layer (&lt;span class="math"&gt;\(\Theta_1^{T}\)&lt;/span&gt;) is 4x3. So the shape of layer 1 is 4x4.&lt;/p&gt;
&lt;p&gt;The layer 1 (&lt;span class="math"&gt;\(a_2\)&lt;/span&gt;) will be the input for the second layer(&lt;span class="math"&gt;\(a_3\)&lt;/span&gt;) with weights parameter &lt;span class="math"&gt;\(\Theta_2^{T}\)&lt;/span&gt; and sigmoid activation function. The shape of &lt;span class="math"&gt;\(\Theta_2^{T}\)&lt;/span&gt; is 4x1 (the shape of input data to layer 2 is 4x4 and the shape of &lt;span class="math"&gt;\(Y\)&lt;/span&gt; is 4x1, these two will determine the shape of &lt;span class="math"&gt;\(\Theta_2\)&lt;/span&gt; because layer 2 is the output layer here).&lt;/p&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;h3&gt;1.1. Model Structure&lt;/h3&gt;
&lt;p&gt;Our prediction Neural Network will have two layers: the first layer a linear combination between input &lt;span class="math"&gt;\(X\)&lt;/span&gt; and parameter &lt;span class="math"&gt;\(\theta_1\)&lt;/span&gt;. Then this will be the input to the &lt;a href="https://en.wikipedia.org/wiki/Sigmoid_function"&gt;sigmoid&lt;/a&gt; function, which is the second layer. &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    a_1 &amp;amp;= X \\
    z_1 &amp;amp;= a_1 \cdot \Theta_1^{T}  \\
    a_2 &amp;amp;= \mbox{sigmoid}(z_1) = \frac{1}{1 + e^{- z_1}}  \qquad \mbox{the first activation layer} \\
    z_2 &amp;amp;= a_2 \cdot \Theta_2^{T} \\
    a_3 &amp;amp;= \mbox{sigmoid}(z_2) = \frac{1}{1 + e^{- z_2}}  \qquad \mbox{the second activation layer}
\end{aligned}&lt;/div&gt;
&lt;p&gt;The symbal &lt;span class="math"&gt;\(\cdot\)&lt;/span&gt; above means &lt;a href="https://en.wikipedia.org/wiki/Dot_product"&gt;dot product&lt;/a&gt; of the vector or the matrix. We also need the [elementwise product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices) denoted as &lt;span class="math"&gt;\(\odot\)&lt;/span&gt;.&lt;/p&gt;
&lt;h3&gt;1.2. Cost Function&lt;/h3&gt;
&lt;p&gt;Cost function generally measures the distance between the predictions and the true value. &lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression"&gt;Cross Entropy&lt;/a&gt; Loss function&lt;/strong&gt;:
&lt;div class="math"&gt;\begin{aligned}
     C = -\frac{1}{n}\sum_{x}\left[y \log(a_3) + (1 - y)\log(1 - a_3) \right]  
\end{aligned}&lt;/div&gt;
&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;L2 cost function&lt;/strong&gt;:&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;\begin{aligned}
    C = \frac{1}{2}||y - a_3||^2  
\end{aligned}&lt;/div&gt;
&lt;h3&gt;1.3. Derivatives / Gradient&lt;/h3&gt;
&lt;p&gt;Suppose the Lost function is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    C = \frac{1}{2} ||y - a_3||^2   
\end{aligned}&lt;/div&gt;
&lt;p&gt;First, let's denote
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    d_3 &amp;amp;= (a_3 - y) \odot (a_3 \odot (1 - a_3))  ,\; \; \mbox{ or call it } a_3\_delta \\
    d_2 &amp;amp;= d_3 \cdot \Theta_2 \odot (a_2 \odot (1 - a_2)) 
\end{aligned}&lt;/div&gt;
&lt;p&gt;The partival derivative of Loss function to parameter &lt;span class="math"&gt;\(\Theta_2\)&lt;/span&gt; using chain rule is:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_2}} &amp;amp; = \frac{\partial{C}}{\partial{a_3}} \frac{\partial{a_3}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{\Theta_2}} \\
    &amp;amp; = (a_3 - y) \left(a_3 (1 - a_3)\right) a_2  \; \; (\mbox{if all are scalers}) \\
    &amp;amp; =  \left[(a_3 - y) \odot \left(a_3 \odot (1 - a_3)\right)\right]^{T} \cdot a_2 \\
    &amp;amp;  \triangleq   d_3^{T} \cdot a_2  
\end{aligned}&lt;/div&gt;
&lt;p&gt;where  &lt;span class="math"&gt;\(d_3 \triangleq (a_3 - y) \odot \left(a_3 \odot (1 - a_3)\right)\)&lt;/span&gt; . &lt;span class="math"&gt;\(\odot\)&lt;/span&gt;  maens element-wise product, &lt;span class="math"&gt;\(\cdot\)&lt;/span&gt; means dot product.&lt;/p&gt;
&lt;p&gt;The partival derivative of Loss function to parameter &lt;span class="math"&gt;\(\Theta_1\)&lt;/span&gt; using chain rule is:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_1}} &amp;amp; = \frac{\partial{C}}{\partial{a_3}} \frac{\partial{a_3}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{a_2}} \frac{\partial{a_2}}{\partial{z_1}} \frac{\partial{z_1}}{\partial{\Theta_1}}  \\
    &amp;amp; \propto (a_3 - y) \left(a_3 (1 - a_3)\right) \Theta_2 (a_2 (1 - a_2)) a_1 \\
    &amp;amp; \propto \left[(a_3 - y) \odot \left(a_3 (1 - a_3)\right) \cdot \Theta_2 \odot (a_2 \odot (1 - a_2))\right]^{T} a_1 \\
    &amp;amp;  = d_2^{T} \cdot a_1
\end{aligned}&lt;/div&gt;
&lt;p&gt;
where &lt;span class="math"&gt;\(d_2 \triangleq d_3 \cdot \Theta_2 \odot \left(a_2 \odot (1 - a_2) \right)\)&lt;/span&gt; .&lt;/p&gt;
&lt;p&gt;To find the value of &lt;span class="math"&gt;\(\Theta_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Theta_2\)&lt;/span&gt;, we need to iteratively adjust their value until their gradient is close to 0. That is, to iterate:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \Theta_2^{i + 1} &amp;amp;= \Theta_2^{i} + \lambda  \frac{\partial {C}}{\partial {\Theta_2^{i}}}  \\
    \Theta_1^{i + 1} &amp;amp;= \Theta_1^{i} + \lambda  \frac{\partial {C}}{\partial {\Theta_1^{i}}} 
\end{aligned}&lt;/div&gt;
&lt;h3&gt;4. Code&lt;/h3&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
np.random.seed(10)
x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
y = np.array([0, 1, 1, 0]).reshape(4, -1)

a1 = x
theta1 = np.random.random(12).reshape(4, 3) * 2 - 1
theta2 = np.random.random(4).reshape(1, 4) * 2 - 1

learning_rate = 1

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

for i in range(100000):
    # forward propagation: including the first layer and the second layer
    z1 = a1.dot(theta1.T)
    a2 = sigmoid(z1) 
    z2 = a2.dot(theta2.T)
    a3 = sigmoid(z2) 

    # backpropagation: chain rules of the partial derivatives
    a3_error = y - a3
    theta2_grad = np.dot(np.transpose(a3_error * (a3 * (1 - a3))), a2) 
    theta1_grad = np.dot(np.transpose(np.dot(a3_error * (a3 * (1 - a3)),  theta2) * (a2 * (1 - a2))), a1)

    # update the parameters until the gradient converges to 0
    theta2 += learning_rate * theta2_grad
    theta1 += learning_rate * theta1_grad
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;theta2
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ -3.16174322,  10.29352444,  11.40818558,  -4.45945196]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;theta1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 3.35916278, -3.32786847,  1.87094082],
       [ 5.84130712, -5.98365309, -3.07635286],
       [-6.45870855,  6.34717945, -3.30437575],
       [-3.91182875,  3.95761648,  2.17740367]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;a3
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[ 0.00276657],
       [ 0.99710763],
       [ 0.99718615],
       [ 0.00243296]])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# The prediction is
a3 &amp;gt; 0.5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[False],
       [ True],
       [ True],
       [False]], dtype=bool)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The above is a simple example to introduce the insides of a neural network: how to calculate the forward propagation from input data to the prediction output and the cost function, how to calcualte the back propagatin of the partial derivatives with chain rules, and how to update the parameters until the gradients converging to zero, although in fact neural network is not necessary for this simple example since there are only 4 data points but we have introduced 16 parameters(parameters in &lt;span class="math"&gt;\(\Theta_1\)&lt;/span&gt; and &lt;span class="math"&gt;\(\Theta_2\)&lt;/span&gt;).&lt;/p&gt;
&lt;h2&gt;Example 2&lt;/h2&gt;
&lt;p&gt;The following example is a more real data question: I collect 60 verifacation code graphs. Each graph has 4 characters. The user needs to recognize these 4 characters correctly to get correct input verification.&lt;/p&gt;
&lt;p&gt;Each graph has 4 characters. An example of the picture is like&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170812_nn_from_scratch_01.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;The corresponding characters are: 'k' 'n' 'n' 'p'. &lt;/p&gt;
&lt;h3&gt;2.1. Model&lt;/h3&gt;
&lt;p&gt;We will build a three layers neural network. &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    a_1 &amp;amp;= X \\
    z_1 &amp;amp;= a_1 \cdot \Theta_1^{T}  \\
    a_2 &amp;amp;= \mbox{sigmoid}(z_1) = \frac{1}{1 + e^{- z_1}}  \qquad \mbox{the first activation layer} \\
    z_2 &amp;amp;= a_2 \cdot \Theta_2^{T} \\
    a_3 &amp;amp;= \mbox{sigmoid}(z_2) = \frac{1}{1 + e^{- z_2}}  \qquad \mbox{the second activation layer} \\
    z_3 &amp;amp;= a_3 \cdot \Theta_3^{T}  \\
    a_4 &amp;amp;= \mbox{sigmoid}(z_3) = \frac{1}{1 + e^{- z_3}}  \qquad \mbox{the third activation layer} 
\end{aligned}&lt;/div&gt;
&lt;h3&gt;2.2. Derivatives&lt;/h3&gt;
&lt;p&gt;Suppose the Lost function is
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    C = -\frac{1}{n}\sum_{x}\left[y \log(a_3) + (1 - y)\log(1 - a_3) \right] 
\end{aligned}&lt;/div&gt;
&lt;p&gt;First, let's denote
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    d_4 &amp;amp;= a_4 - y   ,\; \; \mbox{ or call it } a_4\_delta \\
    d_3 &amp;amp;= d_4 \cdot \Theta_3 \odot (a_3 \odot (1 - a_3))  ,\; \; \mbox{ or call it } a_3\_delta \\
    d_2 &amp;amp;= d_3 \cdot \Theta_2 \odot (a_2 \odot (1 - a_2)) 
\end{aligned}&lt;/div&gt;
&lt;p&gt;The partival derivative of Loss function to parameter &lt;span class="math"&gt;\(\Theta_3\)&lt;/span&gt; using chain rule is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_3}} &amp;amp; = \frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{\Theta_3}} \\
    &amp;amp; = - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) a_3 = ( a_4 - y) a_3  \; \; (\mbox{if all are scalers}) \\
    &amp;amp; =  \left[(a_4 - y)\right]^{T} \cdot a_3 \\
    &amp;amp;  \triangleq   d_4^{T} \cdot a_3  
\end{aligned}&lt;/div&gt;
&lt;p&gt;The partival derivative of Loss function to parameter &lt;span class="math"&gt;\(\Theta_2\)&lt;/span&gt; using chain rule is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_2}} &amp;amp; =\frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{a_3}} \frac{\partial a_3}{\partial z_3} \frac{\partial z_3}{\partial \Theta_2} \\
    &amp;amp; \propto - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) \Theta_3 (a_3(1-a_3)) a_2  \; \; (\mbox{if all are scalers})  \\
    &amp;amp; =  \left[d_4 \cdot \Theta_3 \odot \left(a_3 \odot (1 - a_3)\right)\right]^{T} \cdot a_2 \\
    &amp;amp;  \triangleq   d_3^{T} \cdot a_2  
\end{aligned}&lt;/div&gt;
&lt;p&gt;The partival derivative of Loss function to parameter &lt;span class="math"&gt;\(\Theta_1\)&lt;/span&gt; using chain rule is:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_1}} &amp;amp; = \frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{a_3}} \frac{\partial a_3}{\partial z_3} \frac{\partial z_3}{\partial a_2} \frac{\partial{a_2}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{\Theta_1}}  \\
    &amp;amp; \propto - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) \Theta_3 (a_3(1-a_3)) \Theta_2 (a_2(1-a_2)) \cdot a_1 \; \; (\mbox{if all are scalers}) \\
    &amp;amp; = \left[(a_3 - y) \odot \left(a_3 (1 - a_3)\right) \cdot \Theta_2 \odot (a_2 \odot (1 - a_2))\right]^{T} a_1 \\
    &amp;amp;  = d_2^{T} \cdot a_1
\end{aligned}&lt;/div&gt;
&lt;h3&gt;2.3. Details&lt;/h3&gt;
&lt;p&gt;The overall steps will include these:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;from pic to data&lt;/li&gt;
&lt;li&gt;set up activation function / sigmoid&lt;/li&gt;
&lt;li&gt;calc gradient descent function&lt;/li&gt;
&lt;li&gt;loss function&lt;/li&gt;
&lt;li&gt;predict&lt;/li&gt;
&lt;li&gt;put all together&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;Shape of the data&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;Theta1.T: 261 x 200
Theta2.T: 201 x 200
Theta3.T: 201 x 36

X: 240 * 260
a1: 240 * 261
z2: 240 * 200
a2: 240 * 200 -&amp;gt; 240 * 201
z3: 240 * 201
a3: 240 * 200 -&amp;gt; 240 * 201
z4: 240 * 36
a4: 240 * 36

y_matirx: 240 * 36
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import scipy.io
import os
import scipy.signal
import pandas as pd
import numpy as np
from PIL import Image
import scipy.optimize as opt
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;word = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\
        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

mypath = r'data/yanzhengma/'
train_data = pd.read_pickle(mypath + 'train_data.pkl')

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;
def sigmoid(x, derivative = False):
    if derivative == True:
        return x * (1 - x)
    else:
        return 1 / (1 + np.exp(-x))

def rand_init_weights(layer_in, layer_out):
    epsilon_init = 0.1
    return np.random.rand(layer_out, 1 + layer_in) * 2 * epsilon_init - epsilon_init

def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param):
    '''
    theta1: 200x261
    theta2: 200x201
    theta3: 36x201
    a1: 240x261
    z2: 240x200
    a2: 240x200 -&amp;gt; 240x201
    z3: 240x200
    a3: 240x2-- -&amp;gt; 240x201
    z4: 240x36
    a4: 240x36
    d4: 240x36
    d3: 240x201 -&amp;gt; 240x200
    d2: 240x201 -&amp;gt; 240x200
    theta1_grad: 200x261
    theta2_grad: 200x201
    theta3_grad: 36x201
    final output: 200x261 + 200x201 + 36x201 = 99636
    '''
    theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[(hidden_layer_size * (input_layer_size + 1)) : \
                       (hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    n = np.shape(X)[0]
    a1 = np.append(np.ones((n, 1)), X, axis = 1)
    z2 = a1.dot(theta1.T)
    a2 = sigmoid(z2)
    a2 = np.append(np.ones((n, 1)), a2, axis = 1)
    z3 = a2.dot(theta2.T)
    a3 = sigmoid(z3)
    a3 = np.append(np.ones((n, 1)), a3, axis = 1)
    z4 = a3.dot(theta3.T)
    a4 = sigmoid(z4)

    y_matrix = np.zeros((n, num_labels))
    for i in range(num_labels):
        y_matrix[:, i] = (y == (i + 1)).reshape(-1)

    d4 = a4 - y_matrix
    d3 = d4.dot(theta3) * a3 * (1 - a3)   
    d3 = d3[:, 1:]
    d2 = d3.dot(theta2) * a2 * (1 - a2)  
    d2 = d2[:, 1:]
    theta1_grad = 1 / n * (d2.T.dot(a1))
    theta2_grad = 1 / n * (d3.T.dot(a2)) 
    theta3_grad = 1 / n * (d4.T.dot(a3))  
    return np.append(np.append(theta1_grad.flatten(), theta2_grad.flatten(), axis = 0), theta3_grad.flatten(), axis = 0)

def nn_cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param):
    theta1 = nn_params[0:(hidden_layer_size * (input_layer_size + 1))].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[(hidden_layer_size * (input_layer_size + 1)) : \
                       (hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    n = np.shape(X)[0]
    a1 = np.append(np.ones((n, 1)), X, axis = 1)
    z2 = a1.dot(theta1.T)
    a2 = sigmoid(z2)
    a2 = np.append(np.ones((n, 1)), a2, axis = 1)
    z3 = a2.dot(theta2.T)
    a3 = sigmoid(z3)
    a3 = np.append(np.ones((n, 1)), a3, axis = 1)
    z4 = a3.dot(theta3.T)
    a4 = sigmoid(z4)

    y_matrix = np.zeros((n, num_labels))
    for i in range(num_labels):
        y_matrix[:, i] = (y == (i + 1)).reshape(-1)

    return -(1 / n) * (np.sum(np.sum(y_matrix * np.log(a4))) + np.sum(np.sum((1 - y_matrix) * np.log(1 - a4)))) + \
        lambda_param / ((2 * n) * np.sum(np.sum(theta1[:, 1:]**2)) + np.sum(np.sum(theta2[:, 1:]**2)) + \
                        np.sum(np.sum(theta3[:, 1:]**2)))

def predict(theta1, theta2, theta3, X):
    n = np.shape(X)[0]
    p = np.zeros((n, 1))
    h1 = sigmoid(np.append(np.ones((n, 1)), X, 1).dot(theta1.T))
    h2 = sigmoid(np.append(np.ones((n, 1)), h1, 1).dot(theta2.T))
    h3 = sigmoid(np.append(np.ones((n, 1)), h2, 1).dot(theta3.T))
    p = np.ndarray.argmax(h3, axis = 1).reshape([p.shape[0], 1])
    return p + 1

def test(train_data = train_data):
    data = train_data
    X = train_data['X']
    y = train_data['y']
    input_layer_size = 260
    hidden_layer_size = 200
    num_labels = 36

    initial_theta1 = rand_init_weights(input_layer_size, hidden_layer_size)
    initial_theta2 = rand_init_weights(hidden_layer_size, hidden_layer_size)
    initial_theta3 = rand_init_weights(hidden_layer_size, num_labels)

    initial_nn_params = np.append(np.append(initial_theta1.flatten(), initial_theta2.flatten()), initial_theta3.flatten())
    lambda_param = 0.1
    result = opt.minimize(fun = nn_cost, x0 = initial_nn_params, \
                          args = (input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param), \
                          method = &amp;quot;CG&amp;quot;, jac = nn_gradient, options = {&amp;quot;maxiter&amp;quot;: 100})
    nn_params = result.x
    theta1 = nn_params[0:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[hidden_layer_size*(input_layer_size + 1) : \
                       (hidden_layer_size*(input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    pred = predict(theta1, theta2, theta3, X)
    print(&amp;quot;Training Accuracy: &amp;quot;, np.mean(pred == y) * 100, &amp;quot;%&amp;quot;)
    return

test()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('Training Accuracy: ', 100.0000000, '%')
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="deep learning"></category></entry><entry><title>tensorflow简介--07</title><link href="/pages/2017/07/29/tensorflowjian-jie-07/" rel="alternate"></link><published>2017-07-29T01:03:00-05:00</published><updated>2017-07-29T10:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-07-29:/pages/2017/07/29/tensorflowjian-jie-07/</id><summary type="html">&lt;p&gt;这是以前看卷积神经网络做的笔记.我很喜欢另外一种学习方法:不是从理解什么是CNN开始，也不是去关注后面的数学公式，也不是怎么来用随机梯度下降的办法来最小化cross-entropy函数求解参数。而是直接用tensorflow来build一个简单的卷积神经网络，观察它是怎么工作的，最后的结果是什麽。理解了这些以后，然后在回过头来理解那些理论的部分和数学的部分。这篇blog的绝大部分内容都是来自于stanford的卷积神经网络课程的讲义。用中文整理出来，因为还是看中文快一些。&lt;/p&gt;</summary><content type="html">&lt;p&gt;这是以前看卷积神经网络做的笔记。我把大概的意思用中文写了，毕竟看中文还是快得多。但是关键的专有名字还是保留着英文，这样方便查看英文资料。比如查找帮助的时候经常会出现filter,FC layer这种说法，所以我觉得还是保留英文更好一些，而不是翻译成过滤器和全连接层。&lt;/p&gt;
&lt;p&gt;卷积神经网络和其他神经网络有相似的地方：它们都是由神经元构成，每个神经元接受输入的数据，每个神经元有各个输入的权重(weights)和偏差(biases)参数，最后有对应的输出，然后通过最小化损失函数来求解相应的参数。&lt;/p&gt;
&lt;p&gt;和通常的神经网络不同的是：卷积神经网络通常用来进行图像识别。它通过一些特殊的设计使得参数的数量大为减少。&lt;/p&gt;
&lt;h2&gt;结构总览&lt;/h2&gt;
&lt;p&gt;通常的神经网络中，神经元对每一个输入的数据点都会有一个参数与之对应。在处理图像的时候，这就会导致引入巨多的参数。比如一个1000万象素的图片，一个神经元就会有3000万个(3个chanel)参数。如果有很多神经元的时候参数就会非常多。&lt;/p&gt;
&lt;p&gt;卷积神经网络的神经元都有三个维度：width, height, depth(这儿的depth指的是激活层的第三维，不是神经网络的layer的个数).比如CIFAR-10里一个32x32x3(width x height x depth)的图片，每个layer的神经元只连接这个图片的一小块。最后的输出是一个[1x1x10]的向量，用来表示图片是10个类别里的哪一个类别。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170729_cnn_introduction_01.jpeg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;上图左边是通常的3层神经网络。第一层是输入层。第二层的4个神经元中每个神经元都与输入层的每个输入相连，同样第三层的每个神经元与第二层的每个输出相连。最后的输出层与前一层的所有神经元相连。上图右边是卷积神经网络，红色的是一个图片的输入层，所以其维度是[width x height x 3]，中间layer的每个神经元只跟输入层的一小部分相连，比如通常用一个3x3x3的filter与图片中每个3x3x3的小区域相连。卷积神经网络由各个Layer构成。&lt;/p&gt;
&lt;h2&gt;构造CNN的Layers&lt;/h2&gt;
&lt;p&gt;卷积神经网络主要有三种不同的Layer:convolutional layer, pooling layer, fully-connected layer.我们通常把它们叠加起来构成卷积神经网络。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;例子&lt;/em&gt;： 一个简单的CIFAR-10的分类卷积神经网络包含下面几个layer[INPUT-CONV-RELU-POOL-FC].&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;INPUT[32x32x3]是输入图像的像素矩阵。这些图片的width是32， height是32，有3个chanel&lt;/li&gt;
&lt;li&gt;CONV层用来计算卷积神经网络：input的每一个小块会跟filter矩阵做一个卷积，如果有12个filter矩阵，那么输出就是[32x32x12]。&lt;/li&gt;
&lt;li&gt;RELU层会对CONV的[32x32x12]矩阵里的每一个元素做一个变换&lt;span class="math"&gt;\(max(0, x)\)&lt;/span&gt;.RELU不改变矩阵的shape&lt;/li&gt;
&lt;li&gt;POOL层用来沿着spatial维度(width, height)做一个downsampling，输出矩阵的维度为[16x16x3]&lt;/li&gt;
&lt;li&gt;FC用来计算分类得分。对每一个输入的图片，它的输出是一个[1x1x10]的向量矩阵。这10个数字用来表示图片在每个分类的得分。和普通的NN一样，FC的神经元跟前面输入的每一个数字都有联系。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面的例子中，CONV和FC包含参数，RELU和POOL不含参数。损失函数是FC里面每个分类的得分和图片真实分类的一个函数。通过最小化损失函数可以得到CONV和FC里面的参数的值。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;tiny VGG&lt;/em&gt; : 下面是一个&lt;a href="http://www.robots.ox.ac.uk/~vgg/research/very_deep/"&gt;VGG&lt;/a&gt;结构的卷积神经网络示意图&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170729_cnn_introduction_02.jpeg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;上图最左边是一张输入的图片，中间是CNN的各个layer，最右边FC layer输出图片在每个分类里面的概率。&lt;/p&gt;
&lt;p&gt;下面分别介绍CNN里面几个重要的Layers:CONV, POOLING, FC.&lt;/p&gt;
&lt;h3&gt;Convolutional　Layer&lt;/h3&gt;
&lt;p&gt;卷积层是卷积神经网络的核心部分。简单地说就是对输入的数据通过Filter做卷积然后输出。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总览&lt;/strong&gt; 
卷积层的参数是是一系列的可优化的参数。每个filter对应输入的空间(spatially，width, height)的某一部分,但是在depth上shape是一样的。比如第一个卷积层上的filter的size为[5x5x3] (也就是在width和height上的5个像素，3是因为输入的图片depth为3，有3个chanel)。在forward pass过程中，每一个这样的filter沿着图片的width和height滑动(或者叫卷积)，这样我们就会得到一个二维的矩阵。假设这个卷积层有12个filter，每个都这样做卷积，我们把这12个卷积的矩阵stack在一起就得到新的输出(输出维度为 [width X height X 12] ,输出的width和height的大小跟stride和padding有关，后面会具体介绍)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Local Connectivity&lt;/strong&gt;
当处理高维输入的时候，一般不能把前一个输入的所有神经元都跟卷积层的神经元连接起来，那样会引入天量的参数。我们只是把输入的每一小块区域跟当前卷积层的神经元联系起来。这一小块区域的空间维度大小叫做接受域(receptive field,也就是filter的size)，这是一个可以变化的超参数。在深度(depth)上他们是一样大小。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;例1&lt;/em&gt; 每张CIFAR-10的图片的初始输入维度为[32x32x3]，如果接受域(receptive field, 或者叫filter size)的大小为5x5，那么卷积层的每个神经元都与输入图片的每个[5x5x3]的区域通过filter做卷积(所以filter的大小也为5x5x3，共计75个参数，加上一个bias，总共76个参数)。注意depth方向的大小为3，因为depth方向的大小总是跟输入的depth一样大。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;例2&lt;/em&gt; 假设输入的size为[16x16x20].假如receptive field的size是3x3，那么卷积层上的每个神经元都与输入矩阵的每个[3x3x20]的区域通过filter做卷积。所以每个filter的参数数量是181个[3x3x20=180加1个bias，共计181个]。注意的是这时候沿着width，height的空间大小是3x3，但是沿着depth是20.因为输入的矩阵的depth是20.&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170729_cnn_introduction_04.png" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;上图最左边粉色图片大小为[32x32x3]，receptive field是其中每个粉色的小块，大小为[5x5x3],所以filter的大小为5x5x3。中间的蓝色的图片说明有5个filter，所以conv layer的输出的depth为5。如果假设width和hegith不变，那么中间蓝色的输出区域大小为[32x32x5]。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spatial Arrangement&lt;/strong&gt; 前面介绍了卷积神经网络的神经元怎么与输入的数据相连。下面我们介绍在卷积神经网络里面的输出数据是什么样子（这儿的所有的数据都是指numpy array）。输出数据的size决定于下面三个超参数：&lt;strong&gt;depth, stride, zero-padding&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;depth&lt;/strong&gt;是输出结果的一个超参数，简单来说它指的是filter的数量。有时候也叫&lt;em&gt;fibre&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;stride&lt;/strong&gt;指的是每次滑动filter时移动的步长。stride为1表示每次沿着width或者height方向移动一个点，stride为2表示每次移动两个点的步长；如果步长为2，那么新产生的output就会比输入的数据的在width和height上面小(spatial size会变小)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;zero-padding&lt;/strong&gt; 0填充：有时候需要在输入的矩阵外面填充上0，通常这样做的目的是为了控制输出的空间大小(spatial size)，比如说为了使得输入和输出在width和height上大小一样。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;输出的空间大小可以有下面几个值计算出来：输入的大小(&lt;span class="math"&gt;\(W\)&lt;/span&gt;),接受域(filter)的大小(&lt;span class="math"&gt;\(F\)&lt;/span&gt;),步长(&lt;span class="math"&gt;\(S\)&lt;/span&gt;),以及外围补充的0的个数(&lt;span class="math"&gt;\(P\)&lt;/span&gt;)。这样得到的输出的神经元的个数为&lt;span class="math"&gt;\((W - F + 2P)/S + 1\)&lt;/span&gt;。比如对一个7x7（depth为1，略去）的输入，filter的大小是3x3，步长为1，0个0填充，那么输出的大小为5x5。如果步长为2，那么输出的大小为3x3。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;使用0填充&lt;/em&gt; 通常当S=1的时候，需要设置0填充的数量&lt;span class="math"&gt;\(P= (F-1)/2\)&lt;/span&gt;，这样的话输入和输出会有同样的空间尺寸。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;步长的限制&lt;/em&gt; 设置步长S的值的时候必须要注意使得公式&lt;span class="math"&gt;\((W - F + 2P)/S + 1\)&lt;/span&gt;有效。如果不能整除的话那么步长的设置就是无效的：比如W=10，P=0，F=3，假设设置S=2，那么&lt;span class="math"&gt;\((W - F + 2P)/S + 1 = (10 - 3 + 0) / 2 + 1 = 4.5\)&lt;/span&gt;。这个步长就无效。实际计算的时候要么会报错，要么就是需要采取0填充来使之有效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;参数共享&lt;/strong&gt; 在用filter滑过输入数据的时候，通常不会对输入数据的每一个小块都选择一个不同的filter，如果这样做的话会导致filter过多，因而模型里参数的个数会急剧上升。通常所有输入数据小块会滑过同一个filter，得到一个输出矩阵。然后所有的输入数据小块再滑过一个新的filter，再得到一个新的输出矩阵。最后把这些输出矩阵stack在一起得到最终的输出矩阵。有几个矩阵stack在一起，最后的输出矩阵的depth就是几。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Numpy Examples&lt;/strong&gt; 下面是一个具体的例子，使用numpy的代码来帮助解释清楚。假如输入矩阵是一个numpy数组&lt;code&gt;X&lt;/code&gt;，那么&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在width，height为&lt;code&gt;(x, y)&lt;/code&gt;位置处depth上的值可以表示为&lt;code&gt;X[x, y, :]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;在depth为&lt;code&gt;d&lt;/code&gt;的切片(所有width,height上的值)是一个矩阵，表示为&lt;code&gt;X[:, :, d]&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Conv Layer&lt;/em&gt; 假设&lt;code&gt;X&lt;/code&gt;的维度为&lt;code&gt;X.shape: (11, 11, 4)&lt;/code&gt;，假设没有0填充(P=0)，filter的大小为F=5,步长为S=2,那么输出矩阵的spatial尺寸为(11-5)/2+1 = 4，也就是输出矩阵的width和height大小为4.输出矩阵&lt;code&gt;V&lt;/code&gt;看起来如下&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;V[0, 0, 0] = np.sum(X[:5, :5, :] * W0) + b0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[1, 0, 0] = np.sum(X[2:7, :5, :] * W0) + b0&lt;/code&gt;，注意步长为2&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[2, 0, 0] = np.sum(X[4:9, :5, :] * W0) + b0&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[3, 0, 0] = np.sum(X[6:11, :5, :] * W0) + b0&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在numpy里面，上面的&lt;code&gt;*&lt;/code&gt;表示矩阵对应的元素相乘，不是矩阵相乘。&lt;code&gt;W0&lt;/code&gt;是权重矩阵filter，其size为&lt;code&gt;W0.shape: (5, 5, 4)&lt;/code&gt;.&lt;code&gt;b0&lt;/code&gt;是bias。注意这里的参数共享：在输出的depth的index=0(&lt;code&gt;V[:, : 0]&lt;/code&gt;)的维度上，&lt;code&gt;W0&lt;/code&gt;和&lt;code&gt;b0&lt;/code&gt;都相同。下面计算数据矩阵depth=1上面的值&lt;code&gt;V[:, :, 1]&lt;/code&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;V[0, 0, 1] = np.sum(X[:5, :5, :] * W1) + b1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[1, 0, 1] = np.sum(X[2:7, :5, :] * W1) + b1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[2, 0, 1] = np.sum(X[4:9, :5, :] * W1) + b1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[3, 0, 1] = np.sum(X[6:11, :5, :] * W1) + b1&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[0, 1, 1] = np.sum(X[:5, 2:7, :] * W1 + b1)&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V[2, 3, 1] = np.sum(X[4:9, 6:11, :] * W1) + b1&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们可以看到，在上面输出矩阵depth的index=1(&lt;code&gt;V[:, :, 1]&lt;/code&gt;)上面，我们所有的参数都是&lt;code&gt;W1&lt;/code&gt;和&lt;code&gt;b1&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;总结&lt;/strong&gt; 在卷积层上:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;接受一个输入矩阵，size(shape)为&lt;span class="math"&gt;\(W_1 \times H_1 \times D_1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;需要有4个超参数&lt;ul&gt;
&lt;li&gt;Filter的数量&lt;span class="math"&gt;\(K\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Filter的size &lt;span class="math"&gt;\(F\)&lt;/span&gt;, (输入矩阵每次选取的小块的size)&lt;/li&gt;
&lt;li&gt;步长&lt;span class="math"&gt;\(S\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;0填充的大小&lt;span class="math"&gt;\(P\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;输出一个size为&lt;span class="math"&gt;\(W_2 \times H_2 \times D_2\)&lt;/span&gt;的矩阵，其中&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(W_2 = (W_1 - F + 2P) / S + 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(H_2 = (H_1 - F + 2P) / S +１\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(D_2 = K\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;因为参数共享，每个filter有&lt;span class="math"&gt;\(F \cdot F \cdot D1\)&lt;/span&gt;个权重参数，&lt;span class="math"&gt;\(K\)&lt;/span&gt;个filter总共有&lt;span class="math"&gt;\((F \cdot F \cdot D_1) \cdot K\)&lt;/span&gt;个权重参数与&lt;span class="math"&gt;\(K\)&lt;/span&gt;个bias参数&lt;/li&gt;
&lt;li&gt;输出矩阵上第&lt;span class="math"&gt;\(d\)&lt;/span&gt;个depth的slice上(即&lt;code&gt;V[:, :, d]&lt;/code&gt;，size为&lt;span class="math"&gt;\(W_2 \times H_2\)&lt;/span&gt;)的矩阵是由输入矩阵跟第&lt;span class="math"&gt;\(d\)&lt;/span&gt;个filter作用产生的结果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常超参数的的值会被设置为&lt;span class="math"&gt;\(F = 3, S = 1, P = 1\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;卷积示例&lt;/strong&gt; 下面是一个卷积层的示例。输入数据的3个通道被表示为3个矩阵(下面的 x[:, :, 0], x[:, :, 1], x[:, :, 2])，输入的size大小为&lt;span class="math"&gt;\(W_1 = 5, H_1 = 5, D_1 = 3\)&lt;/span&gt;,卷积层的超参数是&lt;span class="math"&gt;\(K = 2, F = 3, S = 2, P = 1\)&lt;/span&gt;。即有两个width x height大小为[3x3],depth = 3的filter，分别为&lt;span class="math"&gt;\(W_0, W_1\)&lt;/span&gt;。输出的空间size为(5 - 3 + 2)/2 + 1 = 3.输出的depth = 2，因为有两个filter。所以最后的输出是两个3x3的矩阵。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170729_cnn_introduction_05.gif" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;采用矩阵相乘来计算&lt;/strong&gt; 上面卷积层上的计算实际上是各个小矩阵元素相乘的和然后再加上bias，我们可以把它统一成一个大的矩阵相乘的表达式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;我们把输入矩阵的没一小块都拉长为一个列向量，记为&lt;em&gt;im2col&lt;/em&gt;.比如输入为[227x227x3],filter为[11x11x3]，步长为4。我们从输入矩阵上拿出[11x11x3]的小块然后拉长为一个向量，向量的长度为11x11x3=363.每隔步长为4再拿出一小块[11x11x3]的矩阵做这样的拉长，在width和height方向上分别有(227 - 11)/4 + 1 = 55个小矩阵(所以总共有55x55=3025)个[11x11x3]的小矩阵。把3025个小矩阵都拉长为长度363的向量，放在一起就变成了[363x3025]的矩阵。把输入矩阵通过&lt;em&gt;im2col&lt;/em&gt;拉长以后变成的矩阵记为&lt;code&gt;X_col&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;把卷积层上的权重也拉成向量。比如有96个size为[11x11x3]的filter，这样得到一个矩阵记为&lt;code&gt;W_row&lt;/code&gt;，大小为[96x363]&lt;/li&gt;
&lt;li&gt;这样卷积就可以表示为矩阵点乘&lt;code&gt;np.dot(W_row, X_col)&lt;/code&gt;，结果为[96x3025]的矩阵&lt;/li&gt;
&lt;li&gt;最后需要把[96x3025]的矩阵reshape成[55x55x96]&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;上面的办法简单易懂，但是会比较费内存，因为在把输入矩阵拉成向量的时候有很多元素是重复出现的。这就增加了内存的消耗。但是好处是我们有非常快速的算法来计算，而且同样的&lt;code&gt;im2col&lt;/code&gt;想法可以用在下面的pooling里面。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Backpropagation&lt;/strong&gt;(反向传播) 在最小化损失函数的时候，通常使用迭代的办法，沿着梯度方向慢慢收敛。要计算梯度就必须计算偏导数，因为损失函数通常是函数的函数，或者更多的函数复合起来的函数，所以需要使用链式规则(chain rules)层层展开。反向传播其实就是使用链式规则求偏导数。 &lt;/p&gt;
&lt;h3&gt;Pooling Layer (sub-sampling)&lt;/h3&gt;
&lt;p&gt;通常在一个卷积层做好后，把输出用来作为新的输入，再做下一个卷积层之前，都会加上pooling层(抽样)，这样可以减少参数的个数(输入矩阵变小了)，也就相应的减少了overfitting.在实际当中的意义通常是：各个卷积层的目的是从输入图片中寻找到某些feature，通过抽样会使卷积层更多的去关注输入的图片的features，而不是去关注那些太detail的东西。因为太关注detail通常会导致模型过分拟合。 pooling层通常做的就是max-pooling：把输入矩阵选一小块(通常见到的是[2x2]的小矩阵)，然后只保留这一小块(4个数字里面)的最大的那个值。一般情况下，pooling层有如下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输入的矩阵的大小为&lt;span class="math"&gt;\(W_1 \times H_1 \times D1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;需要两个超参数&lt;ul&gt;
&lt;li&gt;他们的空间大小F&lt;/li&gt;
&lt;li&gt;步长S&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;长生一个大小为&lt;span class="math"&gt;\(W_2 \times H_2 \times D2\)&lt;/span&gt;的矩阵，其中&lt;ul&gt;
&lt;li&gt;&lt;span class="math"&gt;\(W_2 = (W_1 - F)/S +１\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(H_2 = (H_1 - F)/S + 1\)&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span class="math"&gt;\(D2 = D1\)&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;这一步没有新的参数，因为只是抽样&lt;/li&gt;
&lt;li&gt;通常也不会使用0填充&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通常见到比较多的是&lt;span class="math"&gt;\(F = 2, S=2\)&lt;/span&gt;，或者&lt;span class="math"&gt;\(F = 3, S = 2\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;下图是一个max pooling的示意图，从每个[2x2]的小区域里面选取最大的一个数，所以输出的矩阵大小是输入的矩阵大小的一半。
&lt;img alt="alt text" src="/figures/20170729_cnn_introduction_06.jpeg" title="Logo Title Text 1"&gt;&lt;/p&gt;
&lt;h3&gt;Normalization Layer / 正则层&lt;/h3&gt;
&lt;p&gt;常见的有batch normalization和layer normalization。尽管stanford那个教程说normalization现在渐渐不是那么流行了，但是我自己的经验是做了normalization还是有效果的。通过normalization以后，所有的数据都一致化了，这样的话做梯度下降的时候更stable。否则的话那些绝对数值过大的数据会影响梯度下降的效果。&lt;/p&gt;
&lt;h3&gt;Fully-connected Layer / 全连接层&lt;/h3&gt;
&lt;p&gt;跟通常神经网络一样，全连接层里面的神经元跟输入数据的每个点都相连。所以全连接层可以看作是一个矩阵相乘，然后加上bias。&lt;/p&gt;
&lt;h2&gt;卷积神经网络结构&lt;/h2&gt;
&lt;p&gt;卷积神经网络通常由这三层构成：conv层，pooling层和FC层。&lt;/p&gt;
&lt;h3&gt;各层结构&lt;/h3&gt;
&lt;p&gt;常见的卷积神经网络是由CONV-RELU层累加在一起，然后跟着pooling层，然后重复这个结构，最后是FC层。就是&lt;/p&gt;
&lt;p&gt;&lt;code&gt;INPUT -&amp;gt; [[CONV -&amp;gt; relu]*N -&amp;gt; POOL?]*M -&amp;gt; [FC -&amp;gt; RELU]*k -&amp;gt; FC&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;这里的&lt;code&gt;*&lt;/code&gt;表示重复，&lt;code&gt;POOL?&lt;/code&gt;表示可选，通常&lt;code&gt;0 &amp;lt;= N &amp;lt;= 3, M &amp;gt;= 0, 0 &amp;lt;= K &amp;lt; 3&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;下面是几个常见个卷积网络&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;INPUT -&amp;gt; FC&lt;/code&gt;, 这是一个线性分类器. 其中 &lt;code&gt;N = M = K = 0&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INPUT -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; FC&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; POOL]*2 -&amp;gt; FC -&amp;gt; RELU -&amp;gt; FC&lt;/code&gt;. 每个CONV层后面都有一个POOL层.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;INPUT -&amp;gt; [CONV -&amp;gt; RELU -&amp;gt; CONV -&amp;gt; RELU -&amp;gt; POOL]*3 -&amp;gt; [FC -&amp;gt; RELU]*2 -&amp;gt; FC&lt;/code&gt; 在POOL层之前有两个CONV层迭加。这样可以建立一些深度比较深的网络，因为不同的卷积层迭加可以用来描述比较复杂的features&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最后用&lt;code&gt;VGG16&lt;/code&gt;作为一个例子来帮助解释上面卷积神经网络的讲解：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;VGG16&lt;/code&gt;曾经取得过很好的成绩，一直到现在还是被很多人使用。它的方法很简单，每次的卷积filter的width和height都是[3x3]。简单高效。&lt;/p&gt;
&lt;p&gt;下面是各层的说明&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;lambda_1:读进去的输入data的大小，224x224，3个通道&lt;/li&gt;
&lt;li&gt;zeropadding2d_1：在输入数据外面进行0填充，P=1，P是超参数，这一步没有模型需要fit的参数。&lt;/li&gt;
&lt;li&gt;convolution2d_1：第一个卷积层，filter的size是3x3x3,bias有一个参数，所以每个filter有28个参数，总计64个filter，所以参数个数为28x64=1792&lt;/li&gt;
&lt;li&gt;zeropadding2d_2：0填充，P=1，没有需要fit的参数&lt;/li&gt;
&lt;li&gt;convolution2d_2：第二个卷积，filter的size是3x3x64,bias有一个参数，所以每个filter参数为577，共计64个filter，所以参数个数为577x64=36928&lt;/li&gt;
&lt;li&gt;maxpooling2d_1: 对每个2x2的矩阵抽样，选取最大的值。这一步没有参数，但是因为抽样，所以输出矩阵的size变为[64x112x112]&lt;/li&gt;
&lt;li&gt;......如此反复&lt;/li&gt;
&lt;li&gt;flatten_1: 把前一层maxpooling2d_5进行拉直，所以元素的个数为512x7x7=25088&lt;/li&gt;
&lt;li&gt;dense_1: 是一个FC层，所以与输入的每个元素都相连，输出为一个4096的向量，所以总共的参数是(25088+1)x4096=102764544个&lt;/li&gt;
&lt;li&gt;dropout_1：直接drop掉一部分数据，这样做的原因也是防止overfitting&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;具体的每层结构详细情况如下:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import keras
import vgg16
from vgg16 import Vgg16
vgg = Vgg16()
vgg.model.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;____________________________________________________________________________________________________
Layer (type)                     Output Shape          Param #     Connected to                     
====================================================================================================
lambda_1 (Lambda)                (None, 3, 224, 224)   0           lambda_input_1[0][0]             
____________________________________________________________________________________________________
zeropadding2d_1 (ZeroPadding2D)  (None, 3, 226, 226)   0           lambda_1[0][0]                   
____________________________________________________________________________________________________
convolution2d_1 (Convolution2D)  (None, 64, 224, 224)  1792        zeropadding2d_1[0][0]            
____________________________________________________________________________________________________
zeropadding2d_2 (ZeroPadding2D)  (None, 64, 226, 226)  0           convolution2d_1[0][0]            
____________________________________________________________________________________________________
convolution2d_2 (Convolution2D)  (None, 64, 224, 224)  36928       zeropadding2d_2[0][0]            
____________________________________________________________________________________________________
maxpooling2d_1 (MaxPooling2D)    (None, 64, 112, 112)  0           convolution2d_2[0][0]            
____________________________________________________________________________________________________
zeropadding2d_3 (ZeroPadding2D)  (None, 64, 114, 114)  0           maxpooling2d_1[0][0]             
____________________________________________________________________________________________________
convolution2d_3 (Convolution2D)  (None, 128, 112, 112) 73856       zeropadding2d_3[0][0]            
____________________________________________________________________________________________________
zeropadding2d_4 (ZeroPadding2D)  (None, 128, 114, 114) 0           convolution2d_3[0][0]            
____________________________________________________________________________________________________
convolution2d_4 (Convolution2D)  (None, 128, 112, 112) 147584      zeropadding2d_4[0][0]            
____________________________________________________________________________________________________
maxpooling2d_2 (MaxPooling2D)    (None, 128, 56, 56)   0           convolution2d_4[0][0]            
____________________________________________________________________________________________________
zeropadding2d_5 (ZeroPadding2D)  (None, 128, 58, 58)   0           maxpooling2d_2[0][0]             
____________________________________________________________________________________________________
convolution2d_5 (Convolution2D)  (None, 256, 56, 56)   295168      zeropadding2d_5[0][0]            
____________________________________________________________________________________________________
zeropadding2d_6 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_5[0][0]            
____________________________________________________________________________________________________
convolution2d_6 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_6[0][0]            
____________________________________________________________________________________________________
zeropadding2d_7 (ZeroPadding2D)  (None, 256, 58, 58)   0           convolution2d_6[0][0]            
____________________________________________________________________________________________________
convolution2d_7 (Convolution2D)  (None, 256, 56, 56)   590080      zeropadding2d_7[0][0]            
____________________________________________________________________________________________________
maxpooling2d_3 (MaxPooling2D)    (None, 256, 28, 28)   0           convolution2d_7[0][0]            
____________________________________________________________________________________________________
zeropadding2d_8 (ZeroPadding2D)  (None, 256, 30, 30)   0           maxpooling2d_3[0][0]             
____________________________________________________________________________________________________
convolution2d_8 (Convolution2D)  (None, 512, 28, 28)   1180160     zeropadding2d_8[0][0]            
____________________________________________________________________________________________________
zeropadding2d_9 (ZeroPadding2D)  (None, 512, 30, 30)   0           convolution2d_8[0][0]            
____________________________________________________________________________________________________
convolution2d_9 (Convolution2D)  (None, 512, 28, 28)   2359808     zeropadding2d_9[0][0]            
____________________________________________________________________________________________________
zeropadding2d_10 (ZeroPadding2D) (None, 512, 30, 30)   0           convolution2d_9[0][0]            
____________________________________________________________________________________________________
convolution2d_10 (Convolution2D) (None, 512, 28, 28)   2359808     zeropadding2d_10[0][0]           
____________________________________________________________________________________________________
maxpooling2d_4 (MaxPooling2D)    (None, 512, 14, 14)   0           convolution2d_10[0][0]           
____________________________________________________________________________________________________
zeropadding2d_11 (ZeroPadding2D) (None, 512, 16, 16)   0           maxpooling2d_4[0][0]             
____________________________________________________________________________________________________
convolution2d_11 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_11[0][0]           
____________________________________________________________________________________________________
zeropadding2d_12 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_11[0][0]           
____________________________________________________________________________________________________
convolution2d_12 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_12[0][0]           
____________________________________________________________________________________________________
zeropadding2d_13 (ZeroPadding2D) (None, 512, 16, 16)   0           convolution2d_12[0][0]           
____________________________________________________________________________________________________
convolution2d_13 (Convolution2D) (None, 512, 14, 14)   2359808     zeropadding2d_13[0][0]           
____________________________________________________________________________________________________
maxpooling2d_5 (MaxPooling2D)    (None, 512, 7, 7)     0           convolution2d_13[0][0]           
____________________________________________________________________________________________________
flatten_1 (Flatten)              (None, 25088)         0           maxpooling2d_5[0][0]             
____________________________________________________________________________________________________
dense_1 (Dense)                  (None, 4096)          102764544   flatten_1[0][0]                  
____________________________________________________________________________________________________
dropout_1 (Dropout)              (None, 4096)          0           dense_1[0][0]                    
____________________________________________________________________________________________________
dense_2 (Dense)                  (None, 4096)          16781312    dropout_1[0][0]                  
____________________________________________________________________________________________________
dropout_2 (Dropout)              (None, 4096)          0           dense_2[0][0]                    
____________________________________________________________________________________________________
dense_3 (Dense)                  (None, 1000)          4097000     dropout_2[0][0]                  
====================================================================================================
Total params: 138357544
____________________________________________________________________________________________________
&lt;/code&gt;&lt;/pre&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>LSTM Introduction</title><link href="/pages/2017/07/22/lstm-introduction/" rel="alternate"></link><published>2017-07-22T00:00:00-05:00</published><updated>2017-07-22T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-07-22:/pages/2017/07/22/lstm-introduction/</id><summary type="html"></summary><content type="html">&lt;h2&gt;Intro&lt;/h2&gt;
&lt;p&gt;通常介绍deep learning的时候总是从卷积神经网络开始，因为最近deep learning的流行就是从卷积神经网络在图像识别上的广泛应用开始的。但是在银行这种地方，没有什么明显的图像处理的需求。更多的是要处理各种序列性的数据，比如时间序列等等。所以这里我会从循环神经网络(&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;RNN&lt;/a&gt;)开始，并且结合一个例子用来展示怎么进行时间序列的建模。&lt;/p&gt;
&lt;p&gt;RNN的一个显著特征是：新的输入(下图t时刻的输入，绿色的箭头)会和前一个state(下图从t-1到t的蓝色箭头)一起变成新的输入。并以此循环。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_00_1.png" title="RNN"&gt;&lt;/p&gt;
&lt;p&gt;把上图画成下图的等号右边部分，那么整个的循环过程通常也就画为下图的等号左边部分。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_00_2.png" title="RNN"&gt;&lt;/p&gt;
&lt;p&gt;RNN好的地方是它可以记住前面一段时间的输入信息，不好的地方是它只能记住某段时间的输入信息，虽然理论上RNN可以处理长时间的信息，但是实际上它却不能很好的学习这些信息。对于长时间的输入信息它无能为力。&lt;/p&gt;
&lt;p&gt;对RNN的一个改进就是长短记忆网络(LSTM). LSTM通过信息阀门来控制信息是否能够通过。这个信息阀门是由一个sigmoid神经网络层和多元运算符组成。sigmoid网络层的输出是一个0，1之间的值，用来控制每个组成部分有多少信息可以通过。0表示没有信息能够通过，1表示所有的信息都可以通过。通过控制不同的信息加入到cell state中，LSTM实现了控制哪些信息可以对最后的stage施加影响。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_00_3.png" title="RNN"&gt;&lt;/p&gt;
&lt;p&gt;上面图中的各个标志的表示意义分别为&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_00_4.png" title="RNN"&gt;&lt;/p&gt;
&lt;p&gt;LSTM在自然语言处理(NLP),自动翻译上面取得了很大的成功。如果我们换一个视角，把RNN/LSTM中的输入当成是按照时间顺序的排列的数据，它们就可以用来对(高维)时间序列建模和预测。我们可以控制在RNN/LSTM的每个输入的Layer上输出或者不输出，这样我们就很容易的进行一步预测比如&lt;span class="math"&gt;\(X_{1}, X_{2}, \cdots, X_{n-1}\)&lt;/span&gt;来预测&lt;span class="math"&gt;\(\hat{X_{n}}\)&lt;/span&gt;，或者进行多步预测，比如用&lt;span class="math"&gt;\(X_{1}\)&lt;/span&gt;来预测&lt;span class="math"&gt;\(\hat{X_{2}}\)&lt;/span&gt;，&lt;span class="math"&gt;\(\hat{X_{2}}\)&lt;/span&gt;来预测&lt;span class="math"&gt;\(\hat{X_{3}}\)&lt;/span&gt;等等。&lt;/p&gt;
&lt;p&gt;有一些实际的例子表明，在&lt;span class="math"&gt;\(X_{1}, X_{2}, \cdots, X_{n-1}\)&lt;/span&gt;来预测&lt;span class="math"&gt;\(\hat{X_{t}}\)&lt;/span&gt;中，即使不需要&lt;span class="math"&gt;\(\hat{X_{2}}, \cdots, \hat{X_{n-1}}\)&lt;/span&gt;，但是在模型中强制这些输出会提高模型的效果。&lt;/p&gt;
&lt;p&gt;也有一些实际的例子显示，经过改进CNN也可以用来处理这种序列性质的数据，而且可以得到比RNN更好的效果。&lt;/p&gt;
&lt;h2&gt;1. DATA &amp;amp; Model&lt;/h2&gt;
&lt;h3&gt;1.1. 数据来源&lt;/h3&gt;
&lt;p&gt;使用的数据是&lt;a href="https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"&gt;S&amp;amp;P500&lt;/a&gt;从02Jan1998到09Aug2013攻击3926天每天的股票价格数据，包括OHLCV,共计500只股票。&lt;/p&gt;
&lt;h3&gt;1.2. 数据转变&lt;/h3&gt;
&lt;p&gt;对每一只股票，计算出它的每天几种不同价格的比值，比如Open对Close(c_2_o),High(c_2_h)以及Low(c_2_l)的比值，以及High对Close(h_2_c)和Low(h_2_l)的比值。因为各个股票的Volume会很不相同，为了让它们在同一个Scale上面，对Volume进行标准化，即减去均值，除以标准差。这些每只股票每天自身的性质会作为我们的自变量input。我们想做的是用这些来预测股票第二天是上涨还是下跌。也就是t+1时间的Close价格除以t时间的价格(c1_c0)。如果第二天上涨了，那么比值大于1，比值的对数函数就大于0.反之这个Close的价格的比值的对数函数就小于0.所有的数据可以从&lt;a href="https://quantquote.com/historical-stock-data"&gt;quantquote&lt;/a&gt;下载。&lt;/p&gt;
&lt;p&gt;因为不是所有的500只股票都有完整的3926天的数据，只有388只股票有完整的数据。取这388只股票上面所说的6个比值作为最后的X，取这388只股票的回报率的均值作为Y.Y可以理解为在388只股票上每只股票投资一块钱的平均回报率。从3926行数据中去前3900行作为最后的分析数据。所以最后X的维度为(3900, 2328),Y的维度为(3900, 1). &lt;/p&gt;
&lt;h3&gt;1.3. 数据说明&lt;/h3&gt;
&lt;h4&gt;1.3.1. 理想状况&lt;/h4&gt;
&lt;p&gt;假如我们已经知道Y的值是大于0还是小于0，我们可以做出最优的决定：对任一天，当第二天的价格高于当天的价格的时候，我们买进一块钱然后第二天卖掉，这样我们就可以赚取那一部分差价。如果第二天的价格低于今天的价格，我们就应该在今天卖一块钱的股票。这样的决策成为理想决策。当然实际上因为没法知道第二天的价格，所以这样的决策不存在。但是因为我们是历史数据，我们还是可以计算出理想决策最终可以获得多少利润。&lt;/p&gt;
&lt;h4&gt;1.3.2. 最差状况&lt;/h4&gt;
&lt;p&gt;最差的状况就是投一块钱，然后什么都不做最后所得到的状态。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_01.png" title="RNN"&gt;&lt;/p&gt;
&lt;h4&gt;1.4. 学习模型&lt;/h4&gt;
&lt;p&gt;因为回报率是一个连续值，但是我们实际关系的是它大于0还是小于0.所以我们把它分为11份，记为0到10.这样的话Y就是一个Categorical Variable，问题也就转化为一个有序分类(ordered categorical data)的问题。如果预测的&lt;span class="math"&gt;\(\hat{Y} &amp;lt; 5\)&lt;/span&gt;，我们应该卖出。如果&lt;span class="math"&gt;\(\hat{Y} == 5\)&lt;/span&gt;,则不作任何交易。如果&lt;span class="math"&gt;\(\hat{Y} &amp;gt; 5\)&lt;/span&gt;，则买入。&lt;/p&gt;
&lt;h4&gt;1.4.1. Multinomial Logistic Regression&lt;/h4&gt;
&lt;p&gt;类似于传统的logistic regression，只是这个时候Y不只是0和1，而是0到10.因为只是logistic regression，没有超参数，也没有变量选择，所以效果不是很好。
&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_02.png" title="RNN"&gt;&lt;/p&gt;
&lt;h4&gt;1.4.2. LSTM Model&lt;/h4&gt;
&lt;p&gt;LSTM会根据输入的数据来有选择的记住前面的信息。这里我们使用tensorflow的&lt;code&gt;MultiRNNCell([LSTMCell() for _ in range(2)])&lt;/code&gt;建立了一个两层的LSTM模型。可以看出，LSTM明天的提高了预测的效果。
&lt;img alt="alt text" src="/figures/20170722_lstm_stock_return_03.png" title="RNN"&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="quant"></category></entry><entry><title>Download Bitcoin Price</title><link href="/pages/2017/06/10/download-bitcoin-price/" rel="alternate"></link><published>2017-06-10T00:00:00-05:00</published><updated>2017-06-10T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-06-10:/pages/2017/06/10/download-bitcoin-price/</id><summary type="html">&lt;p&gt;Recently I read an article from &lt;a href="https://zhuanlan.zhihu.com/p/20090944"&gt;zhihu&lt;/a&gt;. It talks about bitcoin arbitrage traiding with risk hedging. It reminds me the 'one upon a time' in full swing.&lt;/p&gt;</summary><content type="html">&lt;p&gt;前段时间各种E-corn(比特币，以太币)的价格飞涨，买买提很多搞深度学习的朋友都把显卡拿来挖矿了。最近读到&lt;a href="https://zhuanlan.zhihu.com/p/20090944"&gt;知乎上的一篇文章&lt;/a&gt;。讲的是比特币交易。不同的地区，不同的交易平台会出现不同的价格。所以就有了无风险套利的空间。在价格低的平台买进，在价格高的平台卖出。因为要实时监控价格，决定交易策略，所以不可能一直靠人工点击查看价格进行交易。必须依靠机器来自动抓取比特币的价格，自动进行买卖。&lt;/p&gt;
&lt;p&gt;看完以后唏嘘不已，想起了n年前我还在加州读书的时候，在办公室里面搞了一台机器，日夜挖矿，乐此不疲。那个时候每天在办公室就是看看paper，上上买买提。慢慢学会了倒卖各种电脑相机打印机，wii的游戏机平衡板，到后来果果出来iphone ipad更是迎来了高潮。那是一个疯狂的时代，也是一个热火朝天的时代。每天忙乎乎的，然后到半岛搞了螃蟹龙虾九层塔鸭舌。&lt;/p&gt;
&lt;p&gt;黄鹤一去不复返。&lt;/p&gt;
&lt;p&gt;anyway，下面是一个最简单的例子，怎么通过网站的api接口来下载数据。当然要是没有api接口的话，那就只能自己crawl各种网页了。下一篇blog我会简单的介绍一下我怎么爬&lt;a href="https://www.wsj.com/"&gt;wsj&lt;/a&gt;的新闻标题的。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from bs4 import BeautifulSoup
import urllib
import urllib2
import json
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from datetime import datetime

sns.set_style(&amp;quot;darkgrid&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;打开网页，数据是json格式的。直接json load成dict，然后转为DataFrame以方便查看。但是如果保存到sql的话，不需要转为DataFrame。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;#r = urllib.urlopen(r'file:///H:/python/test/wsj_bmo.html').read()
response = urllib2.urlopen('https://data.btcchina.com/data/trades')
data = json.load(response)  
df = pd.DataFrame(data)
# response = urllib.urlopen(r'https://data.btcchina.com/data/trades').read()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;date_num = df.date.map(lambda x: datetime.fromtimestamp(int(x)))
date_str = df.date.map(lambda x: datetime.fromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M'))
idx = range(0, df.shape[0], 50)
plt.plot(idx, df.price[idx], color = &amp;quot;#33b5e5&amp;quot;, label = &amp;quot;&amp;quot;)
plt.xticks(idx, date_str[idx], rotation = 45)
plt.locator_params(tight=True, nbins=10)
plt.title(&amp;quot;Bitcoin Price(CNY) in Last 24 Hours&amp;quot;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170610_bitcoin_price_download.png"&gt;&lt;/p&gt;</content><category term="python"></category><category term="python"></category><category term="quant"></category></entry><entry><title>买入股票时，买入价至少设为多少才能保证有九成的把握在当天成交？</title><link href="/pages/2017/06/03/mai-ru-gu-piao-shi-mai-ru-jie-zhi-shao-she-wei-duo-shao-cai-neng-bao-zheng-you-jiu-cheng-de-ba-wo-zai-dang-tian-cheng-jiao/" rel="alternate"></link><published>2017-06-03T00:00:00-05:00</published><updated>2017-06-03T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-06-03:/pages/2017/06/03/mai-ru-gu-piao-shi-mai-ru-jie-zhi-shao-she-wei-duo-shao-cai-neng-bao-zheng-you-jiu-cheng-de-ba-wo-zai-dang-tian-cheng-jiao/</id><summary type="html">&lt;p&gt;这是我觉得很好的一篇知乎文章，原作者不仅简明扼要的把一个问题转化为数学问题，把书本上关于Ito积分应用到现实问题中，而且给出了一个Simulation解，做到了让人真正明白是怎么回事。&lt;/p&gt;</summary><content type="html">&lt;p&gt;这是我觉得很好的一篇知乎文章，原作者不仅简明扼要的把一个问题转化为数学问题，把书本上关于Ito积分应用到现实问题中，而且给出了一个Simulation解，做到了让人真正明白是怎么回事。&lt;/p&gt;
&lt;p&gt;原文参见&lt;a href="https://www.zhihu.com/question/21561785/answer/18723625"&gt;知乎&lt;/a&gt;。以下摘自原文。&lt;/p&gt;
&lt;p&gt;原始问题是:&lt;strong&gt;买入股票时，买入价至少设为多少才能保证有九成的把握在当天成交？假定股价服从布朗运动。下单后不可修改。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;下面完全是引用的最高票答案：&lt;/p&gt;
&lt;p&gt;既然题目里面说了Brownian Motion，我只说说从金融数学的角度怎么处理这个问题，至于行为经济学、市场文化学这些太抽象，相关答案很多，我就不多嘴了。现在欢迎大家来到Q-measure（Risk-neutral Probability measure）。&lt;/p&gt;
&lt;p&gt;这是程序做市（Electronic Market Making）里面常见的一个问题，不过这些假设太粗糙了，必须要加的一个条件是：购买股票，&lt;strong&gt;下限价订单（Limit order）&lt;/strong&gt;——否则如同大部分答案所说——假设是市场订单（Market Order），那成交概率是100%。&lt;/p&gt;
&lt;p&gt;如同游戏先要弄懂如何打金刷怪一样，数学没有前提条件做啥都是白瞎。前提假设先说清楚——所有市场无套利的假设全部装备，假设初始股价&lt;span class="math"&gt;\(S_0\)&lt;/span&gt;，股票年化波动性&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt;，时间&lt;span class="math"&gt;\(\frac{1}{252}\)&lt;/span&gt;，由于一天之内，就不考虑分红了&lt;span class="math"&gt;\(d = 0\)&lt;/span&gt;，利率也没有影响&lt;span class="math"&gt;\(r = 0\)&lt;/span&gt;。我们要求&lt;span class="math"&gt;\(S_{bid}\)&lt;/span&gt;，满足:&lt;span class="math"&gt;\(P\left(\inf\left\{ S_t \lvert ~ t \in (0, T) \right\} \leq S_{bid}\right) = 90\%\)&lt;/span&gt;，即股价在一天之内的下界小于等于的概率等于90%。因为此概率对于&lt;span class="math"&gt;\(S_{bid}\)&lt;/span&gt;单调递增，故由唯一解。原问题可以两种做法，解析法和模拟法。&lt;/p&gt;
&lt;h3&gt;1. 解析法：&lt;/h3&gt;
&lt;p&gt;设符合条件的买入价为&lt;span class="math"&gt;\(S_{bid} \leq S_0\)&lt;/span&gt;，根据假设，股价服从Geometric Brownian Motion：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
\frac{dS_t}{S_t} = rdt + \sigma dW = \sigma dW ~~~~ (w.r.t. r = 0)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;
因此，根据Ito's Calculus，我们有：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
 \ln S_t = \ln S_0 + \left(- \frac{1}{2} \sigma^2 \right) t + \sigma W_t
\end{align}
$$&lt;/div&gt;
&lt;p&gt;变形，易得：
&lt;/p&gt;
&lt;div class="math"&gt;$$W_t = \frac{ln \frac{S_t}{S_0} + \frac{1}{2}\sigma^2 t }{\sigma}$$&lt;/div&gt;
&lt;p&gt;定义：
&lt;/p&gt;
&lt;div class="math"&gt;$$m(t) = \inf\left\{ \ln S_v \lvert ~ 0 \leq v \leq t \right\}$$&lt;/div&gt;
&lt;p&gt;那么&lt;span class="math"&gt;\(m(t)\)&lt;/span&gt;就是0到t时刻中对数股价的下界。由于对数函数的单调递增性质，我们的目标转化为：
&lt;/p&gt;
&lt;div class="math"&gt;$$P\{m(T) \leq S_{bid}\} = 90\%$$&lt;/div&gt;
&lt;p&gt;由于我们有：
&lt;/p&gt;
&lt;div class="math"&gt;$$\ln S_t \sim N\left(\ln S_0 - \frac{1}{2} \sigma^2 T ,~~ \sigma^2 t\right)$$&lt;/div&gt;
&lt;p&gt;我不加证明的给出&lt;span class="math"&gt;\(m(t)\)&lt;/span&gt;的累积分布函数（CDF）如下，证明需要利用布朗运动的反射性质（Reflection Principle），有兴趣的可以要我补充：
&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{align}
&amp;amp; \mu = -\frac{1}{2}\sigma^2, \alpha = \ln S_0 \\
&amp;amp; F_{m(t)}(x) = N\left(\frac{x - \alpha - \mu t}{\sigma \sqrt{t}}\right) + \exp \left(2 \frac{\mu(x - \alpha)}{\sigma^2}\right) N\left(\frac{x - \alpha + \mu t}{\sigma \sqrt{t}}\right)
\end{align}
$$&lt;/div&gt;
&lt;p&gt;那么利用这一函数的反函数，分分钟搞定这个问题的解析解：
&lt;/p&gt;
&lt;div class="math"&gt;$$S_{bid} = \exp \left(F_{m(T)}^{-1}( 90\% )\right)$$&lt;/div&gt;
&lt;p&gt;如有谬误，欢迎指出。这里不妨看看解析解对应的函数曲线（假设&lt;span class="math"&gt;\(S_0 = 100, \sigma = 0.3\)&lt;/span&gt;)：&lt;/p&gt;
&lt;p&gt;由于我们恒有&lt;span class="math"&gt;\(m(t)\leq S_0\)&lt;/span&gt;，故&lt;span class="math"&gt;\(F_{m(T)}(S_0)=100\%\)&lt;/span&gt;。而根据解析解，我们有：
&lt;span class="math"&gt;\(\exp\left(F^{-1}_m(T)(90\%)\right)=99.77\)&lt;/span&gt;。即在开盘价99.77%的水平下单，可以满足条件。&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(F_{m(T)}\)&lt;/span&gt;的分布函数不是很好反解。模拟了一下它的结果:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats
import scipy.stats as st
import seaborn as sns
%matplotlib inline
plt.rcParams['figure.figsize'] = (9, 6)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;mu = -0.5 * 0.3**2
alpha = np.log(100)
capt = 1/252.0

x = np.log(99.761)
p = st.norm(alpha + mu*capt, 0.3*np.sqrt(capt)).cdf(x) + np.exp(2 * mu * (x - alpha) / 0.3**2) * st.norm(alpha - mu*capt, 0.3*np.sqrt(capt)).cdf(x)

res = []
xrange = np.arange(94, 100, 0.001)
for i in xrange:
    x = np.log(i)
    v = st.norm(alpha + mu*capt, 0.3*np.sqrt(capt)).cdf(x) + np.exp(2 * mu * (x - alpha) / 0.3**2) * st.norm(alpha - mu*capt, 0.3*np.sqrt(capt)).cdf(x)
    res.append(v)


plt.plot(np.arange(94, 100, 0.001), res)
plt.axvline(x = 99.77, color = '#33b5e5', linestyle = '--', label = &amp;quot;90% percentile: 99.761&amp;quot;)
plt.axhline(y = 0.9, color = '#00C851', linestyle = '--', label = &amp;quot;90% Percent&amp;quot;)
plt.legend(loc = 3)
plt.title(&amp;quot;Distribution of $F_{m(T)}(x)$&amp;quot;)
plt.show()

res_s = pd.Series(res)

ss = pd.Series(xrange)
fx = ss[res_s[(res_s-0.9&amp;lt;0.0005) &amp;amp; (res_s-0.9&amp;gt;0)].index[0]]
print(&amp;quot;The 90th quantile is %.4f&amp;quot; %(fx) )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170603_stock_price_90_pct_limit_order_01.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;The 90th quantile is 99.7610
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;2. 模拟法：&lt;/h4&gt;
&lt;p&gt;简单来说就是做一次Monte Carlo的模拟，然后模拟N次股票的轨迹，取每个轨迹的下尾90%分位点.直接模拟&lt;span class="math"&gt;\(\ln S_t = \ln S_0 + \left(- \frac{1}{2} \sigma^2 \right) t + \sigma W_t\)&lt;/span&gt;,其中&lt;span class="math"&gt;\(W_t\)&lt;/span&gt;是布朗运动。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;S0 = 100
sigma = 0.3
mu = 0
deltaT = 1.0/252
N = 10000
M = 200
barrier = 0.45

S_q = []
for i in range(N):
    ts = np.arange(M)/float(M)
    St = S0 * np.exp((mu - 1/2.0*sigma**2)*ts*deltaT + sigma * np.sqrt(deltaT / M) * np.cumsum(np.random.normal(0, 1, M)))
    S_q.append(min(St))

dis = pd.Series(S_q, name = &amp;quot;S_q&amp;quot;)

# dis.plot(kind = &amp;quot;density&amp;quot;)
# plt.show()

# density = stats.kde.gaussian_kde(dis)
# xs = np.linspace(min(dis), max(dis), 200)

# plt.plot(xs, density(xs))
# plt.show()

sns.distplot(dis, bins = 500)
plt.axvline(x = np.percentile(dis, 90), color = '#00C851', label = &amp;quot;90% cutoff&amp;quot;)
plt.legend(loc = 2)
plt.title(&amp;quot;Density function of &amp;quot;)

np.percentile(dis, 90)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;99.841974221885323
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170603_stock_price_90_pct_limit_order_02.png"&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="quant"></category></entry><entry><title>SQLAlchemy</title><link href="/pages/2017/05/28/sqlalchemy/" rel="alternate"></link><published>2017-05-28T00:00:00-05:00</published><updated>2017-05-28T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-28:/pages/2017/05/28/sqlalchemy/</id><summary type="html">&lt;p&gt;SQLAlchemy introduction&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. SQLAlchemy简介&lt;/h2&gt;
&lt;p&gt;SQLAlchemy是一个与数据库进行交互的库。它可以让你用Python的类和语句创建数据模型来查询数据.它可以连接许多数据库，包括 Postgres，MySQL，SQLite和Oracle等等.&lt;/p&gt;
&lt;h3&gt;为什么用SQLAlchemy&lt;/h3&gt;
&lt;p&gt;简单地说，就是用统一的python的格式来写SQLcode.&lt;/p&gt;
&lt;p&gt;我们在使用不同的SQL的时候，语法会不尽相同。比如sqlite，mysql，mongodb等等，进行插入操作，新建变量操作等等，都不相同。这样导致每用一个SQL，都要去看一遍语法.使用SQLAlchemy相当于提供了一个统一的接口，这样我们只要写python的code，而不需要去关注具体的SQL的细节部分。&lt;/p&gt;
&lt;p&gt;用SQLAlchemy的主要原因是，把你从底层的数据库和SQL奇葩语法中解放出来。SQLAlchemy将常用语句和类型和SQL语句对应起来，让你可以更容易地理解数据库类型，而不需要担心太多细节。这样在处理像Oracle到PostgreSQL数据库这类的迁移工作，或从一个应用数据库到数据仓库时，事情就简单了。它还能确保数据在增加到数据库之前是经过安全的，适当转义处理的。这样可以避免SQL注入之类的事情发生。&lt;/p&gt;
&lt;p&gt;SQLAlchemy通过两个主要的模型来实现灵活的操作：SQL表达式语言（通常也叫Core）和ORM（Object-relational mapping，对象关系映射）。这两个模型可以根据你的需要独立使用，也可以合在一起使用。&lt;/p&gt;
&lt;h4&gt;SQLAlchemy Core和SQL表达式语言&lt;/h4&gt;
&lt;p&gt;SQL表达式语言是用Pythonic方式的来表达SQL语句和表达式，只是对传统的SQL语言的轻微抽象。它侧重于实用数据库的模式（schema，其实是具体到一个Tabel和View等），但是它实现了不同数据库之间标准化的接口。SQL表达式语言也是SQLAlchemy ORM的基础。&lt;/p&gt;
&lt;h5&gt;ORM&lt;/h5&gt;
&lt;p&gt;SQLAlchemy ORM与你在其他语言里遇到的ORM类似。它侧重于应用的Domain Model（一种将数据与其行为集成在一起的模式），借助工作单元的模式来维护对象状态。它还在SQL表达式语言之上增加了一层抽象，让用户可以更容易的操作数据库。你可以把ORM和SQL表达式语言结合起来构建强大的应用。ORM构建了一个声明式的系统，与许多其他ORM模型（如Ruby on Rails）使用的 active-record systems类似。&lt;/p&gt;
&lt;p&gt;虽然ORM非常有用，但是你要注意，类的很多用法与数据库的工作方式是不一样的。我们将在后面的章节介绍这些差异。&lt;/p&gt;
&lt;h4&gt;Core和ORM的选择&lt;/h4&gt;
&lt;p&gt;究竟是选择Core还是ORM作为应用的数据链接层呢？除了个人喜好，理由可以归结为一些影响因素。这两种模式的语法不太一样，但Core和ORM最大的差异是Core对数据模式和业务对象（business objects）的不同处理方式。&lt;/p&gt;
&lt;p&gt;SQLAlchemy Core是以模式为中心，和普通SQL一样有表，键和索引等。SQLAlchemy Core最擅长的时数据仓库，报表分析，以及其他使用数据查询和其他操作可以牢牢掌控的地方。它拥有强大的数据库连接池（ connection pool）和数据结果集（ResultSet）优化，非常适合处理大量数据，甚至多数据库也适用。&lt;/p&gt;
&lt;p&gt;但是，如果你更侧重于&lt;a href="https://en.wikipedia.org/wiki/Domain-driven_design"&gt;领域驱动设计&lt;/a&gt;(domain driven design)，那么ORM就可以将原数据和业务对象的底层的模式和结构大部分细节都封装起来。这样封装让数据库连接更简单，更像Python代码。大多数应用都更适合按照这种方法建模。ORM可以用一种非常高效的方法把领域驱动设计方法导入传统应用，或者改造原来带有原始SQL语句的应用。还有一个好处就是，通过对底层数据库的合理抽象，ORM让开发者把精力更多地集中在业务流程的实现上。&lt;/p&gt;
&lt;p&gt;不过，ORM是建立在SQLAlchemy Core基础之上的，你可以把处理MySQL的同样方式用于Oracle的数据仓库和Amazon Redshift数据库。当你需要业务对象和仓库数据时，ORM可以无缝的衔接每个环节。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如果你的应用框架已经使用了ORM，但是想要更强大的报表功能，使用Core&lt;/li&gt;
&lt;li&gt;如果你不想像普通SQL一样以模式为中心，用ORM&lt;/li&gt;
&lt;li&gt;如果你的数据不需要业务对象，用Core&lt;/li&gt;
&lt;li&gt;如果你把数据看成业务对象，用ORM&lt;/li&gt;
&lt;li&gt;如果要建立快速原型，用ORM&lt;/li&gt;
&lt;li&gt;如果你既要业务对象，又要其他数据无关的功能（报表，数据分析等等），两个都用。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知道了如何选择Core和ORM，我们介绍SQLAlchemy的安装与数据库连接方法。&lt;/p&gt;
&lt;h3&gt;SQLAlchemy安装与数据库连接&lt;/h3&gt;
&lt;p&gt;SQLAlchemy支持Python 2.6+，Python 3.3+和Pypy 2.1+，强烈推荐conda安装，pip也可以（Python 2.7.5+和Python 3.4+自带pip）。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-bash"&gt;pip install sqlalchemy
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;连接数据库&lt;/h4&gt;
&lt;p&gt;连接数据库需要SQLAlchemy引擎，SQLAlchemy引擎为数据库执行SQL语句创建了一个常用的接口。引擎通过封装一个数据库连接池和方言来实现不同数据库类型统一的接口。这样做使得Python代码不需要关心不同数据库DBAPI之间的差异。SQLAlchemy提供了一个带连接字符串（connection string）和一些参数的函数来创建引擎。连接字符串形式如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据库类型（SQLite，Postgres，MySQL等）&lt;/li&gt;
&lt;li&gt;默认数据库类型的方言（Psycopg2，PyMySQL等）&lt;/li&gt;
&lt;li&gt;验证信息（用户名和密码）&lt;/li&gt;
&lt;li&gt;数据库的位置（文件名或数据库服务器地址）&lt;/li&gt;
&lt;li&gt;数据库服务器端口（可选）&lt;/li&gt;
&lt;li&gt;数据库名称（可选）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SQLite数据库连接字符串就是一个文件或储存位置。例1-1中，第二行表示SQLAlchemy连接了当前文件夹中的一个SQLite数据库文件&lt;code&gt;cookies.db&lt;/code&gt;，第三行是连接内存数据库，第四、五行分别是Unix和Windows系统中的全路径文件。Windows系统路径名称分隔符（&lt;code&gt;\&lt;/code&gt;）在Python中是&lt;code&gt;'\\'&lt;/code&gt;或&lt;code&gt;r'\'&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import create_engine
engine = create_engine('sqlite:///cookies.db')
engine2 = create_engine('sqlite:///:memory:')
engine3 = create_engine('sqlite:////home/cookiemonster/cookies.db')
engine3 = create_engine('sqlite:///c:\\Users\\cookiemonster\\cookies.db')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;`create_engine`函数创建了一个引擎实例，但是，它并没有真正打开链接，直到一个动作要求引擎执行时才会执行，比如查询或新建数据。
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面再让我们创建一个PostgreSQL数据库&lt;code&gt;mydb&lt;/code&gt;。然后我们用函数构建一个引擎实例，如例1-2所示，你会发现我用了&lt;code&gt;postgresql+psycopg2&lt;/code&gt;作为连接字符串的引擎和方言部分。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import create_engine
engine = create_engine('postgresql+psycopg2://username:password@localhost:5432/mydb')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;同理，我们再看看MySQL引擎的创建，如例1-3所示，我们把参数&lt;code&gt;pool_recycle&lt;/code&gt;设置成&lt;code&gt;3600&lt;/code&gt;，表示每一小时自动连接一次。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://cookiemonster:chocolatechip'
                       '@mysql01.monster.internal/cookies', pool_recycle=3600)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;MySQL默认超过8小时空闲则断开连接。为了绕开这个问题，引擎设置pool_recycle=3600
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;create_engine&lt;/code&gt;函数的可选参数是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;echo&lt;/code&gt;：表示引擎行为的日志是否显示，像执行的SQL语句和其他参数等等。默认是&lt;code&gt;False&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ecoding&lt;/code&gt;：默认使用SQLAlchemy的字符串编码&lt;code&gt;utf-8&lt;/code&gt;，大多数DBAPI都用此编码。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;isolation_level&lt;/code&gt;：SQLAlchemy分离等级。PostgreSQL+psycopg2的分类等级有&lt;code&gt;READ COMMITTED&lt;/code&gt;，&lt;code&gt;READ UNCOMMITTED&lt;/code&gt;，&lt;code&gt;REPEATABLE READ&lt;/code&gt;，&lt;code&gt;SERIALIZABLE&lt;/code&gt;，&lt;code&gt;AUTOCOMMIT&lt;/code&gt;五种，默认是&lt;code&gt;READ COMMITTED&lt;/code&gt;。PyMySQL也是这五种，InnoDB存储引擎数据库默认是&lt;code&gt;REPEATABLE READ&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;用&lt;code&gt;isolation_level&lt;/code&gt;关键词会为具体的DBAPI设置隔离等级，可就像数据库对应连接字符串的键值对一样，比如PostgreSQL是用psycopg2。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;pool_recycle&lt;/code&gt;：这个参数是指数据库连接多少秒循环检查一次，对MySQL非常重要。默认值为-1，表示没有时间限制，一直连接。&lt;/p&gt;
&lt;p&gt;一旦引擎建立，我们就可以连接数据库了。通过&lt;code&gt;connect()&lt;/code&gt;函数就可以。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;connection = engine.connect()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在我们已经连接了数据库，可以用SQLAlchemy Core和ORM了。下面这部分，我们将探索SQLAlchemy Core的内容，学习如何定义和查询数据库。&lt;/p&gt;
&lt;h3&gt;SQLAlchemy Core&lt;/h3&gt;
&lt;p&gt;连接数据库之后，我们就可以使用SQLAlchemy Core来进行数据库操作了。SQLAlchemy Core是用Pythonic方式的SQL表达式语言来表示SQL命令和数据结构的。SQLAlchemy Core具有的这种特性让它不仅可以用于Django或SQLAlchemy ORM，也可以单独使用。&lt;/p&gt;
&lt;p&gt;首先我们需要定义数据表的数据类型，数据的关联性以及其他约束条件。&lt;/p&gt;
&lt;h4&gt;2.Schema and Types&lt;/h4&gt;
&lt;p&gt;为了搞定数据库，SQLAlchemy提供了以下三种方式来表示数据库中的表结构：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Core：自定义表Table对象&lt;/li&gt;
&lt;li&gt;ORM：用类class表示数据表&lt;/li&gt;
&lt;li&gt;SQLsoup和Automap：直接从数据库映射&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本章重点介绍第一种方式，即通过SQLAlchemy Core来实现，后面的章节会介绍其他两种方法。&lt;code&gt;Table&lt;/code&gt;对象包括了一系列具有特定类型的列和属性，这些通过一个常用的元数据容器来控制。首先我们介绍SQLAlchemy建数据表的数据类型。&lt;/p&gt;
&lt;h4&gt;Types&lt;/h4&gt;
&lt;p&gt;SQLAlchemy有四种数据类型：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通用类型（Generic）&lt;/li&gt;
&lt;li&gt;SQL标准类型（SQL standard）&lt;/li&gt;
&lt;li&gt;数据库特有类型（Vendor Specific）&lt;/li&gt;
&lt;li&gt;用户自定义类型（User Defined）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SQLAlchemy为不同的数据库定义了一些通用的数据类型。这些类型都在&lt;code&gt;sqlalchemy.types&lt;/code&gt;模块中，为了引用方便也支持放在&lt;code&gt;sqlalchemy&lt;/code&gt;里面。&lt;/p&gt;
&lt;p&gt;布尔型通用类型使用&lt;code&gt;BOOLEAN&lt;/code&gt; SQL类型，对应的Python类型就是&lt;code&gt;True&lt;/code&gt;和&lt;code&gt;False&lt;/code&gt;；但是，对那些不支持&lt;code&gt;BOOLEAN&lt;/code&gt; SQL类型的数据库通常要使用&lt;code&gt;SMALLINT&lt;/code&gt;来代替。由于SQLAlchemy把这些细节都隐藏了，因此你可以放心大胆的操作数据，不用担心后面数据库用的是什么细节，只要在Python代码里处理&lt;code&gt;True&lt;/code&gt;和&lt;code&gt;False&lt;/code&gt;就行。即使数据仓库和交换数据库不一样，通用类型也可以完成数据处理。通用类型对应Python和SQL的含义如下表所示：&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;SQLAlchemy&lt;/th&gt;
&lt;th style="text-align: center;"&gt;Python&lt;/th&gt;
&lt;th style="text-align: center;"&gt;SQL&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;BigInteger&lt;/td&gt;
&lt;td style="text-align: center;"&gt;int&lt;/td&gt;
&lt;td style="text-align: center;"&gt;BIGINT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Boolean&lt;/td&gt;
&lt;td style="text-align: center;"&gt;bool&lt;/td&gt;
&lt;td style="text-align: center;"&gt;BOOLEAN or SMALLINT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Date&lt;/td&gt;
&lt;td style="text-align: center;"&gt;datetime.date&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Date (SQLite: String)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;DateTime&lt;/td&gt;
&lt;td style="text-align: center;"&gt;datetime.datetime&lt;/td&gt;
&lt;td style="text-align: center;"&gt;DATETIME (SQLite: String)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Enum&lt;/td&gt;
&lt;td style="text-align: center;"&gt;str&lt;/td&gt;
&lt;td style="text-align: center;"&gt;ENUM or VARCHAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Float&lt;/td&gt;
&lt;td style="text-align: center;"&gt;float or Decimal&lt;/td&gt;
&lt;td style="text-align: center;"&gt;FLOAT or REAL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Integer&lt;/td&gt;
&lt;td style="text-align: center;"&gt;int&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Integer&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Interval&lt;/td&gt;
&lt;td style="text-align: center;"&gt;datetime.timedelta&lt;/td&gt;
&lt;td style="text-align: center;"&gt;INTERVAL or DATE from epoch&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;LargeBinary&lt;/td&gt;
&lt;td style="text-align: center;"&gt;byte&lt;/td&gt;
&lt;td style="text-align: center;"&gt;BLOB or BYTEA&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Numeric&lt;/td&gt;
&lt;td style="text-align: center;"&gt;decimal.Decimal&lt;/td&gt;
&lt;td style="text-align: center;"&gt;NUMERIC or DECIMAL&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Unicode&lt;/td&gt;
&lt;td style="text-align: center;"&gt;unicode&lt;/td&gt;
&lt;td style="text-align: center;"&gt;UNICODE or VARCHAR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Text&lt;/td&gt;
&lt;td style="text-align: center;"&gt;str&lt;/td&gt;
&lt;td style="text-align: center;"&gt;CLOB or TEXT&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;Time&lt;/td&gt;
&lt;td style="text-align: center;"&gt;datetime.time&lt;/td&gt;
&lt;td style="text-align: center;"&gt;DATETIME&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;掌握通用类型非常重要，会经常使用&lt;/p&gt;
&lt;p&gt;如果通用类型不能满足需求，也会用到SQL标准类型和数据库专有类型。&lt;code&gt;CHAR&lt;/code&gt;和&lt;code&gt;NVARCHAR&lt;/code&gt;类型就是最好的例证，源自SQL类型。如果数据库的模式是在使用SQLAlchemy之前建立的，我们就要注意原模式与SQLAlchemy的差异。SQL标准类型的特性在不同的数据库里面可能有很大变化，也是在sqlalchemy.types模块中，为了和通用类型分开，都用大写字母表示。&lt;/p&gt;
&lt;p&gt;数据库专有类型只存在于特定的数据库中。你可以通过SQLALchemy网站或某种数据库的方言（dialect）文档查询此类型，放在sqlalchemy.dialects模块和数据库方言的子模块中，这些类型同样是大写字母。例如，我们想使用PostgreSQL强大的JSON类型，我们可以这样：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy.dialects.postgresql import JSON
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们就可以用PostgreSQL特有的JSON函数为我们的应用定义JSON类型，比如array_to_json。&lt;/p&gt;
&lt;p&gt;你也可以根据自己的需要自定义类型。例如，当增加数据记录时，需要对即将存储到VARCHAR列的文本增加前缀，下一次从数据记录中获取这些记录的时候要重新去掉前缀。这样定义类型就可以给那些系统中已经存在的旧数据进行标识，表示这类数据在新应用中并没什么用或者不太重要。&lt;/p&gt;
&lt;p&gt;了解了四类数据类型之后，我们来看看如果用元数据组织数据库的结构。&lt;/p&gt;
&lt;h5&gt;Metadata&lt;/h5&gt;
&lt;p&gt;元数据用于把数据库结构集成在一起方便SQLAlchemy快速对接。可以把元数据看成是一种表目录，再加一些引擎和链接的信息。这些信息可以通过MetaData.tables查看。读操作是线程安全的，但是，表的建立不完全是线程安全的。元数据使用之前需要导入并初始化。下面让我们建立一个Metadata对象，为本章后面的例子建立容器来盛放数据库的信息目录。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import MetaData
metadata = MetaData()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有了盛放数据库结构的地方，我们就可以建立数据表了。&lt;/p&gt;
&lt;h4&gt;Tables&lt;/h4&gt;
&lt;p&gt;在SQLAlchemy Core里数据表对象初始化过程是通过Table通过表名称，元数据和数据列的名称，类型和属性共同构建，最终放入MetaData对象的。列对象表示数据表的每个字段，都是用包含名称、数据类型和一些SQL结构与约束特征的Column对象表示。我们将从这里开始建立一些数据表在SQLAlchemy Core部分使用。例2-1如下所示，建立一个网店的饼干库存表。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import Table, Column, Integer, Numeric, String, ForeignKey

cookies = Table('cookies', metadata,
                Column('cookie_id', Integer(), primary_key=True),
                Column('cookie_name', String(50), index=True),
                Column('cookie_recipe_url', String(255)),
                Column('quantity', Integer()),
                Column('unit_cost', Numeric(12, 2))

)

list(cookies.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;建立新表之前，我们需要理解表的基础单元——列。&lt;/p&gt;
&lt;h5&gt;Columns&lt;/h5&gt;
&lt;p&gt;列定义了数据表中的字段，它们通过我们对其关键词参数的设置来表达具体的含义。不同类型的主要参数不同。例如，字符串类型的基本参数是长度，而带小数的数值类型基本参数是精度和长度。其他类型大都没有基本参数。&lt;/p&gt;
&lt;p&gt;有时你也会看到字符串类型没有设置长度。并非所有的数据库都支持这种特性，包括MySQL也不支持。
列也有一些别的参数来帮助它们建立更丰富的特性。我们可以为列设置是否允许空值或必须具有唯一性（unique）。还可以定义初始值（default），通常这么做是为了日志记录或财务审计的需要，如下所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
from sqlalchemy import DateTime
users = Table('users', metadata,
                Column('user_id', Integer(), primary_key=True),
                Column('username', String(15), nullable=False, unique=True),
                Column('email_address', String(255), nullable=False),
                Column('phone', String(20), nullable=False),
                Column('password', String(25), nullable=False),
                Column('created_on', DateTime(), default=datetime.now),
                Column('updated_on', DateTime(), default=datetime.now, onupdate=datetime.now)
)

list(users.columns)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;你会发现这里使用datatime.now，而不是datatime.now()。如果我们直接用函数，那么时间可能就是数据表第一次建立的时间。不带()我们就可以在每条记录生成时产生一个新的时间。&lt;/p&gt;
&lt;p&gt;我们通过列关键词参数定义了数据表的结构和约束；但是，列对象有时可能需要和其他表进行关联。当你处理数据库时，你必须告诉SQLAlchemy关于这个数据库内部的模式，结构和约束。假如有一个数据库和SQLAlchemy使用的索引名称模式不同，那么你必须定义索引才能正常使用。下面两个小节将告诉你怎么做。&lt;/p&gt;
&lt;p&gt;Keys and Constraints和Index两节的程序其实都可以在Table构造函数里直接实现，也可以通过特定方法在表建立之后实现，再增加到表中。它们都会作为单独一行语句保存到metadata里面。&lt;/p&gt;
&lt;h4&gt;Keys and Constraints&lt;/h4&gt;
&lt;p&gt;键和约束是用来保证数据在存到数据库之前能够满足一些约束条件的方法。表示键和约束的对象在SQLAlchemy模块里面都有，常用的三个如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import PrimaryKeyConstraint, UniqueConstraint, CheckConstraint
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;主键是最常用的，表示数据表中一条记录的唯一标识，可以用来保证不同表里相关数据之间正确的关系。前面的例1和例2中，我们用&lt;code&gt;primary_key&lt;/code&gt;参数将一列设置为主键。你还可以用若干列构成的元组来定义一个复合主键。这个键在表中被看成是一个内容为元组的列，会按照它们原始顺序进行排列。主键还可以在表构建之后再定义，如下所示。你可以用逗号分隔，增加多列形成一个复合主键。如果我们想在上面例2中显示定义主键，可以这样做：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;PrimaryKeyConstraint('user_id', name='user_pk')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另一个常用的约束就是唯一性约束，确保一个字段里任意两个值不重复，如果在登录系统里面出现两个用户名是一样的，那就麻烦了。我们也可以用下面的方式定义例2中用户名列的唯一性约束：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;UniqueConstraint('username', name='uix_username')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;例2里没有用到检查约束类型。这种约束类型是用来保证一列数据与用户定义的条件一致。下面的例子，我们保证unit_cost列永远非负，因为成本不可能小于0。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;CheckConstraint('unit_coust &amp;gt;= 0.00', name='unit_coust_positive')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;除了键和约束，我们还想更高效的查询一些字段。这就要介绍索引（Indexes）。&lt;/p&gt;
&lt;h5&gt;Indexes&lt;/h5&gt;
&lt;p&gt;索引是用来加速查询的，在例1里面，我们把cookie_name加上了索引，因为我们知道我们经常需要查询它们。索引创建之后你就会获得一个ix_cookies_cookie_name索引。我们也可以显式定义一个索引。多列的索引可以通过分号分隔名称来建立。你还可以增加一个参数unique=True来保证索引具有唯一性。如果显式声明索引，它们就会在放在对应列后面。下面的做法与例1相同：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import Index

Index('ix_cookies_cookie_name', 'cookie_name')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们还可以创建了函数索引，不同的数据库可能用法上会有点儿变化。这样可以让你为一些经常查询不常用信息的需求创建检索。例如，如果你想从饼干的SKU号和名称的组合中找SKU0001 Chocolate Chip信息。我们就可以建立一个复合索引来查询：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;Index('ix_test', mytable.c.cookie_sku, mytable.c.cookie_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面到了关系型数据库最重要的部分了，就是表的关联关系与定义。&lt;/p&gt;
&lt;h5&gt;Relationships and ForeignKeyConstraints&lt;/h5&gt;
&lt;p&gt;现在我们有了一个约束和索引都正确的表，让我们看看表之间的关系如何建立。我们需要一种方法来跟踪订单，包括记录已销售的饼干和数量的信息。表关系图如下所示： &lt;/p&gt;
&lt;p&gt;例3实现了line_items表order_id列的关系，就是用ForeignKeyConstraint定义两个表的关系。在本例中，我们有很多line_items表示单个订单。但是，如果你深入到这些订单中，你会发现订单与cookies表的cookie_id外键有关联关系。这是因为line_items表其实与orders和cookies表的一些数据都有关联。关联表用来表示另外两个表之间的多对多关系。通常一个外键用来表示一对多关系，但是如果一个表有多个外键，那么这个表很可能是一个关联表。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import ForeignKey, Boolean
orders = Table('orders', metadata,
            Column('order_id', Integer(), primary_key=True),
            Column('user_id', ForeignKey('users.user_id')),
            Column('shipped', Boolean(), default=False)
)
line_items = Table('line_items', metadata,
                Column('line_items_id', Integer(), primary_key=True),
                Column('order_id', ForeignKey('orders.order_id')),
                Column('cookie_id', ForeignKey('cookies.cookie_id')),
                Column('quantity', Integer()),
                Column('extended_cost', Numeric(12, 2))
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;用字符串替换实际的列名可以在多个模块中把表的定义分离，这样就不用担心表加载的顺序了。这是因为，它只会在首次连接表名称和列名称的时候解析字符串执行表查询。如果我们直接用表引用，比如cookies.c.cookie_id，我们的外键在每次模块初始化的时候都要执行一次，如果表加载的顺序特别靠后就会出错。&lt;/p&gt;
&lt;p&gt;你可以显式的调研ForeignKeyConstraint定义，在SQLAlchemy里面可以对已经建好的表建立外键，就像其他的键，约束和索引的创建一样。需要先倒入ForeignKeyConstraint模块然后定义。下面的代码是创建一个ForeignKeyConstraint表示line_items表和orders之间order_id外键。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import ForeignKeyConstraint

ForeignKeyConstraint(['order_id'], ['orders.order_id'])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;前面做的每件事情都是用SQLAlchemy可以理解的方式定义的。如果你的数据库已经存在，而且schema已经建立，你就可以进行查询了。如果你还没建立，下面就介绍如何把数据库建成文件。&lt;/p&gt;
&lt;h5&gt;Persisting the Tables&lt;/h5&gt;
&lt;p&gt;表和模式定义与原数据相关。把模式保存到数据库很简单，用metadata的create_all()方法就可以了：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;metadata.create_all(engine)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;默认情况下，create_all()方法不会重新创建已经存在的数据库，所有运行多次也很安全。相比直接在应用代码里修改数据库，用Alembic那样的迁移工具处理数据库的更新或其他模式是更好的方法。我们将在后面的章节里介绍。现在数据库里面已经有表了，这一章完整的代码如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
from sqlalchemy import (MetaData, Table, Column, Integer, Numeric, String,
                        Boolean, DateTime, ForeignKey, create_engine)
metadata = MetaData()
cookies = Table('cookies', metadata,
                Column('cookie_id', Integer(), primary_key=True),
                Column('cookie_name', String(50), index=True),
                Column('cookie_recipe_url', String(255)),
                Column('cookie_sku', String(55)),
                Column('quantity', Integer()),
                Column('unit_cost', Numeric(12, 2))
                )
users = Table('users', metadata,
              Column('user_id', Integer(), primary_key=True),
              Column('customer_number', Integer(), autoincrement=True),
              Column('username', String(15), nullable=False, unique=True),
              Column('email_address', String(255), nullable=False),
              Column('phone', String(20), nullable=False),
              Column('password', String(25), nullable=False),
              Column('created_on', DateTime(), default=datetime.now),
              Column('updated_on', DateTime(),
                     default=datetime.now, onupdate=datetime.now)
              )
orders = Table('orders', metadata,
               Column('order_id', Integer(), primary_key=True),
               Column('user_id', ForeignKey('users.user_id')),
               Column('shipped', Boolean(), default=False)
               )
line_items = Table('line_items', metadata,
                   Column('line_items_id', Integer(), primary_key=True),
                   Column('order_id', ForeignKey('orders.order_id')),
                   Column('cookie_id', ForeignKey('cookies.cookie_id')),
                   Column('quantity', Integer()),
                   Column('extended_cost', Numeric(12, 2))
                   )
engine = create_engine('sqlite:///:memory:')
metadata.create_all(engine)
connection = engine.connect()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;本章我们介绍了如何在SQLAlchemy里把元数据当作目录（catalog）来存放表模式和其他数据。我们还定义了带有多列和多约束的表。然后，我们介绍约束的类型和这些约束对列进行的显式定义的方法。其次，我们介绍了默认值的设置和为了查账进行更新值的方法。最后，我们介绍了把模式保存到数据库进行重用的方法。下面我们介绍如何在模式中用SQL表达式语言进行数据的操作。&lt;/p&gt;
&lt;h3&gt;3.SQLAlchemy Core数据操作&lt;/h3&gt;
&lt;p&gt;现在数据库里面有了表，让我们来操作它们。首先我们将演示如何增删改查，然后介绍如果排序，组合以及如何使用关系。我们用SQLAlchemy Core提供的SEL（SQL表达式语言）演示。还是用上一章建立的数据库，首先我们看看如何新建数据。&lt;/p&gt;
&lt;h4&gt;Inserting Data&lt;/h4&gt;
&lt;p&gt;首先，我们在cookies表新建一行我最喜欢的饼干（巧克力味的）。用cookies表的insert()方法，然后在values()语句里面设置各个列的值就可以了。如下所示：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins = cookies.insert().values(
    cookie_name=&amp;quot;chocolate chip&amp;quot;,
    cookie_recipe_url=&amp;quot;http://some.aweso.me/cookie/recipe.html&amp;quot;,
    cookie_sku=&amp;quot;CC01&amp;quot;,
    quantity=&amp;quot;12&amp;quot;,
    unit_cost=&amp;quot;0.50&amp;quot;
)
print(str(ins))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;INSERT INTO cookies (cookie_name, cookie_recipe_url, cookie_sku, quantity, unit_cost) VALUES (:cookie_name, :cookie_recipe_url, :cookie_sku, :quantity, :unit_cost)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面显示了对应的SQL语句。每一列的值都用:column_name代替了，SQLAlchemy的str()函数就是这样显示的。数值都进行过清洗和转义处理，保证数据安全，避免SQL注入攻击。因为不同种类的数据库处理参数值的方言可能有点差别，所以通过编译版本的语句可以看到输入的内容。ins对象的compile()方法会返回一个SQLAlchemy对象，通过params属性就可以看到数值了。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins.compile().params
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;{'cookie_name': 'chocolate chip',
 'cookie_recipe_url': 'http://some.aweso.me/cookie/recipe.html',
 'cookie_sku': 'CC01',
 'quantity': '12',
 'unit_cost': '0.50'}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ins.compile()通过方言编译数据，但是并没执行，因此我们需要用params属性查看。&lt;/p&gt;
&lt;p&gt;介绍了新建语句的用法之后，我们用connection的execute()方法把数据加入数据表。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = connection.execute(ins)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们还可以用inserted_primary_key属性查看刚刚新建数据的ID号。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result.inserted_primary_key
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们简单介绍一下excute()执行的过程。当我们建立前面那条SQL表达式语言的插入语句时，实际上是创建了一个可以快速向下遍历的树状结构。当我们调用excute()方法时，它把刚刚传入的语句和其他任何参数一起编译成对应数据库方言编译器能够识别的语句。编译器通过遍历那个树状结构建成一个普通的SQL语句。这个语句返回到excute()方法，excute()方法通过绑定的连接把语句传递到数据库。数据库服务器就执行SQL语句然后把结果返回给excute()方法。&lt;/p&gt;
&lt;p&gt;insert除了可以作为表对象的实例方法，也可以当作顶层函数使用，这样可以把表对象作为参数，更具灵活性。例如，假如公司的两个部门分别拥有相互独立的库存数据，就可以按照例3-3的形式再插入一行数据。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import insert
ins = insert(cookies).values(
    cookie_name=&amp;quot;chocolate chip&amp;quot;,
    cookie_recipe_url=&amp;quot;http://some.aweso.me/cookie/recipe.html&amp;quot;,
    cookie_sku=&amp;quot;CC01&amp;quot;,
    quantity=&amp;quot;12&amp;quot;,
    unit_cost=&amp;quot;0.50&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;虽然insert的表对象方法和更具一般性的函数两种形式结果一样，我还是更喜欢后者，因为它更接近SQL语句的用法。&lt;/p&gt;
&lt;p&gt;连接对象的execute()方法不仅仅只是处理语句，还可以把values当作execute()方法的参数。当语句被编译时，它会把每个关键词参数的键增加到字段列表中，然后再把每个字段对应的值增加到SQL语句的VALUE参数里。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins = cookies.insert()
result = connection.execute(
    ins,
    cookie_name='dark chocolate chip',
    cookie_recipe_url='http://some.aweso.me/cookie/recipe_dark.html',
    cookie_sku='CC02',
    quantity='1',
    unit_cost='0.75'
)
result.inserted_primary_key
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[2]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;不过这种形式并不常用，但是，它为语句在传到数据库服务器之前的编译和组织方式提供了一个很好的解释。我们可以用放了字段和数值词典的列表一次性插入多个记录。让我们把两种饼干的库存数据插入cookies表。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins = cookies.insert()
inventory_list = [
    {
        'cookie_name': 'peanut butter',
        'cookie_recipe_url': 'http://some.aweso.me/cookie/peanut.html',
        'cookie_sku': 'PB01',
        'quantity': '24',
        'unit_cost': '0.25'
    },
    {
        'cookie_name': 'oatmeal raisin',
        'cookie_recipe_url': 'http://some.okay.me/cookie/raisin.html',
        'cookie_sku': 'EWW01',
        'quantity': '100',
        'unit_cost': '1.00'
    }
]
result = connection.execute(ins, inventory_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;列表中的每个词典都要有同样的键（keys）。首先方言编译器会编译第一个词典的语句内容，如果后面词典的键与第一条不同就会失败，因为第一条的字段已经建好了。
现在有了数据，我们就可以查询了。&lt;/p&gt;
&lt;h5&gt;Querying Data&lt;/h5&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy.sql import select
s = select([cookies])
rp = connection.execute(s)
results = rp.fetchall()
results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(1, 'chocolate chip', 'http://some.aweso.me/cookie/recipe.html', 'CC01', 12, Decimal('0.50')),
 (2, 'dark chocolate chip', 'http://some.aweso.me/cookie/recipe_dark.html', 'CC02', 1, Decimal('0.75')),
 (3, 'peanut butter', 'http://some.aweso.me/cookie/peanut.html', 'PB01', 24, Decimal('0.25')),
 (4, 'oatmeal raisin', 'http://some.okay.me/cookie/raisin.html', 'EWW01', 100, Decimal('1.00'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(str(s))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SELECT cookies.cookie_id, cookies.cookie_name, cookies.cookie_recipe_url, cookies.cookie_sku, cookies.quantity, cookies.unit_cost 
FROM cookies
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和insert用法类似，select也是既可以作为表对象的实例方法，也可以作为更具一般性的顶层函数使用。我更喜欢顶层函数的使用方式，因为和SQL用法一样。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy.sql import select
s = cookies.select()
rp = connection.execute(s)
results = rp.fetchall()
results
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(1, 'chocolate chip', 'http://some.aweso.me/cookie/recipe.html', 'CC01', 12, Decimal('0.50')),
 (2, 'dark chocolate chip', 'http://some.aweso.me/cookie/recipe_dark.html', 'CC02', 1, Decimal('0.75')),
 (3, 'peanut butter', 'http://some.aweso.me/cookie/peanut.html', 'PB01', 24, Decimal('0.25')),
 (4, 'oatmeal raisin', 'http://some.okay.me/cookie/raisin.html', 'EWW01', 100, Decimal('1.00'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;ResultProxy&lt;/h5&gt;
&lt;p&gt;ResultProxy是对数据库API光标对象的封装，其主要目的是让数据操作更简单。例如，它可以通过索引，字段名称和列对象让查询操作更简单，演示如例3-8所示。任何一种方法查询数据都很简单。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;first_row = results[0]

first_row[1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;'chocolate chip'
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;first_row.cookie_name
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;'chocolate chip'
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;first_row[cookies.c.cookie_name]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;'chocolate chip'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的输出结果都是results变量第一个记录的数据chocolate chip。接入灵活只是ResultProxy能力的一部分。我们还可以用ResultProxy实现循环。例如，我们可以打印所有饼干的名称。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;rp = connection.execute(s)
for record in rp:
    print(record.cookie_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;chocolate chip
dark chocolate chip
peanut butter
oatmeal raisin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ResultProxy除了可以循环，或调用fetchall()方法之外，很多其他数据接入方式也可以用。其实，上例中所有的result变量所有的插入数据操作都是用ResultProxy实现的。rowcount()和inserted_primary_key()方法也是ResultProxy获取信息的一种方式。你也可以用下面的方法获取信息：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first()——如果存在则返回第一条记录并关闭连接&lt;/li&gt;
&lt;li&gt;fetchone()——返回一条记录，光标继续开着，等待新的查询&lt;/li&gt;
&lt;li&gt;scalar()——如果查询结果只有一行一列，就返回一个值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如果要看结果中所有列名，可以用keys()方法。后面的章节里，我们还会经常使用first，scalar，fetchone，fetchall方法和ResultProxy循环。&lt;/p&gt;
&lt;p&gt;产品代码 写产品代码的时候，我有几条原则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;first获取一条记录，不用scalar和fetchone，因为这样更清晰&lt;/li&gt;
&lt;li&gt;用ResultProxy循环，不用fetchall和fetchone方法。这样内存开销更新，因为通常我们都是一次处理一条记录&lt;/li&gt;
&lt;li&gt;尽量不要用fetchone，因为它会让连接一直开着&lt;/li&gt;
&lt;li&gt;scalar也要少用，因为当查询返回多余一行一列数据的时候容易出错，这一点在测试的时候经常被忽略&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在上例中，每一次我们查询数据集的时候每条记录的所有列都会返回。而通常我们只需要一部分列就可以。如果数据非常大，这样查询就会非常耗费内存，查询速度就会很慢。SQLAlchemy不会为查询或ResultProxy增加负担；但是，通常查询完成后，你需要看看查询是否消耗了太多内存。下面我们就来介绍如何控制查询的范围。&lt;/p&gt;
&lt;h5&gt;控制查询的列&lt;/h5&gt;
&lt;p&gt;可以用select()方法将要查询的列以列表形式放入。例如，下面代码是只需要查看饼干的名称和质量时的操作。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
rp = connection.execute(s)
print(rp.keys())
result = rp.first()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;['cookie_name', 'quantity']
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('chocolate chip', 12)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们就建立了简单的select语句，我们将看看其他改变选择结果的操作。首先我们看看如何改变顺序。&lt;/p&gt;
&lt;h5&gt;Ordering&lt;/h5&gt;
&lt;p&gt;如果在上面例10中你要查看所有的数据结果，你会发现名称排序很混乱。但是，如果我们想让名称按照指定顺序排列，可用select的order_by()语句。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
s = s.order_by(cookies.c.quantity)
rp = connection.execute(s)
for cookie in rp:
    print('{} - {}'.format(cookie.quantity, cookie.cookie_name))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1 - dark chocolate chip
12 - chocolate chip
24 - peanut butter
100 - oatmeal raisin
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果要逆序排列，就在order_by()里面增加desc()。用desc()包裹排序参数即可。&lt;/p&gt;
&lt;p&gt;desc()也可以当成列对象的方法来使用，&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import desc
s = select([cookies.c.cookie_name, cookies.c.quantity])
s = s.order_by(desc(cookies.c.quantity))
rp = connection.execute(s)
for cookie in rp:
    print('{} - {}'.format(cookie.quantity, cookie.cookie_name))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;100 - oatmeal raisin
24 - peanut butter
12 - chocolate chip
1 - dark chocolate chip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;还可以限制查询记录结果的数量。&lt;/p&gt;
&lt;h5&gt;Limiting&lt;/h5&gt;
&lt;p&gt;前面的例子里，first()和fetchone()方法是用来获得一行记录的。ResultProxy可以提供一行数据，实际中我们经常需要多行数据。如果我们想限制查询数量，我们可以用limit()函数。例如，如果我现在想做销量最好的两种饼干，那么对排序后的查询增加一个限制条件就可以了。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
s = s.order_by(cookies.c.quantity)
s = s.limit(2)
rp = connection.execute(s)
print([result.cookie_name for result in rp])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;['dark chocolate chip', 'chocolate chip']
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在我们知道饼干的种类了，我开始关心库存还有多少。很多数据库都有一堆SQL函数对数据进行统计，必然SUM等等，让我们看看这些函数如何使用。&lt;/p&gt;
&lt;h5&gt;Builtin SQL Functions and Labels&lt;/h5&gt;
&lt;p&gt;SQLAlchemy的SQL函数里最常用的是SUM()和COUNT()。使用这个函数之前我们需要导入sqlalchemy.sql.func模块，这些函数只要包裹列对象就可以运行。所以要统计饼干的总量，我们可以这样：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import func
s = select([func.sum(cookies.c.quantity)])
rp = connection.execute(s)
print(rp.scalar())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;137
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;因为Python内置函数也有sum，所以使用SQLAlchemy的sum时，建议导入func用func.sum。&lt;/p&gt;
&lt;p&gt;现在让我们用count函数统计cookie表里有多少条库存记录。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([func.count(cookies.c.quantity)])
rp = connection.execute(s)
record = rp.first()
print(record.keys()) # 显示Resultproxy里的列
print(record.count_1) # 列名是自动生成的，命名方式是`&amp;lt;函数名&amp;gt;_&amp;lt;位置&amp;gt;`
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;['count_1']
4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这个列名有点隐蔽。如果我们想自定义名称，也可以用在count()函数后用label()函数来定义列名。例如，我们想用更好记的名称来定义记录数量。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([func.count(cookies.c.cookie_name).label('inventory_count')])
rp = connection.execute(s)
record = rp.first()
print(record.keys())
print(record.inventory_count)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;['inventory_count']
4
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;介绍了如何限制行和列的数量之后，我们再看看如何进行数据过滤。&lt;/p&gt;
&lt;h5&gt;Filtering&lt;/h5&gt;
&lt;p&gt;过滤查询和SQL一样用where()函数来实现。通常where()函数有一个列名称，一个操作符和一个值或列。也可以用连接多个where()语句，像SQL里面逻辑操作符AND的作用。下面我们来找名称为chocolate chip的饼干。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies]).where(cookies.c.cookie_name == 'chocolate chip')
rp = connection.execute(s)
record = rp.first()
record.items() # 显示所有列名和数值
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('cookie_id', 1),
 ('cookie_name', 'chocolate chip'),
 ('cookie_recipe_url', 'http://some.aweso.me/cookie/recipe.html'),
 ('cookie_sku', 'CC01'),
 ('quantity', 12),
 ('unit_cost', Decimal('0.50'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们还可以用where()找包含chocolate的饼干名称。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies]).where(cookies.c.cookie_name.like('%chocolate%'))
rp = connection.execute(s)
for record in rp.fetchall():
    print(record.cookie_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;chocolate chip
dark chocolate chip
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;where()语句里我们用cookies.c.cookie_name作为过滤的ClauseElement类型。我们现在停下来，看看ClauseElement类型的其他功能。&lt;/p&gt;
&lt;h5&gt;Operators&lt;/h5&gt;
&lt;p&gt;ClauseElements就是我们在从句中使用的一个元素，通常都是数据表的列；但是，和列不同，ClauseElements有许多功能。在例3-18里面，我们用来like()方法，其实还有很多方法如表3-1所示。每个方法都和标准SQL里面的函数类似。后面会大量使用它们。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: center;"&gt;方法&lt;/th&gt;
&lt;th style="text-align: center;"&gt;目的&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;between(cleft, cright)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column is between cleft and cright&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;concat(column_two)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Concatenate column with column_two&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;distinct()&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find only unique values for column&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;in_([list])&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column is in the list&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;is_(None)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column is None (commonly used for Null checks with None)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;contains(string)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column has string in it (Case-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;endswith(string)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column ends with string (Case-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;like(string)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column is like string (Case-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;startswith(string)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column begins with string (Case-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: center;"&gt;ilike(string)&lt;/td&gt;
&lt;td style="text-align: center;"&gt;Find where the column is like string (NOT Case-sensitive)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;还有两个相反操作的函数notlike()和notin_()，以及一个不带下划线的函数isnot()。&lt;/p&gt;
&lt;p&gt;如果我们不用这些方法，我们还可以用运算符。大部分运算符都和你的习惯一样，不过我们要介绍一些奇怪的地方。&lt;/p&gt;
&lt;h4&gt;操作符&lt;/h4&gt;
&lt;p&gt;到目前为止我们过滤数据都是通过判断列是否等于一个数值，或者用ClauseElement方法的函数，比如like()；其实我们还可以用很多运算符号来过滤数据。SQLAlchemy提供了大量Python的标准运算符。这包括所有的标准比较运算符（==，!=，&amp;lt;，&amp;gt;，&amp;gt;=），与Python语句里使用方法一致。==操作符在和None比较时还表示IS NULL。算法运算符（+,-,*,/,%）也可用于数据库字符串处理，如例3-19所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, 'SKU-' + cookies.c.cookie_sku])
for row in connection.execute(s):
    print(row)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('chocolate chip', 'SKU-CC01')
('dark chocolate chip', 'SKU-CC02')
('peanut butter', 'SKU-PB01')
('oatmeal raisin', 'SKU-EWW01')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;另一个常用操作就是计算多列的数值。通常做财务和统计报表时经常用到，例3-20计算库存金额：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import cast
s = select([cookies.c.cookie_name,
            cast((cookies.c.quantity * cookies.c.unit_cost),
                 Numeric(12,2)).label('inv_cost')])
for row in connection.execute(s):
    print('{} - {}'.format(row.cookie_name, row.inv_cost))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;chocolate chip - 6.00
dark chocolate chip - 0.75
peanut butter - 6.00
oatmeal raisin - 100.00
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;Boolean Operators&lt;/h5&gt;
&lt;p&gt;SQLAlchemy也支持逻辑运算符（与AND，或OR，非NOT），用位操作符表示（&amp;amp;，|，~）。使用时一定要注意，因为Python自带的逻辑运算符规则。比如，&amp;amp;比&amp;lt;优先级高，当你写A &amp;lt; B &amp;amp; C &amp;lt; D的时候，运算结果是A &amp;lt; (B &amp;amp; C) &amp;lt; D，而你实际想写的是(A &amp;lt; B) &amp;amp; (C &amp;lt; D)。为了清晰表达意思，请用连接词（conjunctions），不要用这些重载运算符。&lt;/p&gt;
&lt;p&gt;通常在处理多个从句时，从句之间存在与或非关系时，应该用连接词。&lt;/p&gt;
&lt;h5&gt;Conjunctions&lt;/h5&gt;
&lt;p&gt;虽然可以把多个where()从句连起来，但是用连接词会让语句更清晰好看。SQLAlchemy的连接词是and_()，or_()和not_()。如果我们想要查询库存和单价满足特定条件的饼干时，可以用and_()实现。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import and_, or_, not_
s = select([cookies]).where(
    and_(
        cookies.c.quantity &amp;gt; 23,
        cookies.c.unit_cost &amp;lt; 0.40
    )
)
for row in connection.execute(s):
    print(row.cookie_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;peanut butter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or_()函数与and_()相反，包含任何一种情形就可以。如果我们想找到库存在10到50之间，或名称包含chip的饼干时，可以用or_。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import and_, or_, not_
s = select([cookies]).where(
    or_(
        cookies.c.quantity.between(10, 50),
        cookies.c.cookie_name.contains('chip')
    )
)
for row in connection.execute(s):
    print(row.cookie_name)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;chocolate chip
dark chocolate chip
peanut butter
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;not()连接词类似，只是表示不包含条件的结果。到这里我们已经可以轻松的查询数据了，下面我们看看如何更新数据。&lt;/p&gt;
&lt;h5&gt;Updating Data&lt;/h5&gt;
&lt;p&gt;还有一个数据更新方法，和前面用的insert方法类似，除了它们需要指定一个条件表面要更新的行，语法与insert完全一致。更新方法可以用update()函数或待更新表的update()方法。如果不增加条件，就会更新表中所有行。当我做完新的饼干之后，就要更新库存数据。在例3-23中，我们更新对应饼干的库存，然后计算新的总库存量。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import update
u = update(cookies).where(cookies.c.cookie_name == 'chocolate chip')
u = u.values(quantity=(cookies.c.quantity + 120))
result = connection.execute(u)
print(result.rowcount)

s = select([cookies]).where(cookies.c.cookie_name == &amp;quot;chocolate chip&amp;quot;)
result = connection.execute(s).first()
for key in result.keys():
    print('{:&amp;gt;20}: {}'.format(key, result[key]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1
cookie_id: 1
cookie_name: chocolate chip
cookie_recipe_url: http://some.aweso.me/cookie/recipe.html
cookie_sku: CC01
quantity: 132
unit_cost: 0.50
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;除了更新数据，还要删除数据。&lt;/p&gt;
&lt;h5&gt;Deleting Data&lt;/h5&gt;
&lt;p&gt;删除数据可以用delete()函数或表的delete()方法。和insert()，update()不同的是，delete()无数值参数，只有一个可选的where()用于设置删除区域（如果没有就删除全表所有数据）。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import delete

u = delete(cookies).where(cookies.c.cookie_name == 'dark chocolate chip')
result = connection.execute(u)
print(result.rowcount)

s = select([cookies]).where(cookies.c.cookie_name == &amp;quot;dark chocolate chip&amp;quot;)
result = connection.execute(s).fetchall()
print(len(result))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;1
0
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在让我们回想一下所学的知识，对users，orders，line_items表进行更新。你可以直接复制代码，但是建议你试试其他方法新建数据。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;customer_list = [
    {
        'username': 'cookiemon',
        'email_address': 'mon@cookie.com',
        'phone': '111-111-1111',
        'password': 'password'
    },
    {
        'username': 'cakeeater',
        'email_address': 'cakeeater@cake.com',
        'phone': '222-222-2222',
        'password': 'password'
    },
    {
        'username': 'pieguy',
        'email_address': 'guy@pie.com',
        'phone': '333-333-3333',
        'password': 'password'
    }
]
ins = users.insert()
result = connection.execute(ins, customer_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有了用户数据，让我们再更新他们的orders，line_items表。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import insert
ins = insert(orders).values(user_id=1, order_id=1)
result = connection.execute(ins)
ins = insert(line_items)
order_items = [
    {
        'order_id': 1,
        'cookie_id': 1,
        'quantity': 2,
        'extended_cost': 1.00
    },
    {
        'order_id': 1,
        'cookie_id': 3,
        'quantity': 12,
        'extended_cost': 3.00
    }
]
result = connection.execute(ins, order_items)

ins = insert(orders).values(user_id=2, order_id=2)
result = connection.execute(ins)
ins = insert(line_items)
order_items = [
    {
        'order_id': 2,
        'cookie_id': 1,
        'quantity': 24,
        'extended_cost': 12.00
    },
    {
        'order_id': 2,
        'cookie_id': 4,
        'quantity': 6,
        'extended_cost': 6.00
    }
]
result = connection.execute(ins, order_items)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在SQLAlchemy Core的第二章中我们介绍过外键和关系；但是我们还没有用它们做过查询，下面我们就来看看这些关系。&lt;/p&gt;
&lt;h5&gt;Joins&lt;/h5&gt;
&lt;p&gt;用join()和outerjoin()方法进行数据关联。例如，发货之前需要了解用户cookiemon订购了那种饼干。这就需要用3个join来汇总三张表的数据。另外，当用多个join汇总数据时，你可能需要重新组织from后面join关联内容的顺序，SQLAlchemy提供了select_from来实现这个功能。通过select_from我们可以将整个from从句替换成SQLAlchemy支持的形式。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;columns = [orders.c.order_id, users.c.username, users.c.phone,
            cookies.c.cookie_name, line_items.c.quantity,
            line_items.c.extended_cost]
cookiemon_orders = select(columns)
cookiemon_orders = cookiemon_orders.select_from(orders.join(users).join(
                    line_items).join(cookies)).where(users.c.username ==
                    'cookiemon')
result = connection.execute(cookiemon_orders).fetchall()
for row in result:
    print(row)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(1, 'cookiemon', '111-111-1111', 'chocolate chip', 2, Decimal('1.00'))
(1, 'cookiemon', '111-111-1111', 'peanut butter', 12, Decimal('3.00'))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;上面的SQL语句是这样：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print(str(cookiemon_orders))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SELECT orders.order_id, users.username, users.phone, cookies.cookie_name, line_items.quantity, line_items.extended_cost 
FROM orders JOIN users ON users.user_id = orders.user_id JOIN line_items ON orders.order_id = line_items.order_id JOIN cookies ON cookies.cookie_id = line_items.cookie_id 
WHERE users.username = :username_1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;统计所有用户各自的订单数量也常用，不只是当前的订单。可以通过outerjoin()方法实现，需要注意join的顺序，因为用outerjoin()方法生成的表会返回所有外键匹配结果。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;columns = [users.c.username, func.count(orders.c.order_id)]
all_orders = select(columns)
all_orders = all_orders.select_from(users.outerjoin(orders))
all_orders = all_orders.group_by(users.c.username) #后面介绍分组
result = connection.execute(all_orders).fetchall()
for row in result:
    print(row)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('cakeeater', 1)
('cookiemon', 1)
('pieguy', 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在，我们已经可以通过join实现关联查询了。但是，如果我们有一个像员工与老板关系表，用SQLAlchemy清楚的读取和理解内容需要用alias。&lt;/p&gt;
&lt;h5&gt;Aliases&lt;/h5&gt;
&lt;p&gt;用join的时候，通常要对一个表进行多次引用。在SQL里面，这是通过查询中的aliases实现的。例如，假设我们有下面的schema结构：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;employee_table = Table(
    'employee', metadata,
    Column('id', Integer, primary_key=True),
    Column('manager_id', None, ForeignKey('employee.id')),
    Column('name', String(255)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在假如我们想选择Fred手下的所有员工。在SQL里面，我们会这么做：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-sql"&gt;SELECT employee.name
FROM employee, employee AS manager
WHERE employee.manager_id = manager.id
AND manager.name = 'Fred'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SQLAlchemy也允许用alias()方法实现：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;manager = employee_table.alias('mgr')

stmt = select([employee_table.c.name],
             and_(employee_table.c.id==manager.c.id,
                 manager.c.name=='Fred'))

print(stmt)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SELECT employee.name 
FROM employee, employee AS mgr 
WHERE employee.id = mgr.id AND mgr.name = :name_1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SQLAlchemy也可以自动选择别名：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;manager = employee_table.alias()

stmt = select([employee_table.c.name],
             and_(employee_table.c.id==manager.c.id,
                 manager.c.name=='Fred'))

print(stmt)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;SELECT employee.name 
FROM employee, employee AS employee_1 
WHERE employee.id = employee_1.id AND employee_1.name = :name_1
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;数据分组group_by也是很有用的，下面我们来看看。&lt;/p&gt;
&lt;h5&gt;Grouping&lt;/h5&gt;
&lt;p&gt;SQLAlchemy可以对一个或多个列进行数据分组，然后统计各组的计数项（count），总和（sum）和其他统计参数，与SQL类似。下面我们看看每个客户的订单数：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;columns = [users.c.username, func.count(orders.c.order_id)]
all_orders = select(columns)
all_orders = all_orders.select_from(users.outerjoin(orders))
all_orders = all_orders.group_by(users.c.username)
result = connection.execute(all_orders).fetchall()
for row in result:
    print(row)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('cakeeater', 1)
('cookiemon', 1)
('pieguy', 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;前面的join的例子中我们已经用过，这里再看看group_by的用法。&lt;/p&gt;
&lt;h5&gt;Chaining&lt;/h5&gt;
&lt;p&gt;前面我们已经用过链式表达式，只是那时没有明说。进行数据查询时链式表达式能够清楚地显示查询的逻辑。因此，如果我们想用一个函数获取订单数据，可以如例3-28所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def get_orders_by_customer(cust_name):
    columns = [orders.c.order_id, users.c.username, users.c.phone,
                cookies.c.cookie_name, line_items.c.quantity, 
                line_items.c.extended_cost]
    cust_orders = select(columns)
    cust_orders = cust_orders.select_from(users.join(orders).join(line_items).join(cookies))
    cust_orders = cust_orders.where(users.c.username == cust_name)
    result = connection.execute(cust_orders).fetchall()
    return result

get_orders_by_customer('cakeeater')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(2, 'cakeeater', '222-222-2222', 'chocolate chip', 24, Decimal('12.00')),
 (2, 'cakeeater', '222-222-2222', 'oatmeal raisin', 6, Decimal('6.00'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;如果我们只想显示已发货或未发货的订单，怎么办？我们可以再写函数来支持其他选项，或者我们可以用链式查询来过滤。推荐后一种方法，尤其是处理复杂查询和制作报表时威力非常强大。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def get_orders_by_customer(cust_name, shipped=None, details=False):
    columns = [orders.c.order_id, users.c.username, users.c.phone]
    joins = users.join(orders)
    if details:
        columns.extend([cookies.c.cookie_name, line_items.c.quantity,
                        line_items.c.extended_cost])
        joins = joins.join(line_items).join(cookies)
    cust_orders = select(columns)
    cust_orders = cust_orders.select_from(joins)
    cust_orders = cust_orders.where(users.c.username == cust_name)
    if shipped is not None:
        cust_orders = cust_orders.where(orders.c.shipped == shipped)
    result = connection.execute(cust_orders).fetchall()
    return result

get_orders_by_customer('cakeeater')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(2, 'cakeeater', '222-222-2222')]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;get_orders_by_customer('cakeeater', details=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(2, 'cakeeater', '222-222-2222', 'chocolate chip', 24, Decimal('12.00')),
 (2, 'cakeeater', '222-222-2222', 'oatmeal raisin', 6, Decimal('6.00'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;get_orders_by_customer('cakeeater', shipped=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;get_orders_by_customer('cakeeater', shipped=False)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(2, 'cakeeater', '222-222-2222')]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;get_orders_by_customer('cakeeater', shipped=False, details=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(2, 'cakeeater', '222-222-2222', 'chocolate chip', 24, Decimal('12.00')),
 (2, 'cakeeater', '222-222-2222', 'oatmeal raisin', 6, Decimal('6.00'))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;到此为止，我们已经用SQL表达式语言的例子演示了SQLAlchemy Core。其实，你也可以直接用标准的SQL语言来进行。&lt;/p&gt;
&lt;h5&gt;Raw Queries&lt;/h5&gt;
&lt;p&gt;在SQLAlchemy Core里面可以用原始的SQL语句进行操作，结果也是返回一个代理，后面的操作和SQLAlchemy Core的SQL表达式语法一样。除非必须，一般不推荐使用原始SQL，因为这么做存在安全隐患。下面我们简单演示一下。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;result = connection.execute('select * from orders').fetchall()
print(result)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(1, 1, 0), (2, 2, 0)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;有时候一些SQL小片段可以让语句表述更清晰。下面是用text()函数实现where的条件。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import text
stmt = select([users]).where(text(&amp;quot;username='cookiemon'&amp;quot;))
print(connection.execute(stmt).fetchall())
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[(1, None, 'cookiemon', 'mon@cookie.com', '111-111-1111', 'password', datetime.datetime(2015, 9, 1, 21, 37, 37, 531465), datetime.datetime(2015, 9, 1, 21, 37, 37, 531465))]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过本章介绍的增删改查等操作，现在你应该已经理解SQLAlchemy Core的SQL表达式用法了，用自己的数据库试试吧。下面我们来介绍SQLAlchemy的异常处理，以及事务处理transactions分组的用法。&lt;/p&gt;
&lt;h3&gt;Exceptions and Transactions&lt;/h3&gt;
&lt;p&gt;前面做数据处理时，大部分工作我们都只用一行语句就搞定了，我们尽量避免做任何可能引起异常的事情，而本章我们会刻意搞点bug来演示异常处理方法。另外，我们还会介绍如何把对需要处理的任务分成单独的事务进行处理，保证每个事务都可以被适当地执行或正确地清理。&lt;/p&gt;
&lt;h4&gt;Exceptions&lt;/h4&gt;
&lt;p&gt;SQLAlchemy会产生许多不同类型的异常；不过这里只重点介绍一些常见的异常：AttributeErrors和IntegrityErrors。通过对常用异常处理方法的学习，你可以掌握其他异常的处理方法。&lt;/p&gt;
&lt;p&gt;下面，请重新开一个Python shell或Notebook，然后用SQLAlchemy Core建立数据表。具体过程如例4-1所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
from sqlalchemy import (MetaData, Table, Column, Integer, Numeric, String,
                        DateTime, ForeignKey, Boolean, create_engine,
                        CheckConstraint)
metadata = MetaData()
cookies = Table('cookies', metadata,
                Column('cookie_id', Integer(), primary_key=True),
                Column('cookie_name', String(50), index=True),
                Column('cookie_recipe_url', String(255)),
                Column('cookie_sku', String(55)),
                Column('quantity', Integer()),
                Column('unit_cost', Numeric(12, 2)),
                CheckConstraint('quantity &amp;gt; 0', name='quantity_positive')
                )
users = Table('users', metadata,
              Column('user_id', Integer(), primary_key=True),
              Column('username', String(15), nullable=False, unique=True),
              Column('email_address', String(255), nullable=False),
              Column('phone', String(20), nullable=False),
              Column('password', String(25), nullable=False),
              Column('created_on', DateTime(), default=datetime.now),
              Column('updated_on', DateTime(),
                     default=datetime.now, onupdate=datetime.now)
              )
orders = Table('orders', metadata,
               Column('order_id', Integer()),
               Column('user_id', ForeignKey('users.user_id')),
               Column('shipped', Boolean(), default=False)
               )
line_items = Table('line_items', metadata,
                   Column('line_items_id', Integer(), primary_key=True),
                   Column('order_id', ForeignKey('orders.order_id')),
                   Column('cookie_id', ForeignKey('cookies.cookie_id')),
                   Column('quantity', Integer()),
                   Column('extended_cost', Numeric(12, 2))
                   )
engine = create_engine('sqlite:///:memory:')
metadata.create_all(engine)
connection = engine.connect()

metadata.tables.keys()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;dict_keys(['users', 'cookies', 'line_items', 'orders'])
&lt;/code&gt;&lt;/pre&gt;
&lt;h5&gt;AttributeError&lt;/h5&gt;
&lt;p&gt;AttributeError是指获取一个不存在的属性时产生的异常。经常是在连接ResultProxy里没有的一列时发生。在尝试获取一个对象不存在的属性时AttributeError也会发生。在普通的Python代码里也会发生。这里单独拿出来说是因为SQLAlchemy里非常容易出现这类异常，而其产生的根源却很容易忽略。为了演示这类异常，我们在users表里插入一列数据，然后我们查询没有selsect的一列来诱发异常。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy import select, insert
ins = insert(users).values(
    username=&amp;quot;cookiemon&amp;quot;,
    email_address=&amp;quot;mon@cookie.com&amp;quot;,
    phone=&amp;quot;111-111-1111&amp;quot;,
    password=&amp;quot;password&amp;quot;
)
result = connection.execute(ins)

s = select([users.c.username])
results = connection.execute(s)
for result in results:
    print(result.username)
    print(result.password)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;    cookiemon
    ---------------------------------------------------------------------------
    KeyError                                  Traceback (most recent call last)
    d:\programfiles\Miniconda3\lib\site-packages\sqlalchemy\engine\result.py in __getitem__(self, key)
         69             try:
    ---&amp;gt; 70                 processor, obj, index = self._keymap[key]
         71             except KeyError:

    KeyError: 'password'

    During handling of the above exception, another exception occurred:

    NoSuchColumnError                         Traceback (most recent call last)
    d:\programfiles\Miniconda3\lib\site-packages\sqlalchemy\engine\result.py in __getattr__(self, name)
         95             try:
    ---&amp;gt; 96                 return self[name]
         97             except KeyError as e:

    d:\programfiles\Miniconda3\lib\site-packages\sqlalchemy\engine\result.py in __getitem__(self, key)
         71             except KeyError:
    ---&amp;gt; 72                 processor, obj, index = self._parent._key_fallback(key)
         73             except TypeError:

    d:\programfiles\Miniconda3\lib\site-packages\sqlalchemy\engine\result.py in _key_fallback(self, key, raiseerr)
        405                     &amp;quot;Could not locate column in row for column '%s'&amp;quot; %
    --&amp;gt; 406                     expression._string_or_unprintable(key))
        407             else:

    NoSuchColumnError: &amp;quot;Could not locate column in row for column 'password'&amp;quot;

    During handling of the above exception, another exception occurred:

    AttributeError                            Traceback (most recent call last)
    &amp;lt;ipython-input-9-c4520631a10a&amp;gt; in &amp;lt;module&amp;gt;()
          3 for result in results:
          4     print(result.username)
    ----&amp;gt; 5     print(result.password)

    d:\programfiles\Miniconda3\lib\site-packages\sqlalchemy\engine\result.py in __getattr__(self, name)
         96                 return self[name]
         97             except KeyError as e:
    ---&amp;gt; 98                 raise AttributeError(e.args[0])
         99 
        100 

    AttributeError: Could not locate column in row for column 'password'
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;在例4-2中我们看到Python抛出了AttributeError并停止了程序，我们可以看到AttributeError是Python里常见的形式。首先显示错异常的类型，然后一个箭头会指明异常发生的位置，紧接着是异常发生源代码。最后一行会显示异常的具体内容，它会显示异常类型，以及为什么发生异常。这里出现异常的原因是ResultProxy里面没有password列，我们只查询了username列。这是在使用SQLAlchemy对象时出现的一个常见Python异常，还有一些SQLAlchemy自己特有的异常。下面就是其中一个：IntegrityError。&lt;/p&gt;
&lt;h5&gt;IntegrityError&lt;/h5&gt;
&lt;p&gt;IntegrityError是另一个SQLAlchemy常见的异常，当我们做了不符合数据表或字段约束条件的事情时就会发生。当你请求的数据是唯一的，比如users表中的username,想创建两个同名用户时就会产生IntegrityError，演示程序如下所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([users.c.username])

connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('cookiemon',)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins = insert(users).values(
    username=&amp;quot;cookiemon&amp;quot;,
    email_address=&amp;quot;damon@cookie.com&amp;quot;,
    phone=&amp;quot;111-111-1111&amp;quot;,
    password=&amp;quot;password&amp;quot;
)

result = connection.execute(ins)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;和前面的AttributeError类似。首先显示错异常的类型，然后一个箭头会指明异常发生的位置，紧接着是异常发生源代码。最后一行会显示异常的具体内容，它会显示异常类型，以及为什么发生异常。这里指明了发生的原因：&lt;/p&gt;
&lt;p&gt;UNIQUE constraint failed: users.username
这就告诉我们在users表的username里面插入同名用户是不允许的。之后的内容是SQLAlchemy表达式转变成的SQL语句，我们在第三章里介绍过，还有我们计划插入却产生异常的数据。程序在这里停止。&lt;/p&gt;
&lt;p&gt;当然会有很多种异常类型，这里介绍的两种是最常见的。SQLAlchemy里所有异常发生都会按照这两种方式产生。具体异常的内容请查看SQLAlchemy文档。&lt;/p&gt;
&lt;p&gt;为了保证程序在发生异常时继续运行，我们需要实现异常处理方法。&lt;/p&gt;
&lt;h5&gt;Handling Errors&lt;/h5&gt;
&lt;p&gt;要防止异常中断程序，我们需要正确地处理异常。这与Python的异常处理方法一样，用try/except代码块实现。例如，我们可以用try/except代码块捕捉异常显示信息，然后让后面的程序继续运行。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy.exc import IntegrityError

ins = insert(users).values(
    username=&amp;quot;cookiemon&amp;quot;,
    email_address=&amp;quot;damon@cookie.com&amp;quot;,
    phone=&amp;quot;111-111-1111&amp;quot;,
    password=&amp;quot;password&amp;quot;
)
try:
    result = connection.execute(ins)
except IntegrityError as error:
    print(error)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(sqlite3.IntegrityError) UNIQUE constraint failed: users.username [SQL: 'INSERT INTO users (username, email_address, phone, password, created_on, updated_on) VALUES (?, ?, ?, ?, ?, ?)'] [parameters: ('cookiemon', 'damon@cookie.com', '111-111-1111', 'password', '2015-09-01 13:45:18.991258', '2015-09-01 13:45:18.992259')]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;虽然和前面例子代码一样，但是因为有try/except处理IntegrityError异常，所有结果就是简单的异常信息。这个例子只是演示了异常打印功能，其实我们可以在异常处理中写任何Python代码。返回异常信息告诉用户刚刚的操作失败了是很有用的做法。异常处理完成后，程序还会继续运行。虽然这里只处理了IntegrityError，但对其他SQLAlchemy异常也适用。&lt;/p&gt;
&lt;p&gt;try/except里面的代码越少越好。因为代码太多可能会出现你意料之外的异常，不符合你捕捉的目标。&lt;/p&gt;
&lt;p&gt;用传统的Python方法可以捕捉一行语句的异常，如果我们有多个数据库语句，彼此之间互相关联，这个方法可能就不适用了。这时，我们需要将那些语句封装成一个数据库事务，SQLAlchemy提供了一个简单的包装来建立对象之间的链接：transactions。&lt;/p&gt;
&lt;h5&gt;Transactions&lt;/h5&gt;
&lt;p&gt;我们不需要学习一堆数据库理论，可以把事务看成是一种确保多个数据库语句打包成一组运行成功或运行失败的处理方式。当启动一个事务的时候，我们记录数据库的状态，然后执行多条SQL语句。如果所有的SQL语句都能顺利执行，数据库就会不断的更新状态，忽略前面的状态，如下图所示。 但是，如果有一条语句运行失败了，整个数据库就要回退（rollback）到原来的状态，如下图所示。 举个我们可能会在之前建立的数据库里操作的例子。当客户买了我们的饼干之后，我们就需要把饼干邮寄给客户，同时更新库存量。但是，如果我们没有足够的饼干库存履行客户的订单需求，怎么办呢？我们就查检查库存，并且不邮寄订单。这就可以用事务解决。&lt;/p&gt;
&lt;p&gt;我们再新建一个Python Shell或Notebook，还用第三章的表，只是为quantity字段增加一个CheckConstraint限制条件，即库存量不能小于0。然后我们创建用户cookiemon，并设置chocolate chip和dark chocolate chip cookie的库存量，其中chocolate chip库存量为12，dark chocolate chip cookie库存量为1，具体程序如下所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from datetime import datetime
from sqlalchemy import (MetaData, Table, Column, Integer, Numeric, String,
                        DateTime, ForeignKey, Boolean, create_engine,
                        CheckConstraint)

metadata = MetaData()
cookies = Table('cookies', metadata,
                Column('cookie_id', Integer(), primary_key=True),
                Column('cookie_name', String(50), index=True),
                Column('cookie_recipe_url', String(255)),
                Column('cookie_sku', String(55)),
                Column('quantity', Integer()),
                Column('unit_cost', Numeric(12, 2)),
                CheckConstraint('quantity &amp;gt;= 0', name='quantity_positive')
                )
users = Table('users', metadata,
              Column('user_id', Integer(), primary_key=True),
              Column('username', String(15), nullable=False, unique=True),
              Column('email_address', String(255), nullable=False),
              Column('phone', String(20), nullable=False),
              Column('password', String(25), nullable=False),
              Column('created_on', DateTime(), default=datetime.now),
              Column('updated_on', DateTime(),
                     default=datetime.now, onupdate=datetime.now)
              )
orders = Table('orders', metadata,
               Column('order_id', Integer()),
               Column('user_id', ForeignKey('users.user_id')),
               Column('shipped', Boolean(), default=False)
               )
line_items = Table('line_items', metadata,
                   Column('line_items_id', Integer(), primary_key=True),
                   Column('order_id', ForeignKey('orders.order_id')),
                   Column('cookie_id', ForeignKey('cookies.cookie_id')),
                   Column('quantity', Integer()),
                   Column('extended_cost', Numeric(12, 2))
                   )
engine = create_engine('sqlite:///:memory:')
metadata.create_all(engine)
connection = engine.connect()

from sqlalchemy import select, insert, update
ins = insert(users).values(
    username=&amp;quot;cookiemon&amp;quot;,
    email_address=&amp;quot;mon@cookie.com&amp;quot;,
    phone=&amp;quot;111-111-1111&amp;quot;,
    password=&amp;quot;password&amp;quot;
)
result = connection.execute(ins)
ins = cookies.insert()
inventory_list = [
    {
        'cookie_name': 'chocolate chip',
        'cookie_recipe_url': 'http://some.aweso.me/cookie/recipe.html',
        'cookie_sku': 'CC01',
        'quantity': '12',
        'unit_cost': '0.50'
    },
    {
        'cookie_name': 'dark chocolate chip',
        'cookie_recipe_url': 'http://some.aweso.me/cookie/recipe_dark.html',
        'cookie_sku': 'CC02',
        'quantity': '1',
        'unit_cost': '0.75'
    }
]
result = connection.execute(ins, inventory_list)

s = select([cookies.c.cookie_name, cookies.c.quantity])
connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('chocolate chip', 12), ('dark chocolate chip', 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在，我们再定义两个cookiemon的订单，第一条订单是9块chocolate chip，第二条订单是4块chocolate chip和1块dark chocolate chip cookie。我们用插入语句来实现，具体如下所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ins = insert(orders).values(user_id=1, order_id='1')
result = connection.execute(ins)
ins = insert(line_items)
order_items = [
    {
        'order_id': 1,
        'cookie_id': 1,
        'quantity': 9,
        'extended_cost': 4.50
    }
]
result = connection.execute(ins, order_items)
ins = insert(orders).values(user_id=1, order_id='2')
result = connection.execute(ins)
ins = insert(line_items)
order_items = [
    {
        'order_id': 2,
        'cookie_id': 2,
        'quantity': 1,
        'extended_cost': 0.75
    },
    {
        'order_id': 2,
        'cookie_id': 1,
        'quantity': 4,
        'extended_cost': 2.00
    }
]
result = connection.execute(ins, order_items)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这样我们就有了演示事务的订单数据了，现在我们需要定义一个函数ship_it。这个函数接受一个order_id，然后从库存中去掉对应的购买量，并把订单标记成已发货，shipped=True，如下所示。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sqlalchemy.exc import IntegrityError
def ship_it(order_id):
    try:
        s = select([line_items.c.cookie_id, line_items.c.quantity])
        s = s.where(line_items.c.order_id == order_id)
        cookies_to_ship = connection.execute(s)
        for cookie in cookies_to_ship:
            u = update(cookies).where(cookies.c.cookie_id == cookie.cookie_id)
            u = u.values(quantity=cookies.c.quantity - cookie.quantity)
            result = connection.execute(u)
        u = update(orders).where(orders.c.order_id == order_id)
        u = u.values(shipped=True)
        result = connection.execute(u)
        print(&amp;quot;Shipped order ID: {}&amp;quot;.format(order_id))
    except IntegrityError as error:
        print(error)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;当一条订单发货之后，函数ship_it就会执行动作。让我们对第一条订单执行函数ship_it，然后看看cookies表是否更新了库存。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ship_it(1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Shipped order ID: 1
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('chocolate chip', 3), ('dark chocolate chip', 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;运行正常。但是现在的库存量不能满足第二条订单；但是，在工作节奏很快的仓库中，这些订单应该可以同时处理。现在我们再用函数ship_it处理一下第二条订单。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ship_it(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(sqlite3.IntegrityError) CHECK constraint failed: quantity_positive [SQL: 'UPDATE cookies SET quantity=(cookies.quantity - ?) WHERE cookies.cookie_id = ?'] [parameters: (4, 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('chocolate chip', 3), ('dark chocolate chip', 0)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;程序中库存量先扣减1块dark chocolate chip cookie，可以正常执行，再扣减4块chocolate chip，因为没有足够的chocolate chip库存导致了IntegrityError异常。但是dark chocolate chip的库存扣减了，这不是我们想看到的。我们这里想要发生整个订单，不允许单独让一部分先发货。用前面介绍的异常处理方法可以解决这个问题，但是，事务提供了更好的处理方式。&lt;/p&gt;
&lt;p&gt;事务通过connection对象的begin()方法启动。这样我们就获得一个事务，可以处理后面所有的语句。如果这些语句都成功执行，我们就可以用commit()方法对数据库进行状态确认。如果不完全成功，我们就用rollback()方法让数据库回退到原始状态。下面让我们用事务把函数ship_it改一下。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def ship_it(order_id):
    s = select([line_items.c.cookie_id, line_items.c.quantity])
    s = s.where(line_items.c.order_id == order_id)
    transaction = connection.begin()
    cookies_to_ship = connection.execute(s).fetchall()
    try:
        for cookie in cookies_to_ship:
            u = update(cookies).where(cookies.c.cookie_id == cookie.cookie_id)
            u = u.values(quantity=cookies.c.quantity-cookie.quantity)
            result = connection.execute(u)
        u = update(orders).where(orders.c.order_id == order_id)
        u = u.values(shipped=True)
        result = connection.execute(u)
        print(&amp;quot;Shipped order ID: {}&amp;quot;.format(order_id))
        transaction.commit()
    except IntegrityError as error:
        transaction.rollback()
        print(error)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;现在我们把dark chocolate chip的库存重新补成1。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;u = update(cookies).where(cookies.c.cookie_name == &amp;quot;dark chocolate chip&amp;quot;)
u = u.values(quantity = 1)
result = connection.execute(u)

s = select([cookies.c.cookie_name, cookies.c.quantity])
connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('chocolate chip', 3), ('dark chocolate chip', 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再用新的函数ship_it对第二条订单进行处理。程序不会因为异常而停止，只会打印异常信息。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ship_it(2)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(sqlite3.IntegrityError) CHECK constraint failed: quantity_positive [SQL: 'UPDATE cookies SET quantity=(cookies.quantity - ?) WHERE cookies.cookie_id = ?'] [parameters: (4, 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;再用库存查看语句看看库存的状态，回退到第二条订单处理之前的状态了。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;s = select([cookies.c.cookie_name, cookies.c.quantity])
connection.execute(s).fetchall()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[('chocolate chip', 3), ('dark chocolate chip', 1)]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;这就是事务的作用，短短几行代码就可以让数据库回退到异常发生之前的状态，非常给力吧。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://www.amazon.com/Essential-SQLAlchemy-Mapping-Python-Databases/dp/149191646X"&gt;Essential SQLAlchemy: Mapping Python to Databases&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://muxuezi.github.io/posts/sqlalchemy-introduce.html#SQLAlchemy%E5%AE%89%E8%A3%85%E4%B8%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5"&gt;sqlalchemy introduce&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category><category term="sql"></category></entry><entry><title>Tensorflow简介--06: Logistic regression and KNN analysis for MNIST data</title><link href="/pages/2017/05/27/tensorflowjian-jie-06-logistic-regression-and-knn-analysis-for-mnist-data/" rel="alternate"></link><published>2017-05-27T00:00:00-05:00</published><updated>2017-05-27T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-27:/pages/2017/05/27/tensorflowjian-jie-06-logistic-regression-and-knn-analysis-for-mnist-data/</id><summary type="html">&lt;p&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;MNIST&lt;/a&gt; is used as example in tensorflow. Here are the examples of how to use logistic regression and KNN(K nearest neighbors) to analyze this data.&lt;/p&gt;</summary><content type="html">&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&amp;quot;MNIST_data/&amp;quot;, one_hot=True)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Data Introduction&lt;/h2&gt;
&lt;p&gt;The backgroupnd of MNIST data is introduced in &lt;a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/"&gt;MNIST For ML Beginners&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The MNIST data is split into three parts: 55,000 data points of training data (&lt;code&gt;mnist.train&lt;/code&gt;), 10,000 points of test data (&lt;code&gt;mnist.test&lt;/code&gt;), and 5,000 points of validation data (&lt;code&gt;mnist.validation&lt;/code&gt;). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!&lt;/p&gt;
&lt;p&gt;the training images are &lt;code&gt;mnist.train.images&lt;/code&gt; and the training labels are &lt;code&gt;mnist.train.labels&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers. We can flatten this array into a vector of 28x28 = 784 numbers.&lt;/p&gt;
&lt;p&gt;The result is that &lt;code&gt;mnist.train.images&lt;/code&gt; is a tensor (an n-dimensional array) with a shape of &lt;code&gt;[55000, 784]&lt;/code&gt;. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.&lt;/p&gt;
&lt;p&gt;Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.&lt;/p&gt;
&lt;p&gt;For the purposes of this tutorial, we're going to want our labels as "one-hot vectors". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be &lt;span class="math"&gt;\([0,0,0,1,0,0,0,0,0,0]\)&lt;/span&gt;. Consequently, &lt;code&gt;mnist.train.labels&lt;/code&gt; is a &lt;code&gt;[55000, 10]&lt;/code&gt; array of floats.&lt;/p&gt;
&lt;h2&gt;1. Softmax Regressions (multinomial logistic regression)&lt;/h2&gt;
&lt;p&gt;Here is an introduction of &lt;a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/"&gt;Softmax Regression&lt;/a&gt;. It is a multivariate logistic regression.&lt;/p&gt;
&lt;div class="math"&gt;$$
\begin{aligned}
    &amp;amp; \text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i \\\
    &amp;amp; y = \text{softmax}(\text{evidence})   \\\
    &amp;amp; \text{softmax}(x) = \text{normalize}(\exp(x))   \\\
    &amp;amp; \text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{aligned}
$$&lt;/div&gt;
&lt;h3&gt;Implementation&lt;/h3&gt;
&lt;p&gt;First we will define the input data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;init_param = lambda shape: tf.random_normal(shape, dtype=tf.float32)

with tf.name_scope(&amp;quot;IO&amp;quot;):
    inputs = tf.placeholder(tf.float32, [None, 784], name=&amp;quot;X&amp;quot;)
    targets = tf.placeholder(tf.float32, [None, 10], name=&amp;quot;Yhat&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As explained above, Input data(training data x) is the array with shape of [55000, 784]. The target variable shape is [55000, 10]. &lt;code&gt;placeholder&lt;/code&gt;: a value that we'll input when we ask TensorFlow to run a computation. It will enable you to assemble the graph first without knowing the values needed for computation. In the computation, you can feed the values to placeholders using a dictionary. &lt;code&gt;None&lt;/code&gt; means that a dimension can be of any length.&lt;/p&gt;
&lt;p&gt;Next we will define the variables in the model. &lt;/p&gt;
&lt;p&gt;&lt;code&gt;Variable&lt;/code&gt; is a modifiable tensor that lives in TensorFlow's graph of interacting operations. &lt;code&gt;tf.Variable&lt;/code&gt; is a class, but &lt;code&gt;tf.constant&lt;/code&gt; is an operation. &lt;code&gt;tf.Variable&lt;/code&gt; also hold some operations lile &lt;code&gt;assign&lt;/code&gt;. &lt;code&gt;tf.Variable&lt;/code&gt; must be initialized and can be evaluated to get the values.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with tf.name_scope(&amp;quot;LogReg&amp;quot;):
    W = tf.Variable(init_param([784, 10]), name=&amp;quot;W&amp;quot;)
    B = tf.Variable(init_param([10]))
    logits = tf.matmul(inputs, W) + B
    y = tf.nn.softmax(logits)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we will define the loss function. The purpose it to optimize(minimize) the loss function to find the value of the parameters defined as variables.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with tf.name_scope(&amp;quot;train&amp;quot;):
    learning_rate = tf.Variable(0.5, trainable=False)
    cost_op = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = targets)
    cost_op = tf.reduce_mean(cost_op) 
    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_op)

    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(targets,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Finally is to loop through the epochs to minimize the loss function. The iteration will stop when it reaches the stoping condition or the max of epoch.&lt;/p&gt;
&lt;p&gt;Sometimes the input data will be very big(considering training the pictures with CNN in computer vision / object recognizatio). If dump all data into memory, it will cause the system crashed unless you have lots of ram installed. To avoid this, the best way is to split the input into different batches, then read in and train each batch. Please refer to &lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-02/"&gt;tensorflow--02&lt;/a&gt; for the details how batch and mini-batch works.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tolerance = 1e-4
epochs = 1
last_cost = 0.0
alpha = 0.5
max_epochs = 10
batch_size = 100
costs = []
sess = tf.Session()

with sess.as_default():
    init = tf.global_variables_initializer()
    sess.run(init)
    sess.run(tf.assign(learning_rate, alpha))
    writer = tf.summary.FileWriter(&amp;quot;./tfboard&amp;quot;, sess.graph)
    while True:

        num_batches = int(mnist.train.num_examples/batch_size)
        cost=0
        for _ in range(num_batches):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            tcost, _ = sess.run([cost_op, train_op], feed_dict={inputs: batch_xs, targets: batch_ys})
            cost += tcost
        cost /= num_batches

        tcost = sess.run(cost_op, feed_dict={inputs: mnist.test.images, targets: mnist.test.labels})
        costs.append([cost, tcost])

        if epochs%5==0:
            acc = sess.run(accuracy, feed_dict={inputs: mnist.train.images, targets: mnist.train.labels})
            print (&amp;quot;Epoch: %d - Error: %.4f - Accuracy - %.2f%%&amp;quot; %(epochs, cost, acc))

            if abs(last_cost - cost) &amp;lt; tolerance or epochs &amp;gt; max_epochs:
                break

            last_cost = cost

        epochs += 1

    tcost, taccuracy = sess.run([cost_op, accuracy], feed_dict={inputs: mnist.test.images, targets: mnist.test.labels})
    print (&amp;quot;Test Cost: %.4f - Accuracy: %.2f%% &amp;quot; %(tcost, taccuracy))
    test_plot = sess.run(tf.argmax(y, 1), feed_dict = {inputs: mnist.test.images[:9]})

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch: 5 - Error: 0.4307 - Accuracy - 89.50%
Epoch: 10 - Error: 0.3502 - Accuracy - 91.15%
Epoch: 15 - Error: 0.3164 - Accuracy - 91.91%
Test Cost: 0.3292 - Accuracy: 91.27%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;%matplotlib inline
fig = plt.figure(dpi=100, figsize=(9, 6))
labels = np.argmax(mnist.test.labels[:9], axis = 1)
for i in range(9):
    fig.add_subplot(3, 3, i+1)
    plt.imshow(mnist.test.images[i].reshape(28, 28))
    plt.axis(&amp;quot;off&amp;quot;)
    plt.tight_layout()
    plt.title(&amp;quot;pred: &amp;quot; + str(test_plot[i]) + ', actual: ' + str(labels[i]))
    #frame = plt.gca()
    #frame.axes.get_xaxis().set_visible(False)
    #frame.axes.get_yaxis().set_visible(False)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170527_tf_06_mnist_logistic_knn_01.png"&gt;&lt;/p&gt;
&lt;h2&gt;2. KNN&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm"&gt;KNN&lt;/a&gt; is a non-parametric method for classification and regression. It will measure the distance and group the k nearest data together for classification or regression.&lt;/p&gt;
&lt;p&gt;A common used distance is Euclidean distance given by
&lt;/p&gt;
&lt;div class="math"&gt;$$
d(x, x')=\sqrt{(x_1 − x′_1)^2+(x_2−x′_2)^2+ \cdots +(x_n−x′_n)^2}
$$&lt;/div&gt;
&lt;p&gt;More formally, given a positive integer &lt;span class="math"&gt;\(K\)&lt;/span&gt;, an unseen observation &lt;span class="math"&gt;\(x\)&lt;/span&gt; and a similarity metric &lt;span class="math"&gt;\(d\)&lt;/span&gt;, KNN classifier performs the following two steps:&lt;/p&gt;
&lt;p&gt;1, It runs through the whole dataset computing &lt;span class="math"&gt;\(d\)&lt;/span&gt; between &lt;span class="math"&gt;\(x\)&lt;/span&gt; and each training observation. We’ll call the &lt;span class="math"&gt;\(K\)&lt;/span&gt; points in the training data that are closest to &lt;span class="math"&gt;\(x\)&lt;/span&gt; the set &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt;. Note that &lt;span class="math"&gt;\(K\)&lt;/span&gt; is usually odd to prevent tie situations.&lt;/p&gt;
&lt;p&gt;2, It then estimates the conditional probability for each class, that is, the fraction of points in &lt;span class="math"&gt;\(\mathcal{A}\)&lt;/span&gt; with that given class label. (Note &lt;span class="math"&gt;\(I(x)\)&lt;/span&gt; is the indicator function which evaluates to 1 when the argument &lt;span class="math"&gt;\(x\)&lt;/span&gt; is true and 0 otherwise)&lt;/p&gt;
&lt;div class="math"&gt;$$
P(y = j | X = x) = \frac{1}{K} \sum_{i \in \mathcal{A}} I(y^{(i)} = j)
$$&lt;/div&gt;
&lt;p&gt;Finally, our input &lt;span class="math"&gt;\(x\)&lt;/span&gt; gets assigned to the class with the largest probability.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;sess = tf.Session()


np.random.seed(13)  # set seed for reproducibility
train_size = 2000
test_size = 200
rand_train_indices = np.random.choice(len(mnist.train.images), train_size, replace=False)
rand_test_indices = np.random.choice(len(mnist.test.images), test_size, replace=False)
x_vals_train = mnist.train.images[rand_train_indices]
x_vals_test = mnist.test.images[rand_test_indices]
y_vals_train = mnist.train.labels[rand_train_indices]
y_vals_test = mnist.test.labels[rand_test_indices]

k = 5
batch_size=5

with tf.name_scope(&amp;quot;IO&amp;quot;):
# Placeholders
    x_data_train = tf.placeholder(shape=[None, 784], dtype=tf.float32)
    x_data_test = tf.placeholder(shape=[None, 784], dtype=tf.float32)
    y_target_train = tf.placeholder(shape=[None, 10], dtype=tf.float32)
    y_target_test = tf.placeholder(shape=[None, 10], dtype=tf.float32)

with tf.name_scope(&amp;quot;KNN&amp;quot;):    
    #each train and each test dist
    distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2) 
    # Get min distance index (Nearest neighbor)
    top_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)
    prediction_indices = tf.gather(y_target_train, top_k_indices)
    # Predict the mode category: k nearest nbrs may result in different preds, pick the pred with highest freq
    count_of_predictions = tf.reduce_sum(prediction_indices, axis=1)
    prediction = tf.argmax(count_of_predictions, axis=1)


num_loops = int(np.ceil(len(x_vals_test)/batch_size))
test_output = []
actual_vals = []

with sess.as_default():
    for i in range(num_loops):
        init = tf.global_variables_initializer()
        sess.run(init)
        min_index = i*batch_size
        max_index = min((i+1)*batch_size,len(x_vals_train))
        x_batch = x_vals_test[min_index:max_index] 
        y_batch = y_vals_test[min_index:max_index]
        predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,
                                             y_target_train: y_vals_train, y_target_test: y_batch})
        test_output.extend(predictions)
        actual_vals.extend(np.argmax(y_batch, axis=1))

    accuracy = sum([1./test_size for i in range(test_size) if test_output[i]==actual_vals[i]])
    print(&amp;quot;Accuracy on test data: %.2f%% &amp;quot; %(accuracy))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Accuracy on test data: 0.87%
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;%matplotlib inline
fig = plt.figure(dpi=100, figsize=(9, 6))


for i in range(9):
    fig.add_subplot(3, 3, i+1)
    plt.imshow(x_vals_test[:9][i].reshape(28, 28))
    plt.axis(&amp;quot;off&amp;quot;)
    plt.tight_layout()
    plt.title(&amp;quot;pred: &amp;quot; + str(test_output[:9][i]) + ', actual: ' + str(actual_vals[:9][i]))

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170527_tf_06_mnist_logistic_knn_02.png"&gt;&lt;/p&gt;
&lt;h3&gt;broadcasting&lt;/h3&gt;
&lt;p&gt;the dimension of xtrain and xtest are different. so the substraction cannot be applied directly. &lt;code&gt;tf.expand_dims(x_vals_test,1)&lt;/code&gt; will add one dimension on the data. After expanding the dim, the substraction is fine becasue of broadcasting. It is the same as &lt;code&gt;np.expand_dim&lt;/code&gt;. An example is given below.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
x = np.arange(100).reshape(20, 5)  # 20x5
y = np.arange(20).reshape(4, 5)    # 4x5
x - y # error

print (np.expand_dims(y, 1).shape)        # 4x1x5
print((x  - np.expand_dims(y, 1)).shape)  # 4x20x5
print(np.sum(np.abs((x  - np.expand_dims(y, 1))), axis = 2).shape)    # 4x20
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(4, 1, 5)
(4, 20, 5)
(4, 20)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;tf.reduce_sum(, axis = 2)&lt;/code&gt; will calculate the sum on the given axis = 2. It is similar to &lt;code&gt;np.sum(, axis = 2)&lt;/code&gt;                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(99)
float_formatter = lambda x: &amp;quot;%.2f&amp;quot; % x

s = np.round(np.random.normal(0, 1, 10), 2) 
#s = np.array(map(float_formatter, s))

print s

print(-s[np.argsort(-s)[::-1][:2]])

with tf.Session() as sess:
    sess.run(init)
    print sess.run(tf.nn.top_k(tf.negative(s), k=2))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[-0.14  2.06  0.28  1.33 -0.15 -0.07  0.76  0.83 -0.11 -2.37]

[ 2.37  0.15]

TopKV2(values=array([ 2.37,  0.15]), indices=array([9, 4], dtype=int32))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://docs.scipy.org/doc/numpy-1.12.0/user/basics.broadcasting.html"&gt;Broadcasting&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc"&gt;Array Broadcasting in numpy&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>summary about http.request, GET and POST, form, and route in flask</title><link href="/pages/2017/05/21/summary-about-httprequest-get-and-post-form-and-route-in-flask/" rel="alternate"></link><published>2017-05-21T00:00:00-05:00</published><updated>2017-05-21T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-21:/pages/2017/05/21/summary-about-httprequest-get-and-post-form-and-route-in-flask/</id><summary type="html">&lt;p&gt;summary about http.request, GET and POST, form, and route in flask&lt;/p&gt;</summary><content type="html">&lt;h2&gt;1. Request&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;HTTP&lt;/code&gt; works as a request-response protocol between a client and server.
To access incoming request data, you can use the global &lt;code&gt;request&lt;/code&gt; object. Flask parses incoming request data for you and gives you access to it through that global object. Internally Flask makes sure that you always get the correct data for the active thread if you are in a multithreaded environment. Some examples of how to get &lt;code&gt;request&lt;/code&gt; data: &lt;code&gt;request.form&lt;/code&gt;, &lt;code&gt;request.args.get&lt;/code&gt;, &lt;code&gt;request.files&lt;/code&gt;.&lt;/p&gt;
&lt;h2&gt;2. GET and POST&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;POST&lt;/code&gt;: The browser tells the server that it wants to post some new information to that URL and that the server must ensure the data is stored and only stored once. This is how HTML forms usually transmit data to the server.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;PUT&lt;/code&gt;: Similar to &lt;code&gt;POST&lt;/code&gt; but the server might trigger the store procedure multiple times by overwriting the old values more than once. Now you might be asking why this is useful, but there are some good reasons to do it this way. Consider that the connection is lost during transmission: in this situation a system between the browser and the server might receive the request safely a second time without breaking things. With &lt;code&gt;POST&lt;/code&gt; that would not be possible because it must only be triggered once.&lt;/p&gt;
&lt;h2&gt;3. form&lt;/h2&gt;
&lt;p&gt;The &lt;code&gt;form&lt;/code&gt; tag needs two attributes set:&lt;/p&gt;
&lt;h3&gt;3.1. form action&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;action&lt;/code&gt; is where the data is sent. If action is omitted, it assumed to be the current page (the same URL). In &lt;code&gt;flask&lt;/code&gt;, &lt;code&gt;action&lt;/code&gt; is the URL that the form data is sent to on submit. Generate it with url_for. It can be omitted if the same URL handles showing the form and processing the data.&lt;/p&gt;
&lt;p&gt;Add a view to handle the form data:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;@app.route('/handle_data', methods=['POST'])
def handle_data():
    projectpath = request.form['projectFilepath']
    # your code
    # return a response
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Set the form's action to that view's URL:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;form action=&amp;quot;{{ url_for('handle_data') }}&amp;quot; method=&amp;quot;post&amp;quot;&amp;gt;
    &amp;lt;input type=&amp;quot;text&amp;quot; name=&amp;quot;projectFilepath&amp;quot;&amp;gt;
    &amp;lt;input type=&amp;quot;submit&amp;quot;&amp;gt;
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;3.2. form method&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;method&lt;/code&gt;: Whether to submit the data as a query string (GET) or form data (POST).&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.w3schools.com/tags/ref_httpmethods.asp"&gt;HTTP Methods: GET vs. POST&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/26954122/how-can-i-pass-arguments-into-redirecturl-for-of-flask"&gt;How can I pass arguments into redirect(url_for()) of Flask?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/26217779/how-to-get-the-name-of-a-submitted-form-in-flask"&gt;How to get the name of a submitted form in Flask?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/11556958/sending-data-from-html-form-to-a-python-script-in-flask"&gt;Sending data from HTML form to a Python script in Flask&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://flask.pocoo.org/docs/0.12/quickstart/"&gt;HTTP Methods&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/31935307/what-is-the-difference-between-action-and-target-of-post"&gt;What is the difference between action and target of post?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://flask.pocoo.org/docs/0.12/api/#flask.Request"&gt;Incoming Request Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/10434599/how-to-get-data-received-in-flask-request"&gt;How to get data received in Flask request&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category><category term="flask"></category></entry><entry><title>Online IFRS9/CECL lifetime credit loss calculation engine</title><link href="/pages/2017/05/20/online-ifrs9cecl-lifetime-credit-loss-calculation-engine/" rel="alternate"></link><published>2017-05-20T00:00:00-05:00</published><updated>2017-05-20T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-20:/pages/2017/05/20/online-ifrs9cecl-lifetime-credit-loss-calculation-engine/</id><summary type="html">&lt;p&gt;It is to provide an online IFRS9/CECL lifetime credit loss calculation engine.&lt;/p&gt;</summary><content type="html">&lt;h3&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/IFRS_9"&gt;IFRS 9&lt;/a&gt; is an International Financial Reporting Standard (IFRS) promulgated by the International Accounting Standards Board (IASB). It addresses the accounting for financial instruments. It contains three main topics: classification and measurement of financial instruments, impairment of financial assets and hedge accounting. It will replace the earlier IFRS for financial instruments, IAS 39, when it becomes effective in 2018.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_00.PNG"&gt;   &lt;/p&gt;
&lt;p&gt;Both IFRS9(staging 2) and CECL will requre the lifetime expected credit: loss. Here is to introduce an online lifetime credit loss calculation engine.&lt;/p&gt;
&lt;p&gt;Please click &lt;a href="http://159.203.111.2/ifrs9/"&gt;ifrs9 calc engine&lt;/a&gt; to run with this &lt;a href="https://github.com/songhuiming/git_it/blob/master/share_files/sample_pd_file.csv"&gt;sample data&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;2. Calculaiton Engine usage&lt;/h3&gt;
&lt;p&gt;To do the calculation, we need these items:&lt;/p&gt;
&lt;p&gt;1, loan information: outstanding(EAD), maturity time, interest rate, and the current time of the loan (to link to the correct time predicted pd as explained below). To be simile, this online engine assumes the LGD/Prepay are constant for ech month. So the input loan informaiton should include these columns:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_01.PNG"&gt;   &lt;/p&gt;
&lt;p&gt;2, predicted lifetime pd file (this engine is developed to calculate IFRS9/CECL ECL for any time loan, including historical time for backtesting. so it needs these 4 columns: 3 columns of combined levels(year/quarter, month order to indicate the pd, pd risk rating) and 1 column gives the pd values). The most simple file should have the loan snapshot time and the PD risk rating of the borrower. A sample data is from &lt;a href="https://github.com/songhuiming/git_it/blob/master/share_files/sample_pd_file.csv"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;3. Calculation&lt;/h3&gt;
&lt;h4&gt;3.1. Upload the PD file&lt;/h4&gt;
&lt;p&gt;The predicted PD file is necessary for calculation. Since different kind of PD and different value of PD will be used for test, the option is provided to upload the customed PD values for the test.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_02.PNG"&gt;   &lt;/p&gt;
&lt;h4&gt;3.2. Input the loan information&lt;/h4&gt;
&lt;p&gt;As explained before, the information about the loan is needed. An example of the loan information is given as below:&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_03.PNG"&gt;   &lt;/p&gt;
&lt;h4&gt;3.3. Output&lt;/h4&gt;
&lt;p&gt;The lifetime ECL/PD and 1 year ECL/PD will pop out.
&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_04.PNG"&gt;   &lt;/p&gt;
&lt;p&gt;At the bottom it prints out the monthly cash flow for ECL calculation. It starts from Month 1 with full outstanding balance. Every month there is amoortization payment, interest payment, prepay, credit loss and new drawn amout if there is. Finally there is the month end balance and the corresponding ECL in that month. It will starts from Month 1 until the balance down to 0.  &lt;/p&gt;
&lt;p&gt;A snapshot of the detailed calculation is&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_05.PNG"&gt;   &lt;/p&gt;
&lt;h3&gt;4. Web Development&lt;/h3&gt;
&lt;p&gt;This part will introduce about the development of the online calculation engine website. It is more like a notes for myself to summarize the logic of the development.&lt;/p&gt;
&lt;p&gt;Since the calcualtion is by python,  &lt;a href="http://flask.pocoo.org/"&gt;flask&lt;/a&gt; is selected as the framework. CSS is &lt;a href="http://getbootstrap.com/"&gt;Bootstrap&lt;/a&gt; which is very easy to use to build a moden look webpage. The ouload/submit/forms are all Bootstrap widgets.&lt;/p&gt;
&lt;h4&gt;4.1. html file&lt;/h4&gt;
&lt;p&gt;html file has six functions:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. get the uploaded pd file
2. post the uploaded pd file
3. get the input loan informaiton
4. post the input loan information
5. get the submition for calculation
6. post the submition result
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The full page is a Bootstrap container with two main forms.&lt;/p&gt;
&lt;p&gt;First, The upload widget is fieldset inside the form. A &lt;code&gt;&amp;lt;form&amp;gt;&lt;/code&gt; tag is marked with &lt;code&gt;enctype=multipart/form-data&lt;/code&gt; and an &lt;code&gt;&amp;lt;input type=file&amp;gt;&lt;/code&gt; is placed in that form. If PD file is submitted to upload, the python code will &lt;code&gt;GET&lt;/code&gt; the &lt;code&gt;POST&lt;/code&gt; information and will check if it is csv or not. It will only accept csv file. If there is no PD file or the POST method is not activated in the html page, it will return to the upload page. &lt;code&gt;&amp;lt;form&amp;gt;&lt;/code&gt; tag has the &lt;code&gt;post&lt;/code&gt; method to active the route. The route will call the ptyhon upload file function and upload the file to the server. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;{% if request.args.get('filename') is none %}
    &amp;lt;form action=&amp;quot;&amp;quot; method=post enctype=multipart/form-data&amp;gt;
        &amp;lt;fieldset&amp;gt;
        &amp;lt;legend&amp;gt;Upload PD CSV File&amp;lt;/legend&amp;gt;
    .........
     &amp;lt;/form&amp;gt;
{% else %}
    PD file {{request.args.get('filename')}} has been uploaded; Please fill in the loan information.
{% endif %}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If the file is successfully uploaded and successfully POST, it will redirect to ifrs9 with the filename as the parameter like below&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170525_ifrs9_calc_engine_06.PNG"&gt;   &lt;/p&gt;
&lt;p&gt;The html file and the python upload file function is linked by "request". &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;{{filename}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Second is to input the loan information in the form. It also has the &lt;code&gt;post&lt;/code&gt; method&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;&amp;lt;form action = &amp;quot;&amp;quot; method=&amp;quot;post&amp;quot; role=&amp;quot;form&amp;quot;&amp;gt;
    &amp;lt;fieldset&amp;gt;
    &amp;lt;legend&amp;gt;Input Loan/Borrower Information&amp;lt;/legend&amp;gt;
    .....
&amp;lt;/form&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After all the input forms are filled and submited to calculate, the route will call the flask &lt;strong&gt;ifrs9&lt;/strong&gt; function as introduced below.&lt;/p&gt;
&lt;p&gt;if calculaion data is returned then it will be displayed on the page in div of the centered class.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-html"&gt;{% if data is not none %}
&amp;lt;br&amp;gt;
&amp;lt;div class=&amp;quot;centered&amp;quot;&amp;gt;
{{data | safe}}
&amp;lt;/div&amp;gt;
{% else%}
NONE
{% endif %}
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;4.2. python file&lt;/h4&gt;
&lt;p&gt;There are two piece of python code.&lt;/p&gt;
&lt;p&gt;The first function is to upload the PD file with enabled GET and POST method. It will use &lt;code&gt;@app.route(url, methods=[])&lt;/code&gt; to tell what url will activate the python function. The &lt;code&gt;route()&lt;/code&gt; decorator is used to bind a function to a URL. If it excutes successfully, it will &lt;code&gt;redirect&lt;/code&gt; to the ifrs9 url.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;return redirect(url_for('ifrs9', filename = filename))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The second function is &lt;strong&gt;ifrs9&lt;/strong&gt; which is to get html information and do the calculation.&lt;/p&gt;
&lt;p&gt;After all the input forms are filled and submited to calculate, the route will call the flask &lt;strong&gt;ifrs9&lt;/strong&gt; function: &lt;/p&gt;
&lt;p&gt;1, it will check the filename first, if no file uploaded, it will pop out message to ask for upload the pd file.&lt;/p&gt;
&lt;p&gt;2, if there is uploaded file, then it will request the filled in form values from html&lt;/p&gt;
&lt;p&gt;3, it will call the function to do ECL calculation&lt;/p&gt;
&lt;p&gt;4, it will flash out the calculation result&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;flash('Total IFRS9 Lifetime EL is: ' + str(output[1]) + ', Total IFRS9 Lifetime PD is: ' + str(output[2]))
flash('Total IFRS9 1 year EL is: ' + str(output[3]) + ', Total IFRS9 1 year PD is: ' + str(output[4]))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;5, it will return to &lt;code&gt;render_template&lt;/code&gt; with the output data(the detailed monthly calculation) as parameters to html file for display&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;return render_template(&amp;quot;ifrs9_mockup.html&amp;quot;, data = style + output[0].to_html(index = False, col_space = 5, float_format = lambda x: '%10.2f' % x, classes='centered'))
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;5. Batch Run&lt;/h3&gt;
&lt;p&gt;The above is an example for &lt;strong&gt;one single&lt;/strong&gt; loan calculation. To calculate all the loan at one time, it needs batch run. The basic idea is very simple:&lt;/p&gt;
&lt;p&gt;1, get the input file(loan file, pd file and so on)&lt;/p&gt;
&lt;p&gt;2, loop through each loan, get the corresponding pd/lgd/prepay information. Then do the calculation.&lt;/p&gt;
&lt;p&gt;3, Since step 2 is idpendent for each loan, it can be done parallely.&lt;/p&gt;
&lt;p&gt;4, with &lt;code&gt;multiprocessing&lt;/code&gt; in python for parallel calculation, the calculation speed will increase dramatically &lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://stackoverflow.com/questions/11556958/sending-data-from-html-form-to-a-python-script-in-flask"&gt;Sending data from HTML form to a Python script in Flask&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="python"></category><category term="python"></category></entry><entry><title>Gradient Descent in solving linear regression and logistic regression</title><link href="/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/" rel="alternate"></link><published>2017-05-13T00:00:00-05:00</published><updated>2017-05-13T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-13:/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/</id><summary type="html">&lt;p&gt;use linear regression and logistic regression as examples to show how gradient descent is used to minimize the loss function or to maximize the target function.&lt;/p&gt;</summary><content type="html">&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np, pandas as pd
from matplotlib import pyplot as plt
import math
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Gradient Descent is one of the optimization method by changing the parameters values in the negative gradient direction.&lt;/p&gt;
&lt;p&gt;Let us assume the multi-variable function &lt;span class="math"&gt;\(F(\theta|x)\)&lt;/span&gt; is differenable about &lt;span class="math"&gt;\(\theta\)&lt;/span&gt;. It is first order differentation to &lt;span class="math"&gt;\(x\)&lt;/span&gt; is denoted as &lt;span class="math"&gt;\(\triangledown F(\theta|x)\)&lt;/span&gt;. To find the maximum(mimimum) of &lt;span class="math"&gt;\(F(\theta|x)\)&lt;/span&gt; by iteration, we will let the parameter &lt;span class="math"&gt;\(\theta\)&lt;/span&gt; change its value a little every time. The value changed along the gradient direction will be the fastest way to converge. So, from iteration &lt;span class="math"&gt;\(\theta^{i}\)&lt;/span&gt; to iteration &lt;span class="math"&gt;\(\theta^{i+1}\)&lt;/span&gt; we will let
&lt;/p&gt;
&lt;div class="math"&gt;$$ \theta^{i + 1} = \theta^{i} - \lambda \cdot \triangledown F\left(\theta^{i}|x\right)$$&lt;/div&gt;
&lt;p&gt; where &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is called learning rate.&lt;/p&gt;
&lt;p&gt;In the following two examples will be shown how gradient descent works to find the solution: one is for linear regression(which has closed-form solution) and the other is for logistic regression(which does not have closed-form solution).&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170513_gradient_descent_linreg_animation.gif" title="Linear Regression"&gt;
&lt;img alt="alt text" src="/figures/20170513_gradient_descent_logistic_animation.gif" title="Logistic Regression"&gt;&lt;/p&gt;
&lt;h2&gt;Example 1: Linear regression&lt;/h2&gt;
&lt;p&gt;A simple example of linear regression function can be written as&lt;/p&gt;
&lt;div class="math"&gt;$$y_i = \alpha_0 + \alpha_1 \times x_i + \epsilon_i, ~~~ i = 1 \cdots n$$&lt;/div&gt;
&lt;p&gt;we can write it in vector form as &lt;/p&gt;
&lt;div class="math"&gt;$$ Y = XW + \epsilon $$&lt;/div&gt;
&lt;p&gt;where &lt;/p&gt;
&lt;div class="math"&gt;$$W = [\alpha_0, \alpha_1]'$$&lt;/div&gt;
&lt;p&gt;The target is to minimize the &lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error"&gt;Mean Square Error(mse)&lt;/a&gt; function defined as&lt;/p&gt;
&lt;div class="math"&gt;$$MSE = \frac{1}{n}\sum_{i=1}^{n}\left(y_i - \alpha_0 - \alpha_1 \times x_i \right)^{2} = \frac{1}{n} (Y - XW)'(Y-XW)$$&lt;/div&gt;
&lt;p&gt;We will call its main part  Loss function :&lt;/p&gt;
&lt;div class="math"&gt;$$ L = (Y - XW)'(Y-XW)$$&lt;/div&gt;
&lt;p&gt;As introducted above, the gradient of the loss function to the parameter &lt;span class="math"&gt;\(W\)&lt;/span&gt; is:&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
 \frac{\partial L}{\partial W}  &amp;amp; = \frac{\partial ( (Y - XW)'(Y-XW))}{\partial W}  \\
 &amp;amp; = \frac{\partial (Y'Y - 2W'X'Y  + W'X'XW)}{\partial W} \\
 &amp;amp;= -2X'(Y - XW)
\end{aligned}&lt;/div&gt;
&lt;p&gt;Now we will show how to use this gradient to find the final value of &lt;span class="math"&gt;\(W\)&lt;/span&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;ns = 100
x = np.c_[np.ones(ns), np.linspace(0, 10, ns)]
y = x.dot([[3], [8]]) + np.random.normal(0, 1, ns).reshape(-1, 1)

def loss(w):
    e = y - x.dot(w)
    return e.T.dot(e)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The initial value of &lt;span class="math"&gt;\(W\)&lt;/span&gt; will be &lt;/p&gt;
&lt;div class="math"&gt;$$W_0 = [0, 0]'$$&lt;/div&gt;
&lt;p&gt;For each loop, &lt;/p&gt;
&lt;div class="math"&gt;$$W_{i+1} = W_i - \frac{\partial L}{\partial W} \times \lambda$$&lt;/div&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;w0 = np.array([0.0, 0.0]).reshape(2, 1)
learning_rate = 0.00001

i = 0
w = w0
while True:
    l1 = loss(w)
    dldw = -2 * x.T.dot(y - x.dot(w))
    w_new = w - dldw * learning_rate
    l2 = loss(w_new)
    w = w_new
    i += 1
    if i &amp;gt; 10000 or abs(l1 - l2) &amp;lt; 0.00001:
        break

print(&amp;quot;The final value from gradient descent is alpha_0 = %.2f, alpha_1 = %.2f&amp;quot; %(w[0], w[1]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The final value from gradient descent is alpha_0 = 2.41, alpha_1 = 8.09
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170513_gradient_descent_linreg_animation.gif" title="Linear Regression"&gt;&lt;/p&gt;
&lt;p&gt;For linear regression, we have the analytical solution(or closed-form solution) in the form:&lt;/p&gt;
&lt;div class="math"&gt;$$W = (X'X)^{-1}X'Y$$&lt;/div&gt;
&lt;p&gt;So the analytical solution can be calculated directly in python. The analytical solution is: constant = 2.73 and the slope is 8.02.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;regres = np.linalg.inv((x.T.dot(x))).dot(x.T.dot(y))
print(&amp;quot;The regression result is alpha_0 = {0:.2f}, alpha_1 = {1:.2f}&amp;quot;.format(round(regres[0], 2), round(regres[1], 2)))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The regression result is alpha_0 = 2.43, alpha_1 = 8.09
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;p&gt;Random variable &lt;span class="math"&gt;\(Y\)&lt;/span&gt; has &lt;a href="https://en.wikipedia.org/wiki/Bernoulli_distribution"&gt;Bernoulli distribution&lt;/a&gt;:
&lt;/p&gt;
&lt;div class="math"&gt;$$
    Y=\left\{
                \begin{array}{ll}
                  1, \mbox{with probability} ~p\\
                  0, \mbox{with probability} ~1 - p\\
                \end{array}
              \right.
 $$&lt;/div&gt;
&lt;p&gt;Here &lt;span class="math"&gt;\(p\)&lt;/span&gt; is the function of &lt;span class="math"&gt;\(X\)&lt;/span&gt; given parameters &lt;span class="math"&gt;\(W\)&lt;/span&gt;: &lt;span class="math"&gt;\(p = p(X|W)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The logit function is&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
       &amp;amp; ~~~~ \log \frac{p(X|W)}{1 - p(X|W)} = XW + \epsilon   \\\\
       &amp;amp; ==&amp;gt; p(X|W) = \frac{\exp(XW)}{ 1 + \exp(XW)}   \triangleq  h(W, X)
\end{aligned}&lt;/div&gt;
&lt;p&gt;here both &lt;span class="math"&gt;\(X\)&lt;/span&gt; and &lt;span class="math"&gt;\(W\)&lt;/span&gt; are vectors(shall be written as &lt;span class="math"&gt;\(\vec{X}\)&lt;/span&gt; and &lt;span class="math"&gt;\(\vec{W}\)&lt;/span&gt;)&lt;/p&gt;
&lt;p&gt;So the likelihood funciton can be written as &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    L(X, W) = \prod_{i=1}^{n} p\left(X_i|W\right)^{y_i} \left(1 - p\left(X_i|W\right)\right)^{(1 - y_i)}  
\end{aligned}&lt;/div&gt;
&lt;p&gt;To max the likelihood function is the same as maximizing the log-likelihood funtion. The log-likelihood function is&lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    LL(X, W) &amp;amp;= \log(L(X, W))  \\\\
             &amp;amp;= \log\left(\prod_{i=1}^{n} p(X_i|W)^{y_i} (1 - p(X_i|W))^{(1 - y_i)}\right)  \\\\
             &amp;amp;= \sum_{i=1}^n \left[ y_i \log(p(X_i|W)) + (1 - y_i) \log(1 - p(X_i|W)) \right]
\end{aligned}&lt;/div&gt;
&lt;p&gt;from this we can get the gradient of &lt;span class="math"&gt;\(W\)&lt;/span&gt; is &lt;/p&gt;
&lt;div class="math"&gt;\begin{aligned}
    \frac{\partial LL(X, W)}{\partial W} = \sum_{i=1}^n \left( h(W, X_i) - y_i \right) X_i 
\end{aligned}&lt;/div&gt;
&lt;p&gt;where
&lt;/p&gt;
&lt;div class="math"&gt;$$
 h(W, X_i) = \frac{\exp(X_i W)}{ 1 + \exp(X_i W)}
$$&lt;/div&gt;
&lt;p&gt;Next an example is shown how to use gradient descent to find the solution of &lt;span class="math"&gt;\(W\)&lt;/span&gt; to maximize the likelihood function.The link function is of the form:&lt;/p&gt;
&lt;div class="math"&gt;$$\log \frac{p}{1-p} = \beta_0 + \beta_1 x_1 + \beta_2 x_2$$&lt;/div&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;w = np.array([1, 2, 5]).reshape(3, 1)
np.random.seed(99)
x = np.c_[np.ones(100).reshape(100, 1), np.random.normal(0, 2, 200).reshape(100, 2)]
s = x.dot(w) + np.random.normal(0, 1, 100).reshape(100, 1) 
p = np.exp(s) / (1 + np.exp(s)) 
y = np.random.binomial(1, p).reshape(100, 1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def sgd(x, y, w):
    expval = np.exp(x[:, :].dot(w))
    return np.sum([expval / (1 + expval) - y] * x, axis = 1).reshape(3, -1)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;w0 = np.zeros(3).reshape(3, 1)
learning_rate = 0.01

i = 0
w = w0
while True:
    dlldw = sgd(x, y, w)
    w_new = w - dlldw * learning_rate
    i += 1
    if i &amp;gt; 1000 or sum((w - w_new)**2)&amp;lt;0.00001:
        break
    w = w_new

print(&amp;quot;The fitting result from gradient descent is beta0 = %.2f, beta1 = %.2f, beta2 = %.2f&amp;quot; %(w[0], w[1], w[2]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The fitting result from gradient descent is beta0 = 1.09, beta1 = 1.82, beta2 = 5.20
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.scatter(x[y.reshape(100, ) ==1, 1], x[y.reshape(100, ) ==1, 2], color = &amp;quot;#aa66cc&amp;quot;, label = &amp;quot;Y = 1&amp;quot;)
plt.scatter(x[y.reshape(100, ) ==0, 1], x[y.reshape(100, ) ==0, 2], color = &amp;quot;#00aabb&amp;quot;, label = &amp;quot;Y = 0&amp;quot;)
plt.plot(x[:, 1], -0.2 - 0.4 * x[:, 1], color = &amp;quot;#4285F4&amp;quot;, label = &amp;quot;True Separation Line&amp;quot;)
plt.plot(x[:, 1], -x[:, :2].dot(w[:2, 0] / w[2, 0]), color = &amp;quot;#00C851&amp;quot;, label = &amp;quot;Fitted Separation Line&amp;quot;)
plt.xlabel(&amp;quot;X1&amp;quot;, color = '#42a5f5', size = &amp;quot;18&amp;quot;)
h = plt.ylabel('X2', color = '#42a5f5', size = &amp;quot;18&amp;quot;)
h.set_rotation(0)
plt.title(&amp;quot;Logistic Regression Separation Line&amp;quot;, color = &amp;quot;#42a5f5&amp;quot;, size = &amp;quot;18&amp;quot;)
plt.legend(loc=2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170513_gradient_descent_logistic_01.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/20170513_gradient_descent_logistic_animation.gif" title="Logistic Regression"&gt;&lt;/p&gt;
&lt;p&gt;The same as linear regression, we can use &lt;code&gt;sklearn&lt;/code&gt;(it also use gradient method to solve) or &lt;code&gt;statsmodels&lt;/code&gt;(it is the same as traditional method like R or SAS did) to get the regression result for this example:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn.linear_model import LogisticRegression
logfit = LogisticRegression()
logfit.fit(x, y)
sklearn_w = logfit.coef_[0]
print(&amp;quot;The fitting result from gradient descent is beta0 = %.2f, beta1 = %.2f, beta2 = %.2f&amp;quot; %(sklearn_w[0], sklearn_w[1], sklearn_w[2]))

# import statsmodels.formula.api as smf
# import statsmodels.api as sm
# import pandas as pd
# mydata = pd.DataFrame(np.c_[x, y], columns = [&amp;quot;const&amp;quot;, &amp;quot;x1&amp;quot;, &amp;quot;x2&amp;quot;, &amp;quot;y&amp;quot;])
# mod1 = smf.glm(formula = &amp;quot;y ~ x1 + x2&amp;quot;, data=mydata, family=sm.families.Binomial()).fit()
# print (mod1.params)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The fitting result from gradient descent is beta0 = 0.27, beta1 = 0.95, beta2 = 2.72
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href="https://mdbootstrap.com/css/colors/"&gt;bootstrap color hex value&lt;/a&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="deep learning"></category><category term="data mining"></category></entry><entry><title>Tensorflow简介--05: Multivariate Regression with Stochastic Gradient Descent</title><link href="/pages/2017/05/06/tensorflowjian-jie-05-multivariate-regression-with-stochastic-gradient-descent/" rel="alternate"></link><published>2017-05-06T00:00:00-05:00</published><updated>2017-05-06T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-06:/pages/2017/05/06/tensorflowjian-jie-05-multivariate-regression-with-stochastic-gradient-descent/</id><summary type="html">&lt;p&gt;在&lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-03/"&gt;3&lt;/a&gt;中我们介绍了怎么使用&lt;code&gt;TensorFlow&lt;/code&gt;做多元回归。本文使用一个图文结合的具体例子详细说明怎么做。下一篇我们将会沿用&lt;a href="http://songhuiming.github.io/pages/2017/03/05/tensorflowjian-jie-04/"&gt;4&lt;/a&gt;中所说的办法介绍怎么做logistic regression.做线性回归和logistic regression不是&lt;code&gt;tf&lt;/code&gt;的特长。&lt;code&gt;tf&lt;/code&gt;的特长是用各种神经网络做深度学习。我们将会在后几篇陆续介绍。&lt;/p&gt;</summary><content type="html">&lt;p&gt;尽管&lt;code&gt;TenforFlow(tf)&lt;/code&gt;最主要的功能是进行深度学习,包括&lt;a href="https://en.wikipedia.org/wiki/Convolutional_neural_network"&gt;卷积神经网络(CNN)&lt;/a&gt;,&lt;a href="https://en.wikipedia.org/wiki/Recurrent_neural_network"&gt;循环神经网络(RNN)&lt;/a&gt;和&lt;a href="https://en.wikipedia.org/wiki/Deep_learning"&gt;深度神经网络(DNN)&lt;/a&gt;等等。但是作为一个深度学习框架，其也提供了进行一般统计学习和优化的工具。前面(&lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-01/"&gt;1&lt;/a&gt;, &lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-02/"&gt;2&lt;/a&gt;, &lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-03/"&gt;3&lt;/a&gt;, &lt;a href="http://songhuiming.github.io/pages/2017/03/05/tensorflowjian-jie-04/"&gt;4&lt;/a&gt;)我们已经介绍了&lt;code&gt;tf&lt;/code&gt;的基本设置和各个命令的含义。下面我们使用一个例子来展示怎么使用&lt;code&gt;tf&lt;/code&gt;来进行多元回归拟合数据。具体的说，用一个例子来说明怎么借助&lt;code&gt;tf&lt;/code&gt;的&lt;code&gt;Stochastic Gradient Descent&lt;/code&gt;做多项式回归。下面的动图展示了拟合函数的参数(回归系数)在&lt;code&gt;tf&lt;/code&gt;经过多次迭代以后，拟合曲线渐渐逼近真实值的过程。&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/tf_multivar_regression.gif" title="Tensorflow fit result on different Epoch"&gt;&lt;/p&gt;
&lt;p&gt;在使用&lt;code&gt;tf&lt;/code&gt;的时候，我们需要定义下面几个东西:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. 输入和输出数据
2. 定义参数
3. 定义损失函数，以及使用什么办法来优化(比如下面使用随机梯度下降的办法)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;首先我们需要调用将要使用的一些packages:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import math,sys,os,numpy as np, pandas as pd, tensorflow as tf
from matplotlib import pyplot as plt, rcParams, animation, rc, style
from __future__ import print_function, division
from ipywidgets import interact, interactive, fixed
from ipywidgets.widgets import *

%precision 4
np.set_printoptions(precision=4, linewidth=100)

style.use('ggplot')
%matplotlib inline
plt.rcParams['figure.figsize'] = (12, 9)
rc('animation', html='html5')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;我们的目的是使用&lt;code&gt;tf&lt;/code&gt;来进行多元回归(这个例子是多项式)。首先我们需要模拟产生一个样本数据：&lt;span class="math"&gt;\(X\)&lt;/span&gt;是0到9上的均匀分布的点，&lt;span class="math"&gt;\(Y\)&lt;/span&gt;是&lt;span class="math"&gt;\(X\)&lt;/span&gt;的多项式函数， &lt;span class="math"&gt;\(y = -50 + 30 \cdot x - 12 \cdot x^2 + x^3 + \epsilon\)&lt;/span&gt;. &lt;span class="math"&gt;\(X\)&lt;/span&gt;和&lt;span class="math"&gt;\(Y\)&lt;/span&gt;的关系见下图。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(99)
x = np.linspace(0, 9, 100)[:, np.newaxis]
y =  np.polyval([1, -12, 30, -50], x) + np.random.randn(100, 1)

plt.plot(x, y)
plt.xlabel('X')
h = plt.ylabel('Y')
h.set_rotation(0)
plt.title(&amp;quot;$y=x^3 - 12 x^2 + 30 x - 50 + \epsilon$&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170505_tf_multivar_regression_01.png"&gt;&lt;/p&gt;
&lt;p&gt;做多项式回归的时候选择基函数为&lt;span class="math"&gt;\([1, \space x, \space x^2, \space x^3 ]\)&lt;/span&gt;,python中可以使用&lt;code&gt;np.power(x, range(4))&lt;/code&gt;得到。因为随机梯度下降的时候梯度跟&lt;span class="math"&gt;\(X\)&lt;/span&gt;的值有关系，所以我们需要对&lt;span class="math"&gt;\(X\)&lt;/span&gt;的每一列进行标准化让所有的&lt;span class="math"&gt;\(X\)&lt;/span&gt;的值处在同样的尺度下。&lt;/p&gt;
&lt;p&gt;数据标准化的一个办法是所有的值(我们的例子中是每一列数据)减去均值，除以方差 &lt;/p&gt;
&lt;div class="math"&gt;$$ X^{normalized} = \frac{X - \mu(X)}{\sigma(X)}$$&lt;/div&gt;
&lt;p&gt; &lt;code&gt;numpy&lt;/code&gt;对应的code是&lt;code&gt;(x - x.mean(axis = 0)) / x.std(axis = 0)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;另一个办法是每一列数据除以最大值。因为我们的&lt;span class="math"&gt;\(X\)&lt;/span&gt;都大于0，所以这样做的结果就是把所有的&lt;span class="math"&gt;\(X\)&lt;/span&gt;变换到&lt;span class="math"&gt;\([0, 1]\)&lt;/span&gt;区间上。&lt;code&gt;numpy&lt;/code&gt;对应的code是&lt;code&gt;x / np.max(x, axis = 0)&lt;/code&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pol_order = 4
x = np.power(x, range(pol_order))

def normalized_(x):
    return (x - x.mean(axis = 0)) / x.std(axis = 0)

def normalized(x):
    l = len(x)
    return np.concatenate((np.ones(l).reshape(l, 1), normalized_(x[:, 1:])), axis = 1)

x = normalized(x)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;normalized过的&lt;span class="math"&gt;\(X\)&lt;/span&gt;和&lt;span class="math"&gt;\(Y\)&lt;/span&gt;的关系为&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170505_tf_multivar_regression_02.png"&gt;&lt;/p&gt;
&lt;p&gt;把上面定义好的data分为training和test两部分：&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(99)
order = np.random.permutation(len(x))
portion = 20
test_x = x[order[:portion]]
test_y = y[order[:portion]]
train_x = x[order[portion:]]
train_y = y[order[portion:]]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;下面开始定义模型。&lt;code&gt;tf&lt;/code&gt;通过数据流程图来定义模型，然后编译执行。&lt;/p&gt;
&lt;p&gt;我们从定义输入(实际值)输出开始。如前所述，这是通过占位符(placehold)来定义.placehold有两个必须的参数，第一个是数据类型，通常为浮点类型(float).第二个是输入输出的数据的维度。假如我们的输入X是一个(m, n)维的矩阵，我们可以把第二个参数定义为[m, n]。我们用[&lt;code&gt;None&lt;/code&gt; n]来表示待定的维度，这样我们可以使用&lt;code&gt;batch&lt;/code&gt;来部分的输入数据，而不是全部m个数据。这样做的原因是如果m很大，机器的内存可能处理不了。这时候把数据分为很多batch，每次输入部分的数据(一个batch)来拟合，保证内存不会溢出。我们还可以添加第三个参数&lt;code&gt;name&lt;/code&gt;，用来在&lt;code&gt;TensorBoard&lt;/code&gt;中展示名字。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with tf.name_scope(&amp;quot;Input/Output&amp;quot;):
    inputs = tf.placeholder(tf.float32, [None, pol_order], name=&amp;quot;X&amp;quot;)
    outputs = tf.placeholder(tf.float32, [None, 1], name=&amp;quot;Yhat&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;接下来定义模型表达式。模型的参数变量通过&lt;code&gt;tf.Variable&lt;/code&gt;来定义，跟&lt;code&gt;tf.placehold&lt;/code&gt;的定义很相似，不同的地方在于&lt;code&gt;tf.Variable&lt;/code&gt;的值会随着拟合而发生变化。线性模型的表达式为&lt;span class="math"&gt;\(Y=X \times W\)&lt;/span&gt;。所以参数W的维度与X的列数相等。矩阵X和W相乘由&lt;code&gt;tf.matmul&lt;/code&gt;定义。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with tf.name_scope(&amp;quot;Model&amp;quot;):
    W = tf.Variable(tf.zeros([pol_order, 1], dtype=tf.float32), name=&amp;quot;W&amp;quot;)
    y = tf.matmul(inputs, W)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最后定义损失函数和它的最优化函数。通常对线性回归模型我们都是要最小化均方误差。损失函数定义为平方损失&lt;span class="math"&gt;\(L = \sum_{i=1}^{n}(y_i - f(x_{i}|W)) ^2\)&lt;/span&gt;。 把回归函数写成矩阵的形式为&lt;span class="math"&gt;\(Y = X \cdot W\)&lt;/span&gt;，那么损失函数也可以表达为矩阵相乘&lt;span class="math"&gt;\(L = (Y - XW)'(Y-XW)\)&lt;/span&gt;. 要最小化损失函数，我们首先求解出梯度为&lt;/p&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
 \frac{\partial L}{\partial W}  &amp;amp; = \frac{\partial ( (Y - XW)'(Y-XW))}{\partial W}  \\\\
 &amp;amp; = \frac{\partial (Y'Y - 2W'X'Y  + W'X'XW)}{\partial W} \\\\
 &amp;amp;= -2X'(Y - XW)
\end{aligned}$$&lt;/div&gt;
&lt;p&gt;&lt;code&gt;tf&lt;/code&gt;(包括&lt;code&gt;sklearn&lt;/code&gt;)都是通过数值办法来最小化损失函数的，而不是通过解析解，尽管我们知道对线性模型回归参数&lt;span class="math"&gt;\(W\)&lt;/span&gt;参在解析解&lt;span class="math"&gt;\(\hat{W} = (X'X)^{-1}X'Y\)&lt;/span&gt;. 具体就是让W每次沿着它的梯度方向变化一点点(learning rate),然后反复迭代，直到达到我们的收敛条件或者停止条件。在第一篇中我们介绍过梯度下降的原理：假如损失函数是一座山，我们现在站在山坡上朝四周看看，然后找一个方向能让我们走一点点路但是下降的最快，那个方向就是梯度的方向。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;tf&lt;/code&gt;提供了很多最优化(不同的梯度下降)的办法，参见&lt;a href="https://www.tensorflow.org/api_guides/python/train#Optimizers"&gt;Optimizers&lt;/a&gt;.对某些数据使用梯度下降的时候会出现鞍点，导致&lt;code&gt;GradientDescentOptimizer&lt;/code&gt;可能会找不到最优点或者只是local的最优点。这时应该尝试别的最优化的办法。在我们的例子中，&lt;code&gt;GradientDescentOptimizer&lt;/code&gt;找不到最优解， 但是&lt;code&gt;AdamOptimizer&lt;/code&gt;可以找到最优解。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;learning rate&lt;/code&gt;的参数设置为&lt;code&gt;trainable=False&lt;/code&gt;表示每次迭代的时候&lt;code&gt;learning rate&lt;/code&gt;本身不变化。&lt;code&gt;learning rate&lt;/code&gt;太小会导致迭代步骤太多，计算所需要的时间就会很长。&lt;code&gt;learning rate&lt;/code&gt;太大也会有问题：在你将要靠近最优点的时候，因为下一步跨的太大，导致跨过了最优点(overshoot)。比较好的办法是：刚开始迭代的时候&lt;code&gt;learning rate&lt;/code&gt;可以大一些，然后随着渐渐靠近最优点，&lt;code&gt;learning rate&lt;/code&gt;应该渐渐减小。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;with tf.name_scope(&amp;quot;SGD&amp;quot;):
    learning_rate = tf.Variable(0.5, trainable=False)
    cost_op = tf.reduce_mean(tf.pow(y-outputs, 2))
    train_op = tf.train.AdamOptimizer(learning_rate).minimize(cost_op)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;到此为止我们已经准备好了计算流程图，下面我们开始定义对话(&lt;code&gt;session&lt;/code&gt;)来执行数据流程图。数据流程图中的每个节点(&lt;code&gt;operation&lt;/code&gt; or &lt;code&gt;op&lt;/code&gt;)在计算过程当中会获得&lt;code&gt;tensor&lt;/code&gt;，每个&lt;code&gt;tensor&lt;/code&gt;是一个多为数组。对话将图的&lt;code&gt;op&lt;/code&gt;分到cpu或者gpu进行计算。计算完后以&lt;code&gt;tensor&lt;/code&gt;返回，在python中其类型为&lt;code&gt;np.ndarray&lt;/code&gt;。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. `sess.as_default()`定义了默认对话, `sess.run()` 和 `sess.eval()` 只能在这个对话下面执行
2. `sess.run()`如果执行`operation`的话(比如`cost_op`)，需要提供输入值`feed_dict={inputs: train_x, outputs: train_y}`
3. 对tensor来说，`tensor.eval()` == `sess.run(tensor)`
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tolerance = 1e-5

epochs = 1
last_cost = 0
alpha = 0.3
max_epochs = 50000

sess = tf.Session() # default session

# make default session, run() and eval() should be executed in this session
with sess.as_default(): 
    init = tf.initialize_all_variables()
    sess.run(init)
    sess.run(tf.assign(learning_rate, alpha))
    while True:
        sess.run(train_op, feed_dict={inputs: train_x, outputs: train_y})

        if epochs%100==0:
            cost = sess.run(cost_op, feed_dict={inputs: train_x, outputs: train_y})
            print (&amp;quot;Epoch: %d - Error: %.4f&amp;quot; %(epochs, cost))

            if abs(last_cost - cost) &amp;lt; tolerance or epochs &amp;gt; max_epochs:
                print (&amp;quot;Stop Criteria Reached. Break!&amp;quot;)
                break
            last_cost = cost

        epochs += 1

    w = W.eval()  # fetch the value of tensor
    print (&amp;quot;w =&amp;quot;, w)
    print (&amp;quot;Test Cost =&amp;quot;, sess.run(cost_op, feed_dict={inputs: test_x, outputs: test_y}))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Epoch: 100 - Error: 1110.4116
Epoch: 200 - Error: 321.6223
Epoch: 300 - Error: 167.4508
Epoch: 400 - Error: 144.2539
Epoch: 500 - Error: 135.3392
Epoch: 600 - Error: 126.8727
Epoch: 700 - Error: 118.1265
Epoch: 800 - Error: 109.2254
Epoch: 900 - Error: 100.3067
Epoch: 1000 - Error: 91.4902
Epoch: 1100 - Error: 82.8799
Epoch: 1200 - Error: 74.5638
Epoch: 1300 - Error: 66.6154
Epoch: 1400 - Error: 59.0937
Epoch: 1500 - Error: 52.0441
Epoch: 1600 - Error: 45.4990
Epoch: 1700 - Error: 39.4793
Epoch: 1800 - Error: 33.9944
Epoch: 1900 - Error: 29.0440
Epoch: 2000 - Error: 24.6189
Epoch: 2100 - Error: 20.7023
Epoch: 2200 - Error: 17.2708
Epoch: 2300 - Error: 14.2958
Epoch: 2400 - Error: 11.7446
Epoch: 2500 - Error: 9.5815
Epoch: 2600 - Error: 7.7693
Epoch: 2700 - Error: 6.2698
Epoch: 2800 - Error: 5.0450
Epoch: 2900 - Error: 4.0584
Epoch: 3000 - Error: 3.2750
Epoch: 3100 - Error: 2.6623
Epoch: 3200 - Error: 2.1907
Epoch: 3300 - Error: 1.8338
Epoch: 3400 - Error: 1.5684
Epoch: 3500 - Error: 1.3748
Epoch: 3600 - Error: 1.2362
Epoch: 3700 - Error: 1.1391
Epoch: 3800 - Error: 1.0725
Epoch: 3900 - Error: 1.0279
Epoch: 4000 - Error: 0.9986
Epoch: 4100 - Error: 0.9800
Epoch: 4200 - Error: 0.9685
Epoch: 4300 - Error: 0.9615
Epoch: 4400 - Error: 0.9574
Epoch: 4500 - Error: 0.9551
Epoch: 4600 - Error: 0.9538
Epoch: 4700 - Error: 0.9531
Epoch: 4800 - Error: 0.9528
Epoch: 4900 - Error: 0.9526
Epoch: 5000 - Error: 0.9525
Epoch: 5100 - Error: 0.9525
Epoch: 5200 - Error: 0.9525
Epoch: 5300 - Error: 0.9525
Stop Criteria Reached. Break!
w = [[ -56.3677]
 [  78.7995]
 [-293.1844]
 [ 209.9559]]
Test Cost = 1.12171
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;最终，损失函数在training上面收敛到0.9525，循环达到了停止的条件。这时候损失函数在test上的值为1.12177&lt;/p&gt;
&lt;p&gt;最后的拟合函数为 &lt;/p&gt;
&lt;div class="math"&gt;$$ y = -56.3677 + 78.8013 \cdot x - 293.1888 \cdot x^2  + 209.9586 \cdot x^3$$&lt;/div&gt;
&lt;p&gt;用于拟合的数据的前5行为&lt;/p&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
      &lt;th&gt;x_0&lt;/th&gt;
      &lt;th&gt;x_1&lt;/th&gt;
      &lt;th&gt;x_2&lt;/th&gt;
      &lt;th&gt;x_3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;-32.780156&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;1.645531&lt;/td&gt;
      &lt;td&gt;2.074123&lt;/td&gt;
      &lt;td&gt;2.392560&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-40.784224&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;-1.576245&lt;/td&gt;
      &lt;td&gt;-1.106391&lt;/td&gt;
      &lt;td&gt;-0.877834&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-59.082880&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;-0.155892&lt;/td&gt;
      &lt;td&gt;-0.426134&lt;/td&gt;
      &lt;td&gt;-0.551511&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;-73.715893&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.155892&lt;/td&gt;
      &lt;td&gt;-0.124437&lt;/td&gt;
      &lt;td&gt;-0.313780&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;-32.524330&lt;/td&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;-0.848747&lt;/td&gt;
      &lt;td&gt;-0.900180&lt;/td&gt;
      &lt;td&gt;-0.822070&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;拟合函数的图像为&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170505_tf_multivar_regression_03.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="alt text" src="/figures/tf_multivar_regression.gif" title="Tensorflow fit result on different Epoch"&gt;&lt;/p&gt;
&lt;p&gt;最后说明的是，我们用多元线性回归的例子来演示怎么使用&lt;code&gt;tf&lt;/code&gt;. 对于这个问题，python的机器学习package &lt;code&gt;sklearn&lt;/code&gt; 可以提供更快捷的结果。&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from sklearn import linear_model
from sklearn.metrics import mean_squared_error

reg = linear_model.LinearRegression()
reg.fit(train_x, train_y)
print (&amp;quot;The regression coefficient is %s&amp;quot; %(reg.coef_))

print (&amp;quot;The MSE from sklearn is: %s&amp;quot; %(mean_squared_error(test_y, reg.predict(test_x))))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;The regression coefficient is [[   0.       78.8177 -293.2297  209.9838]]
The MSE from sklearn is: 1.12222708205
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/get_started/mnist/mechanics"&gt;TensorFlow Mechanics 101&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.tensorflow.org/versions/r0.11/api_docs/python/client/session_management"&gt;Session management&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/33610685/in-tensorflow-what-is-the-difference-between-session-run-and-tensor-eval"&gt;what is the difference between Session.run() and Tensor.eval()&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>test math formula in github pages</title><link href="/pages/2017/05/04/test-math-formula-in-github-pages/" rel="alternate"></link><published>2017-05-04T00:00:00-05:00</published><updated>2017-05-04T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-05-04:/pages/2017/05/04/test-math-formula-in-github-pages/</id><summary type="html">&lt;p&gt;A quick way to enable Latex math formula in github pages&lt;/p&gt;</summary><content type="html">&lt;h3&gt;update 20170513:&lt;/h3&gt;
&lt;p&gt;If use &lt;code&gt;pelican&lt;/code&gt;, the easist way is to include the plug-in called &lt;a href="https://github.com/getpelican/pelican-plugins/tree/master/render_math"&gt;Math Render Plugin For Pelican&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It use &lt;code&gt;MathJax&lt;/code&gt; to give pelican the ability to render mathematics. It supports both markdown and reStructuredText.&lt;/p&gt;
&lt;p&gt;It also supports &lt;a href="http://docs.mathjax.org/en/latest/tex.html#color"&gt;color&lt;/a&gt; and &lt;a href="http://docs.mathjax.org/en/latest/tex.html#mhchem"&gt;mhchem&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;It only enables math formula. But it did not support complicated things like equation number and so on.&lt;/p&gt;
&lt;h3&gt;method 1. copy the following in the md file&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;method 2. copy that into the &lt;code&gt;header.html&lt;/code&gt; file&lt;/h3&gt;
&lt;p&gt;For me, the &lt;code&gt;header.html&lt;/code&gt; is in
&lt;code&gt;blog/pelican-themes/pelican-octopress-theme/templates/_includes/header.html&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Then input the formula inside &lt;code&gt;$$ ... $$&lt;/code&gt; will create formula centered in new line&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$$\frac{\partial f(y)}{\partial x} = \frac{\partial f}{\partial y} \times \frac{\partial y}{\partial x}$$
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;will generate &lt;/p&gt;
&lt;div class="math"&gt;$$\frac{\partial f(y)}{\partial x} = \frac{\partial f}{\partial y} \times \frac{\partial y}{\partial x}$$&lt;/div&gt;
&lt;p&gt;And &lt;code&gt;\\( \frac{1}{n} \\)&lt;/code&gt; will create formula inline: \( \frac{1}{n} \).&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Let's test some inline math &lt;span class="math"&gt;\(x\)&lt;/span&gt;, &lt;span class="math"&gt;\(y\)&lt;/span&gt;, &lt;span class="math"&gt;\(x_1\)&lt;/span&gt;, &lt;span class="math"&gt;\(y_1\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Now a inline math with special character: &lt;span class="math"&gt;\(|\psi\rangle\)&lt;/span&gt;, &lt;span class="math"&gt;\(x'\)&lt;/span&gt;, &lt;span class="math"&gt;\(x^\*\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Test a display math:
&lt;/p&gt;
&lt;div class="math"&gt;$$
   |\psi_1\rangle = a|0\rangle + b|1\rangle
$$&lt;/div&gt;
&lt;p&gt;Test a display math with equation number:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{equation}
   |\psi_1\rangle = a|0\rangle + b|1\rangle
\end{equation}&lt;/div&gt;
&lt;p&gt;Test a display math with equation number:
&lt;/p&gt;
&lt;div class="math"&gt;$$
  \begin{align}
    |\psi_1\rangle &amp;amp;= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &amp;amp;= c|0\rangle + d|1\rangle
  \end{align}
$$&lt;/div&gt;
&lt;p&gt;And test a display math without equaltion number:
&lt;/p&gt;
&lt;div class="math"&gt;$$
  \begin{align\*}
    |\psi_1\rangle &amp;amp;= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &amp;amp;= c|0\rangle + d|1\rangle
  \end{align\*}
$$&lt;/div&gt;
&lt;p&gt;Test a display math with equation number:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align}
    |\psi_1\rangle &amp;amp;= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &amp;amp;= c|0\rangle + d|1\rangle
\end{align}&lt;/div&gt;
&lt;p&gt;And test a display math without equaltion number:
&lt;/p&gt;
&lt;div class="math"&gt;\begin{align\*}
    |\psi_1\rangle &amp;amp;= a|0\rangle + b|1\rangle \\\\
    |\psi_2\rangle &amp;amp;= c|0\rangle + d|1\rangle
\end{align\*}&lt;/div&gt;
&lt;p&gt;Here are some notes about the above example:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The inline formula is between &lt;code&gt;$ ... $&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The display formula is between &lt;code&gt;$$ ... $$&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;You can use the math envrionment directly, for example, &lt;code&gt;\begin{equation}...\end{equation}&lt;/code&gt; or &lt;code&gt;\begin{align}...\end{align}&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Whenever in the inline math or display math, the star character &lt;code&gt;'*'&lt;/code&gt; must be escaped.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the multi-lines display math, the line break symbol double-backslash &lt;code&gt;'\\'&lt;/code&gt; should be escaped, i.e., use four backslash &lt;code&gt;'\\\\'&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If you found error while typeseting math formula, try to escape some special characters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="math"&gt;$$ \begin{aligned}
 \frac{\partial L}{\partial W}  &amp;amp; = \frac{\partial ( (Y - XW)'(Y-XW))}{\partial W}  \\\\ 
 &amp;amp; = \frac{\partial (Y'Y - 2W'X'Y  + W'X'XW)}{\partial W} \\\\
 &amp;amp;= -2X'(Y - XW)  
\end{aligned}$$&lt;/div&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://stackoverflow.com/questions/26275645/how-to-supported-latex-in-github-pages"&gt;How to supported latex in github-pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://haixing-hu.github.io/programming/2013/09/20/how-to-use-mathjax-in-jekyll-generated-github-pages/"&gt;How to use MathJax in Jekyll generated Github pages&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://zishuaiz.github.io/blog/how-to-enable-mathjax-in-github-pages"&gt;How to enable mathjax/latex in github pages&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category></entry><entry><title>convolve, correlate and image process in numpy</title><link href="/pages/2017/04/16/convolve-correlate-and-image-process-in-numpy/" rel="alternate"></link><published>2017-04-16T22:03:00-05:00</published><updated>2017-04-16T22:03:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-04-16:/pages/2017/04/16/convolve-correlate-and-image-process-in-numpy/</id><summary type="html">&lt;p&gt;convolution matrix is the most import conception in cnn. Here is a simple introduction of convolve, correlate and some image processing basic techs in numpy.&lt;/p&gt;</summary><content type="html">&lt;pre&gt;&lt;code class="language-python"&gt;%matplotlib inline
import math,sys,os,numpy as np, pandas as pd
from numpy.linalg import norm
from PIL import Image
from matplotlib import pyplot as plt, rcParams, rc
from scipy.ndimage import imread
from skimage.measure import block_reduce
import cPickle as pickle
from scipy.ndimage.filters import correlate, convolve
from ipywidgets import interact, interactive, fixed
from ipywidgets.widgets import *
rc('animation', html='html5')
rcParams['figure.figsize'] = 3, 6
%precision 4
np.set_printoptions(precision=4, linewidth=100)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;1. convolve and correlate in numpy&lt;/h2&gt;
&lt;h3&gt;1.1. convolve of two vectors&lt;/h3&gt;
&lt;p&gt;The convolution of two vectors, u and v, represents the area of overlap under the points as v slides across u. Algebraically, convolution is the same operation as multiplying polynomials whose coefficients are the elements of u and v.&lt;/p&gt;
&lt;p&gt;Let m = length(u) and n = length(v) . Then w is the vector of length m+n-1 whose kth element is
&lt;span class="math"&gt;\(w(k)=\sum_j u(j)v(k−j+1)\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The sum is over all the values of j that lead to legal subscripts for u(j) and v(k-j+1), specifically j = max(1,k+1-n):1:min(k,m). When m = n, this gives&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;w(1) = u(1)*v(1)
w(2) = u(1)*v(2)+u(2)*v(1)
w(3) = u(1)*v(3)+u(2)*v(2)+u(3)*v(1)
...
w(n) = u(1)*v(n)+u(2)*v(n-1)+ ... +u(n)*v(1)
...
w(2*n-1) = u(n)*v(n)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.convolve([1,2], [3,4,5])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 3, 10, 13, 10])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;3 = 1 * 3
10 = 1 * 4 + 2 * 3
13 = 1 * 5 + 2 * 4
10 = 2 * 5
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;convolve([1,2], [3,4,5])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([15, 19])
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;1.2. correlate of two vectors&lt;/h3&gt;
&lt;pre&gt;&lt;code&gt;c_{av}[k] = sum_n a[n+k] * conj(v[n])

The definition of correlation above is not unique and sometimes correlation may be defined differently. Another common definition is:

c'_{av}[k] = sum_n a[n] conj(v[n+k])
which is related to c_{av}[k] by c'_{av}[k] = c_{av}[-k].

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;numpy.correlate(a, v, mode='valid', old_behavior=False)[source]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Cross-correlation of two 1-dimensional sequences.&lt;/p&gt;
&lt;p&gt;This function computes the correlation as generally defined in signal processing texts:
     &lt;code&gt;z[k] = sum_n a[n] * conj(v[n+k])&lt;/code&gt;
  with a and v sequences being zero-padded where necessary and conj being the conjugate.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.correlate([1,2,3], [4,5,6], mode = 'full')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 6, 17, 32, 23, 12])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; 0 lag: 32 = 1 * 4 + 2 * 5 + 3 * 6
 1 lag: 23 = 2 * 4 + 3 * 5
 2 lag: 12 = 3 * 4
-1 lag: 17 = 1 * 5 + 2 * 6
-2 lag: 6 = 1 * 6
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;correlate([1,2,3], [4,5,6])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([21, 32, 41])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.correlate([1,2], [3,4,5], mode = 'full')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([ 5, 14, 11,  6])
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt; 0 lag: not same length, not available
 1 lag: 11 = 1 *3 + 2 * 4
 2 lag:  6 = 2 * 3
-1 lag: 14 = 1 * 4 + 2 * 5
-2 lag:  5 = 1 * 5
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;2. read and plot image in matplotlib&lt;/h2&gt;
&lt;h3&gt;2.1. Color image&lt;/h3&gt;
&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/Channel_(digital_image)"&gt;An image from a standard digital camera will have a red, green and blue channel(RGB)&lt;/a&gt;. A &lt;a href="https://en.wikipedia.org/wiki/Grayscale"&gt;grayscale&lt;/a&gt; image has just one channel.&lt;/p&gt;
&lt;p&gt;So if a color image is read in, the data will have three dimensions: width, height and chanels. And number of chanels(the 3rd dimension) all the time is three. For a grayscale image, the number of chanels will be one.&lt;/p&gt;
&lt;p&gt;For example, the following Cherry blossom will have dimension of (, , ) and the third dimension is 3.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# read in an image and show the image inline
im = plt.imread(&amp;quot;heben.jpg&amp;quot;)#r&amp;quot;Vd-Orig.png&amp;quot;) 
print im.shape
plt.imshow(im)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(213L, 237L, 3L)





&amp;lt;matplotlib.image.AxesImage at 0x13775860&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_12_2.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;im[:10, :10, 1]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[247, 247, 247, 247, 247, 247, 247, 247, 249, 249],
       [247, 247, 247, 247, 247, 247, 247, 247, 249, 249],
       [248, 248, 248, 248, 248, 248, 248, 248, 249, 249],
       [248, 248, 248, 248, 248, 248, 248, 248, 249, 249],
       [249, 249, 249, 249, 249, 249, 249, 249, 249, 249],
       [249, 249, 249, 249, 249, 249, 249, 249, 249, 249],
       [249, 249, 249, 249, 249, 249, 249, 249, 249, 249],
       [250, 250, 250, 250, 250, 250, 250, 250, 249, 249],
       [248, 248, 248, 248, 248, 248, 248, 248, 248, 248],
       [248, 248, 248, 248, 248, 248, 248, 248, 248, 248]], dtype=uint8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can also only show one chanel of the image. For example, the following is only the 3rd chanel of the image.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# only show the third chanel (index 2)
plt.imshow(im[:, :, [0,0,0]].astype(dtype=&amp;quot;uint8&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1971e898&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_15_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.imshow(im[:, :, 2].astype(dtype=&amp;quot;uint8&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x163bca58&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_16_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Another useful method is to plot the red(or green, blue) chanel only in the graph. To do that, we will keep only that chanel data, and set the rest of the chanel as 0.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;tmp_im = np.zeros(im.shape)
tmp_im[:, :, 0] = im[:, :, 0]
plt.imshow(tmp_im)
tmp_im[:3, :3, :]
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;array([[[ 247.,    0.,    0.],
        [ 247.,    0.,    0.],
        [ 247.,    0.,    0.]],

       [[ 247.,    0.,    0.],
        [ 247.,    0.,    0.],
        [ 247.,    0.,    0.]],

       [[ 248.,    0.,    0.],
        [ 248.,    0.,    0.],
        [ 248.,    0.,    0.]]])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_18_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def plti(im, **kwargs):
    plt.imshow(im, interpolation=&amp;quot;none&amp;quot;, **kwargs)
    plt.axis('off') # turn off axis
    plt.show()  

im = im[:, :, :]  # slice
plti(im)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_19_0.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(15,5)) 

for c, ax in zi//figures/[png](/figures/range(3), axs):  
    tmp_im = np.zeros(im.shape) 
    tmp_im[:,:,c] = im[:,:,c] 
    one_channel = im[:,:,c].flatte//figures/[png](/figures/) 
    print(&amp;quot;channel&amp;quot;, c, &amp;quot; max = &amp;quot;, max(one_channel), &amp;quot;min = &amp;quot;, mi//figures/[png](/figures/one_channel))  
    ax.imshow(tmp_im)  
    ax.set_axis_off() 
plt.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;('channel', 0, ' max = ', 255, 'min = ', 15)
('channel', 1, ' max = ', 255, 'min = ', 1)
('channel', 2, ' max = ', 255, 'min = ', 0)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_20_1.png"&gt;&lt;/p&gt;
&lt;h3&gt;2.2. Grayscale image&lt;/h3&gt;
&lt;p&gt;To convert a color image to grayscale, the 3 chanels data of RGB will be weighted sumed to one dimension. The weights is the average of the 3 chanels data by &lt;code&gt;np.mea//figures/[png](/figures/im, axis = (0, 1))&lt;/code&gt;. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;im_mean = np.mea//figures/[png](/figures/im, axis = (0, 1))
print im_mean / sum(im_mean)

def to_grayscale(im, weights = np.c_[0.3445, 0.3081, 0.3474]):
    &amp;quot;&amp;quot;&amp;quot;
    Transforms a colour image to a greyscale image by
    taking the mean of the RGB values, weighted
    by the matrix weights
    &amp;quot;&amp;quot;&amp;quot;
    tile = np.tile(weights, reps=(im.shape[0], im.shape[1], 1))
    return np.sum(tile * im, axis=2)

img = to_grayscale(im, np.c_[0.3957, 0.325, 0.2793])

plti(img, cmap='Greys')
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;[ 0.4002  0.3167  0.2831]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_22_1.png"&gt;&lt;/p&gt;
&lt;h2&gt;3. Convolve over image&lt;/h2&gt;
&lt;p&gt;In image processing, &lt;a href="https://en.wikipedia.org/wiki/Kernel_(image_processing)"&gt;convolution matrix&lt;/a&gt; is a matrix that each element will be multiplied by the part of the matrix that is been convolved. &lt;/p&gt;
&lt;p&gt;For example, matrix A is of dimension 10*10,  matrix B which is the conversion matrix of dimension 3 * 3. The convolution of B over A means for each 3 * 3 subset in A(or maybe zero padding of A), do the elementwise multiply between the subset and B, then the sum of the multiply will be the corresponding element in the output matrix. More details will be introduced later.&lt;/p&gt;
&lt;p&gt;There will be some corresponding question when we do the above calculation: 1. how to pick the dimension of B? 2. How many stride shall we skip when selecting the subset? 3. what kind of zero-padding shall be used? These questions will be answered later.&lt;/p&gt;
&lt;p&gt;The following is an example of convolution matrix called window with shape 5 * 5, with all elements equal and the sum is 1.Since its weights are all the same, this will mask the image.  &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;### convolve to mask the image

from scipy.signal import convolve2d
from scipy.ndimage.interpolation import zoom

im_small = zoom(im, (0.1, 0.1, 1))

def convolve_all_colours(im, window):
    &amp;quot;&amp;quot;&amp;quot;
    Convolves im with window, over all three colour channels
    &amp;quot;&amp;quot;&amp;quot;
    ims = []
    for d in range(3):
        im_conv_d = convolve2d(im[:,:,d], window, mode=&amp;quot;same&amp;quot;, boundary=&amp;quot;symm&amp;quot;)
        ims.append(im_conv_d)

    im_conv = np.stack(ims, axis=2).astype(&amp;quot;uint8&amp;quot;)

    return im_conv

n = 5
window = np.ones((n,n))
window /= np.sum(window)
plti(convolve_all_colours(im_small, window))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_24_0.png"&gt;&lt;/p&gt;
&lt;p&gt;Some more interesting convolution matrix: edge detection&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from numpy import *
import scipy
from scipy import misc, ndimage

def detect_edges(image,masks):
    edges=zeros(image.shape)
    for mask in masks:
        edges=maximum(scipy.ndimage.convolve(image, mask), edges)
    return edges

image=plt.imread(&amp;quot;lenna.png&amp;quot;)

Faler=[ [[-1,0,1],[-1,0,1],[-1,0,1]], 
        [[1,1,1],[0,0,0],[-1,-1,-1]],
    [[-1,-1,-1],[-1,8,-1],[-1,-1,-1]],
    [[0,1,0],[-1,0,1],[0,-1,0]] ]

edges=detect_edges(image, Faler)
plt.imshow(edges)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1be16f28&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_26_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.imshow(correlate(im[:, :, 0], [[1, 0, -1], [0,0,0], [-1,0,1]]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1b7a9908&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_27_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.imshow(correlate(im[:, :, 0], [[-1, -1, -1], [1,1,1], [0,0,0]]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x132a69e8&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_28_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.imshow(correlate(im[:, :, 0], [[-1, -1, -1], [-1,8,-1], [-1,-1,-1]]))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x1233a320&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_29_1.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;im1 = correlate(im[:, :, 2], [[1, 1, 0], [1,0,1], [0, -1, -1]])
im2 = convolve(im[:, :, 2], np.rot90([[1, 1, 0], [1,0,1], [0, -1, -1]],2))

print np.allclose(im1, im2)

plt.imshow(im1) 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;True





&amp;lt;matplotlib.image.AxesImage at 0xf6fa860&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_30_2.png"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;plt.imshow(im2)   
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;matplotlib.image.AxesImage at 0x10505470&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_31_1.png"&gt;&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://www.degeneratestate.org/posts/2016/Oct/23/image-processing-with-numpy/"&gt;image processing with numpy&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from scipy import signal
from scipy import misc
print ascent.shape
scharr = np.array([[ -3-3j, 0-10j,  +3 -3j],
                   [-10+0j, 0+ 0j, +10 +0j],
                   [ -3+3j, 0+10j,  +3 +3j]]) # Gx + j*Gy
grad = signal.convolve2d(ascent, scharr, boundary='symm', mode='same')



import matplotlib.pyplot as plt
fig, (ax_orig, ax_mag, ax_ang) = plt.subplots(3, 1, figsize=(6, 15))
ax_orig.imshow(ascent, cmap='gray')
ax_orig.set_title('Original')
ax_orig.set_axis_off()
ax_mag.imshow(np.absolute(grad), cmap='gray')
ax_mag.set_title('Gradient magnitude')
ax_mag.set_axis_off()
ax_ang.imshow(np.angle(grad), cmap='hsv') # hsv is cyclic, like angles
ax_ang.set_title('Gradient orientation')
ax_ang.set_axis_off()
fig.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(512L, 512L)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/output_33_1.png"&gt;&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="python"></category><category term="python"></category><category term="numpy"></category></entry><entry><title>variable selection in linear regression: 2</title><link href="/pages/2017/04/15/variable-selection-in-linear-regression-2/" rel="alternate"></link><published>2017-04-15T22:03:00-05:00</published><updated>2017-04-15T22:03:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-04-15:/pages/2017/04/15/variable-selection-in-linear-regression-2/</id><summary type="html">&lt;p&gt;This is an example of variable selection in linear regression(and it can be easily applied in logistic regression). It has two steps:&lt;/p&gt;</summary><content type="html">&lt;h2&gt;summary&lt;/h2&gt;
&lt;p&gt;This is an example of variable selection in linear regression(and it can be easily applied in logistic regression). It has two steps:&lt;/p&gt;
&lt;p&gt;The first step is univariate analysis. It checks the pearson correlation between each x and y. This is helpful to reduce the candidate predictors by only selecting the variables has significant correlation. The cutoff point is set p-value no more than 10%. Also the correlation coefficient sign is tested to make sure the correlation should make sense.&lt;/p&gt;
&lt;p&gt;The second step is multi-variate analysis. For the variables selected in step 1(say there are n variables left), all possible variables combinations will be run in the linear regression. Like all 2 variables combination(C(n, 2) combinations), 3 variables combination(C(n, 3) combinations)... will be tested in the linear regression. If there are duplicated variables(like GDP annual change and GDP quarterly change) in the model, it will de droped. The regression sign of each variable will be checked again. If any sign does not make sense, then the model will be droped. If there is any variable is not significant, then the model will be droped. If there is multicollinearity(checked by VIF), the model will be droped. &lt;/p&gt;
&lt;p&gt;The output will list the variables in the model, the regression coefficient, the p-value, vif, R2 and RMSE on training data and validation data set.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import numpy as np
from itertools import chain, combinations

import statsmodels.formula.api as smf
import scipy.stats as scipystats
from scipy.stats import norm
from scipy.stats.stats import pearsonr
import statsmodels.api as sm
import statsmodels.stats.stattools as stools
import statsmodels.stats as stats 
from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.graphics.regressionplots import *
import matplotlib.pyplot as plt
import math
mypath =  r'F:\Dropbox\ipynb_notes\ipynb_blog\\'

indata = pd.read_pickle(mypath + &amp;quot;data.pkl&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;set up display options&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;pd.options.display.max_rows
pd.set_option('display.max_colwidth', -1)

from IPython.display import display, HTML
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;step 1. univariate variable selection&lt;/h3&gt;
&lt;h4&gt;1.1. pearson calculation&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;set up the variable sign &lt;/li&gt;
&lt;li&gt;calculate the pearson correlation&lt;/li&gt;
&lt;li&gt;keep only significant variables by p-value &amp;lt; 0.1&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def pearson_corr(indata, xvar, yvar):
    outdf = pd.DataFrame()
    for i in range(len(xvar)):
        pcorr = pearsonr(indata[xvar[i]], indata[yvar])
        corrdf = pd.DataFrame({&amp;quot;var_name&amp;quot;: xvar[i], &amp;quot;pearson_corr&amp;quot;:pcorr[0], &amp;quot;p_value&amp;quot;:pcorr[1]}, index = [0])
        outdf = pd.concat([outdf, corrdf], axis = 0)
        outdf.sort_values('p_value')
    outdf.columns = corrdf.columns
    return outdf

def pearson_calc(indata, xvar, yvar):
    pearsoncorr = pearson_corr(indata, xvar, yvar)
    pearsoncorr = pearsoncorr.sort_values(by = ['p_value'])
    pearsoncorr = pearsoncorr.query('p_value &amp;lt;= 0.1')
    pearsoncorr = pearsoncorr.reset_index(drop = True)
    return pearsoncorr
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;xvarcol = indata.columns[1:-3]
yvarcol = indata.columns[0]
pearsoncorr = pearson_calc(indata, xvarcol, yvarcol)
pearsoncorr.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
      &lt;th&gt;pearson_corr&lt;/th&gt;
      &lt;th&gt;var_name&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.617254e-09&lt;/td&gt;
      &lt;td&gt;0.498257&lt;/td&gt;
      &lt;td&gt;cap_rate_p0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.287193e-08&lt;/td&gt;
      &lt;td&gt;-0.466109&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.615205e-08&lt;/td&gt;
      &lt;td&gt;0.464391&lt;/td&gt;
      &lt;td&gt;ted_spread_p4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.876455e-08&lt;/td&gt;
      &lt;td&gt;-0.448275&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2.407472e-07&lt;/td&gt;
      &lt;td&gt;0.434471&lt;/td&gt;
      &lt;td&gt;cap_rate_p1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h4&gt;1.2. check the sign of pearson correlation&lt;/h4&gt;
&lt;p&gt;only the correlation that makes sense will be kept. For example, gdp growth should have negative correlation with default rate. If the data shows positive sign, we will drop that data.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;varsign = {'bbb_spread_p': 1, 'ted_spread_p': 1, 'unemp_p': 1, 'realgpd_yoy_p': -1, 'cap_rate_p': 1, 'crea_hp_yoy_p': -1,    \
    'corp_profit_p': -1, 'com_price_index_yoy_p': -1, 'spindex_yoy_p': -1}

def check_sign(indata, varsign):
    varsign = pd.DataFrame.from_dict(varsign, orient = &amp;quot;index&amp;quot;).reset_index()
    varsign.columns = [&amp;quot;var_name_abb&amp;quot;, &amp;quot;sign&amp;quot;]
    indata['var_name_abb'] = indata.var_name.map(lambda x: x[:-1])
    indata = pd.merge(indata, varsign, how = &amp;quot;left&amp;quot;, on = ['var_name_abb'])
    indata['sign_true'] = np.where(indata.pearson_corr &amp;gt; 0, 1, -1) * indata.sign
    return indata.query('sign_true == 1')

output = check_sign(pearsoncorr, varsign)
output.sort_values('var_name')
output.head()
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;p_value&lt;/th&gt;
      &lt;th&gt;pearson_corr&lt;/th&gt;
      &lt;th&gt;var_name&lt;/th&gt;
      &lt;th&gt;var_name_abb&lt;/th&gt;
      &lt;th&gt;sign&lt;/th&gt;
      &lt;th&gt;sign_true&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1.617254e-09&lt;/td&gt;
      &lt;td&gt;0.498257&lt;/td&gt;
      &lt;td&gt;cap_rate_p0&lt;/td&gt;
      &lt;td&gt;cap_rate_p&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2.287193e-08&lt;/td&gt;
      &lt;td&gt;-0.466109&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p1&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;2.615205e-08&lt;/td&gt;
      &lt;td&gt;0.464391&lt;/td&gt;
      &lt;td&gt;ted_spread_p4&lt;/td&gt;
      &lt;td&gt;ted_spread_p&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;8.876455e-08&lt;/td&gt;
      &lt;td&gt;-0.448275&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p0&lt;/td&gt;
      &lt;td&gt;realgpd_yoy_p&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;2.407472e-07&lt;/td&gt;
      &lt;td&gt;0.434471&lt;/td&gt;
      &lt;td&gt;cap_rate_p1&lt;/td&gt;
      &lt;td&gt;cap_rate_p&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3&gt;2. Multi-variate variable selection&lt;/h3&gt;
&lt;p&gt;In this step, we will try all the combinations of the X variables in the data and run the multi-variable regression. &lt;/p&gt;
&lt;h4&gt;2.1 Prepare data&lt;/h4&gt;
&lt;p&gt;From the univariate analysis, we will get the list of X variables(pearson p-vaule is significant, and correlation sign is correct) that should be tested on the multi-variable analysis.&lt;/p&gt;
&lt;p&gt;We will consider both the training data and the validation data. So the data will be prepared into training and validation.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;xvar = output.var_name
cniX = indata.ix[:, xvar]
cniY = indata.ix[:, &amp;quot;cum_pd_num&amp;quot;]    
train_flag = indata.train_flag

xtrain, xtest = cniX[train_flag], cniX[~train_flag] 
ytrain, ytest = cniY[train_flag], cniY[~train_flag] 
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;print xtrain.shape
print xtest.shape
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(91, 28)
(39, 28)
&lt;/code&gt;&lt;/pre&gt;
&lt;h4&gt;2.2. multi-variable analysis&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;for all 2/3/4 ... variable combination, run the linear regression&lt;/li&gt;
&lt;li&gt;calculate the vif and vif_max for the X combination &lt;/li&gt;
&lt;li&gt;run linear regression&lt;/li&gt;
&lt;li&gt;if there is variables duplicated, then drop it, otherwise go to next step&lt;/li&gt;
&lt;li&gt;if the any variables coefficient sign is not intuitive, drop it; otherwise go to next step&lt;/li&gt;
&lt;li&gt;if new model score is better than the previous model, keep it; otherwise drop it&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def verify_sign(params):
    setsign = varsign
    setsign = pd.DataFrame.from_dict(setsign, orient = &amp;quot;index&amp;quot;).reset_index()
    setsign.columns = ['vars', 'coefsign']
    params = params.reset_index()
    params.columns = ['vars', 'coefd']
    params['vars'] = params.vars.map(lambda x: x[:-1])
    combinedata = pd.merge(setsign, params, how = 'inner', on = 'vars')
    combinedata['coefd_'] = combinedata.coefsign * np.sign(combinedata.coefd)
    if sum(combinedata['coefd_'] &amp;gt; 0) == combinedata.shape[0]:
        return True
    else:
        return False


def check_dup_vars(xvar):
    xx = xvar
    xxx = set([x[:-1] for x in xx])
    if len(xxx) == len(xx):
        return True
    else:
        return False

def all_combination_withsign2(xtrain, ytrain, xtest, ytest, k_features = [1]):
    outdf = pd.DataFrame()
    features = xtrain.columns.tolist()
    n_features = xtrain.shape[1]
    subsets = chain.from_iterable(combinations(xrange(n_features), k) for k in k_features)
    best_score = -np.inf
    all_combination = None
    for subset in subsets:
        newxtrain = xtrain.iloc[:, subset]
        newxtrain = sm.add_constant(newxtrain)
        vif = dict(zip(newxtrain.columns, [variance_inflation_factor(newxtrain.values, i) for i in range(newxtrain.shape[1])])) 
        del vif['const']
        vif_max = max(vif.itervalues())
        newxtest = xtest.iloc[:, subset]
        newxtest = sm.add_constant(newxtest)        
        lin_reg = sm.OLS(ytrain, newxtrain).fit()       
        #lin_reg = sm.OLS(ytrain, newxtrain).fit(cov_type='HAC',cov_kwds={'maxlags':1})
        if check_dup_vars(lin_reg.params.index.tolist()):        
            r2train = lin_reg.rsquared
            r2test1 = np.sum((lin_reg.predict(newxtest) - np.mean(ytest))**2) / np.sum((ytest - np.mean(ytest))**2)
            r2test = 1 - np.sum((lin_reg.predict(newxtest) -  ytest)**2) / np.sum((ytest - np.mean(ytest))**2)            
            r2test2 = 1 - np.var(ytest - lin_reg.predict(newxtest)) / np.var(ytest)       
            rmse_train = math.sqrt(sum((lin_reg.predict(newxtrain) - ytrain)**2) / (len(ytrain) - len(lin_reg.params)))
            rmse_train2 = math.sqrt(lin_reg.mse_resid)
            pred = lin_reg.predict(newxtest)        
            rmse_test = math.sqrt(sum((pred - ytest)**2)/(len(ytest) - len(lin_reg.params)))
            if (verify_sign(lin_reg.params)):
                sss = &amp;quot; &amp;quot;.join([features[i] for i in subset])
                pvalues = lin_reg.pvalues.drop('const').to_dict(); pvalues_max = max(pvalues.itervalues())
                dd = {'vars':sss, 'r2train':r2train, 'r2test':r2test, 'rmse_train':rmse_train,  'rmse_test':rmse_test, 'pvalues': [pvalues], \
                    'pvalues_max': pvalues_max, &amp;quot;reg_coef&amp;quot;: [dict(lin_reg.params)], &amp;quot;vif&amp;quot;:[vif], &amp;quot;vif_max&amp;quot;:vif_max}
                df_i = pd.DataFrame(dd, index = [0], columns = dd.keys())  
                outdf = pd.concat([df_i, outdf], axis = 0)
                score = lin_reg.rsquared
                if (score &amp;gt; best_score) &amp;amp; (vif_max &amp;lt; 5) &amp;amp; (pvalues_max &amp;lt; 0.1):                  
                    best_score, all_combination = score, subset
                    best_reg_result = lin_reg

    outdf = outdf[[&amp;quot;vars&amp;quot;, &amp;quot;reg_coef&amp;quot;, &amp;quot;pvalues&amp;quot;, &amp;quot;pvalues_max&amp;quot;, &amp;quot;vif&amp;quot;, &amp;quot;vif_max&amp;quot;, &amp;quot;r2train&amp;quot;, &amp;quot;r2test&amp;quot;, &amp;quot;rmse_train&amp;quot;, &amp;quot;rmse_test&amp;quot;]]                                       
    return all_combination, best_score, best_reg_result, outdf

all_combination, best_score, best_reg_result, outdf  = all_combination_withsign2(xtrain, ytrain, xtest, ytest, k_features = [3])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;List all the candidate models and sort them by the R2 on training data. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;outdf.sort_values('r2train', ascending = False).query('r2train &amp;gt; 0.5')
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;vars&lt;/th&gt;
      &lt;th&gt;reg_coef&lt;/th&gt;
      &lt;th&gt;pvalues&lt;/th&gt;
      &lt;th&gt;pvalues_max&lt;/th&gt;
      &lt;th&gt;vif&lt;/th&gt;
      &lt;th&gt;vif_max&lt;/th&gt;
      &lt;th&gt;r2train&lt;/th&gt;
      &lt;th&gt;r2test&lt;/th&gt;
      &lt;th&gt;rmse_train&lt;/th&gt;
      &lt;th&gt;rmse_test&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p3 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.844407825413, u'crea_hp_yoy_p3': -9.75552407688, u'const': -5.82475185843, u'ted_spread_p0': 2.9978370395}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 3.94306563611e-10, u'crea_hp_yoy_p3': 1.24108359437e-13, u'ted_spread_p0': 1.12453478033e-13}&lt;/td&gt;
      &lt;td&gt;3.943066e-10&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.0291090023, u'crea_hp_yoy_p3': 1.26682457883, u'ted_spread_p0': 1.29763514744}&lt;/td&gt;
      &lt;td&gt;1.297635&lt;/td&gt;
      &lt;td&gt;0.614574&lt;/td&gt;
      &lt;td&gt;0.478290&lt;/td&gt;
      &lt;td&gt;0.633015&lt;/td&gt;
      &lt;td&gt;0.767676&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p2 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.811141476444, u'crea_hp_yoy_p3': -8.88100839499, u'const': -5.58527727789, u'ted_spread_p0': 2.83870388981}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 5.84092070233e-10, u'crea_hp_yoy_p3': 5.64924235802e-12, u'ted_spread_p0': 8.5084006685e-13}&lt;/td&gt;
      &lt;td&gt;5.840921e-10&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.02146127586, u'crea_hp_yoy_p3': 1.26361213133, u'ted_spread_p0': 1.27468501164}&lt;/td&gt;
      &lt;td&gt;1.274685&lt;/td&gt;
      &lt;td&gt;0.611142&lt;/td&gt;
      &lt;td&gt;0.509502&lt;/td&gt;
      &lt;td&gt;0.635827&lt;/td&gt;
      &lt;td&gt;0.744358&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p3 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.02088758198, u'corp_profit_p1': -0.0194663514096, u'const': -7.15607863379, u'ted_spread_p0': 2.49614318727}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.48967861708e-12, u'corp_profit_p1': 2.36587341489e-13, u'ted_spread_p0': 1.39564236076e-11}&lt;/td&gt;
      &lt;td&gt;1.395642e-11&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.08231697692, u'corp_profit_p1': 1.14482598717, u'ted_spread_p0': 1.13625413757}&lt;/td&gt;
      &lt;td&gt;1.144826&lt;/td&gt;
      &lt;td&gt;0.608887&lt;/td&gt;
      &lt;td&gt;0.414865&lt;/td&gt;
      &lt;td&gt;0.637668&lt;/td&gt;
      &lt;td&gt;0.813001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p1 ted_spread_p0 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 0.789251206651, u'const': -5.40588738423, u'crea_hp_yoy_p3': -8.3294013128, u'ted_spread_p0': 2.69331735652}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 1.01807994814e-09, u'crea_hp_yoy_p3': 8.57212928459e-11, u'ted_spread_p0': 6.79453180355e-12}&lt;/td&gt;
      &lt;td&gt;1.018080e-09&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 1.02530776004, u'crea_hp_yoy_p3': 1.27841193373, u'ted_spread_p0': 1.26335700212}&lt;/td&gt;
      &lt;td&gt;1.278412&lt;/td&gt;
      &lt;td&gt;0.606241&lt;/td&gt;
      &lt;td&gt;0.549077&lt;/td&gt;
      &lt;td&gt;0.639821&lt;/td&gt;
      &lt;td&gt;0.713698&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 crea_hp_yoy_p3 cap_rate_p4&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -10.9366916932, u'cap_rate_p4': 0.873672209208, u'const': -5.99301833528, u'ted_spread_p0': 3.0785031203}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 4.03277792628e-15, u'cap_rate_p4': 1.21300198442e-09, u'ted_spread_p0': 8.47813573869e-14}&lt;/td&gt;
      &lt;td&gt;1.213002e-09&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.32415796614, u'cap_rate_p4': 1.06453290377, u'ted_spread_p0': 1.31619929247}&lt;/td&gt;
      &lt;td&gt;1.324158&lt;/td&gt;
      &lt;td&gt;0.604684&lt;/td&gt;
      &lt;td&gt;0.434813&lt;/td&gt;
      &lt;td&gt;0.641085&lt;/td&gt;
      &lt;td&gt;0.799023&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p2 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.961038613195, u'corp_profit_p1': -0.0172934378291, u'const': -6.71919753298, u'ted_spread_p0': 2.35603497118}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 2.71762604741e-12, u'corp_profit_p1': 1.33144749492e-11, u'ted_spread_p0': 9.53692120781e-11}&lt;/td&gt;
      &lt;td&gt;9.536921e-11&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.03374482226, u'corp_profit_p1': 1.09884169943, u'ted_spread_p0': 1.11410042186}&lt;/td&gt;
      &lt;td&gt;1.114100&lt;/td&gt;
      &lt;td&gt;0.603520&lt;/td&gt;
      &lt;td&gt;0.423553&lt;/td&gt;
      &lt;td&gt;0.642028&lt;/td&gt;
      &lt;td&gt;0.806943&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p1 ted_spread_p0 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 0.920991926182, u'corp_profit_p1': -0.0158660128406, u'const': -6.4030489504, u'ted_spread_p0': 2.21313727996}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 7.0585880264e-12, u'corp_profit_p1': 3.02949988624e-10, u'ted_spread_p0': 8.21215334728e-10}&lt;/td&gt;
      &lt;td&gt;8.212153e-10&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 1.01298762173, u'corp_profit_p1': 1.08530202926, u'ted_spread_p0': 1.09761360937}&lt;/td&gt;
      &lt;td&gt;1.097614&lt;/td&gt;
      &lt;td&gt;0.594854&lt;/td&gt;
      &lt;td&gt;0.457804&lt;/td&gt;
      &lt;td&gt;0.649007&lt;/td&gt;
      &lt;td&gt;0.782603&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 corp_profit_p1 cap_rate_p4&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': -0.0223204781179, u'const': -7.56469914659, u'cap_rate_p4': 1.08379220206, u'ted_spread_p0': 2.51812630047}&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 1.1949152689e-14, u'cap_rate_p4': 7.10244169808e-12, u'ted_spread_p0': 2.14516288708e-11}&lt;/td&gt;
      &lt;td&gt;2.145163e-11&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 1.2612989854, u'cap_rate_p4': 1.18006908653, u'ted_spread_p0': 1.1431268946}&lt;/td&gt;
      &lt;td&gt;1.261299&lt;/td&gt;
      &lt;td&gt;0.594797&lt;/td&gt;
      &lt;td&gt;0.388572&lt;/td&gt;
      &lt;td&gt;0.649052&lt;/td&gt;
      &lt;td&gt;0.831067&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 ted_spread_p0 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -7.8525157607, u'cap_rate_p0': 0.737744628637, u'const': -4.98396736099, u'ted_spread_p0': 2.38727745306}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.75576444313e-09, u'cap_rate_p0': 8.69348819727e-09, u'ted_spread_p0': 1.05148081701e-09}&lt;/td&gt;
      &lt;td&gt;8.693488e-09&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.30577823981, u'cap_rate_p0': 1.03552221083, u'ted_spread_p0': 1.27216361003}&lt;/td&gt;
      &lt;td&gt;1.305778&lt;/td&gt;
      &lt;td&gt;0.586771&lt;/td&gt;
      &lt;td&gt;0.550868&lt;/td&gt;
      &lt;td&gt;0.655449&lt;/td&gt;
      &lt;td&gt;0.712279&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 ted_spread_p0 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 0.872842905445, u'const': -5.97367379688, u'corp_profit_p1': -0.0149625516801, u'ted_spread_p0': 1.89650848195}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 2.90085948633e-11, u'corp_profit_p1': 3.02326631849e-09, u'ted_spread_p0': 8.42420292381e-08}&lt;/td&gt;
      &lt;td&gt;8.424203e-08&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 1.00022202315, u'corp_profit_p1': 1.08376800619, u'ted_spread_p0': 1.08376614561}&lt;/td&gt;
      &lt;td&gt;1.083768&lt;/td&gt;
      &lt;td&gt;0.581685&lt;/td&gt;
      &lt;td&gt;0.465779&lt;/td&gt;
      &lt;td&gt;0.659471&lt;/td&gt;
      &lt;td&gt;0.776826&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 cap_rate_p4&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p4': 0.459310529281, u'const': -3.19209503322, u'realgpd_yoy_p1': -0.317694979609, u'ted_spread_p0': 2.24499133704}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p4': 0.000880252388483, u'realgpd_yoy_p1': 4.64502170524e-13, u'ted_spread_p0': 1.71481655757e-09}&lt;/td&gt;
      &lt;td&gt;8.802524e-04&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p4': 1.03139580039, u'realgpd_yoy_p1': 1.10223254627, u'ted_spread_p0': 1.08994866485}&lt;/td&gt;
      &lt;td&gt;1.102233&lt;/td&gt;
      &lt;td&gt;0.559669&lt;/td&gt;
      &lt;td&gt;0.461177&lt;/td&gt;
      &lt;td&gt;0.676602&lt;/td&gt;
      &lt;td&gt;0.780165&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 cap_rate_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.441726138975, u'const': -3.10473119229, u'realgpd_yoy_p1': -0.292255362611, u'ted_spread_p0': 2.22392026902}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.0016672139689, u'realgpd_yoy_p1': 8.08991799692e-11, u'ted_spread_p0': 2.71002168046e-09}&lt;/td&gt;
      &lt;td&gt;1.667214e-03&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.14924858178, u'realgpd_yoy_p1': 1.21544761004, u'ted_spread_p0': 1.08765234604}&lt;/td&gt;
      &lt;td&gt;1.215448&lt;/td&gt;
      &lt;td&gt;0.553622&lt;/td&gt;
      &lt;td&gt;0.462181&lt;/td&gt;
      &lt;td&gt;0.681231&lt;/td&gt;
      &lt;td&gt;0.779438&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 cap_rate_p2&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.43466635192, u'const': -3.05451545084, u'realgpd_yoy_p1': -0.275385243575, u'ted_spread_p0': 2.16541214241}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.00266629601475, u'realgpd_yoy_p1': 3.95110812196e-09, u'ted_spread_p0': 6.47022044576e-09}&lt;/td&gt;
      &lt;td&gt;2.666296e-03&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.28453682239, u'realgpd_yoy_p1': 1.36522930468, u'ted_spread_p0': 1.08353965769}&lt;/td&gt;
      &lt;td&gt;1.365229&lt;/td&gt;
      &lt;td&gt;0.549157&lt;/td&gt;
      &lt;td&gt;0.458911&lt;/td&gt;
      &lt;td&gt;0.684631&lt;/td&gt;
      &lt;td&gt;0.781804&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 ted_spread_p0 corp_profit_p2&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p2': -0.013387541678, u'cap_rate_p0': 0.866163613446, u'const': -5.84769476189, u'ted_spread_p0': 1.66498201142}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 1.4478538301e-10, u'corp_profit_p2': 9.61024512588e-08, u'ted_spread_p0': 2.40214450834e-06}&lt;/td&gt;
      &lt;td&gt;2.402145e-06&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p2': 1.03765625407, u'cap_rate_p0': 1.00059373935, u'ted_spread_p0': 1.03729905889}&lt;/td&gt;
      &lt;td&gt;1.037656&lt;/td&gt;
      &lt;td&gt;0.547915&lt;/td&gt;
      &lt;td&gt;0.428487&lt;/td&gt;
      &lt;td&gt;0.685573&lt;/td&gt;
      &lt;td&gt;0.803482&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 cap_rate_p1 ted_spread_p0&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 0.42522657551, u'const': -2.97879289237, u'realgpd_yoy_p1': -0.264924269333, u'ted_spread_p0': 2.10282371324}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 0.00449937414885, u'realgpd_yoy_p1': 5.72394243797e-08, u'ted_spread_p0': 1.78714604679e-08}&lt;/td&gt;
      &lt;td&gt;4.499374e-03&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 1.41658994804, u'realgpd_yoy_p1': 1.51749717529, u'ted_spread_p0': 1.08664051545}&lt;/td&gt;
      &lt;td&gt;1.517497&lt;/td&gt;
      &lt;td&gt;0.544166&lt;/td&gt;
      &lt;td&gt;0.475067&lt;/td&gt;
      &lt;td&gt;0.688410&lt;/td&gt;
      &lt;td&gt;0.770043&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.554595648355, u'corp_profit_p1': -0.0076010199434, u'const': -2.38704628717, u'ted_spread_p0': 2.85607944113}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.8523928591e-09, u'corp_profit_p1': 0.00482637840243, u'ted_spread_p0': 1.3484537929e-11}&lt;/td&gt;
      &lt;td&gt;4.826378e-03&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.54114854916, u'corp_profit_p1': 1.32791416339, u'ted_spread_p0': 1.26387542399}&lt;/td&gt;
      &lt;td&gt;1.541149&lt;/td&gt;
      &lt;td&gt;0.540536&lt;/td&gt;
      &lt;td&gt;0.425760&lt;/td&gt;
      &lt;td&gt;0.691145&lt;/td&gt;
      &lt;td&gt;0.805397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p1 ted_spread_p0 corp_profit_p2&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p2': -0.0135243557809, u'cap_rate_p1': 0.877749475261, u'const': -6.03948102864, u'ted_spread_p0': 1.93705865732}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 3.44329193716e-10, u'corp_profit_p2': 9.48169185403e-08, u'ted_spread_p0': 1.09945034371e-07}&lt;/td&gt;
      &lt;td&gt;1.099450e-07&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p2': 1.03734814056, u'cap_rate_p1': 1.01163126479, u'ted_spread_p0': 1.04793790581}&lt;/td&gt;
      &lt;td&gt;1.047938&lt;/td&gt;
      &lt;td&gt;0.538986&lt;/td&gt;
      &lt;td&gt;0.407210&lt;/td&gt;
      &lt;td&gt;0.692310&lt;/td&gt;
      &lt;td&gt;0.818302&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 realgpd_yoy_p1 ted_spread_p0&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 0.399969892292, u'const': -2.76856127445, u'realgpd_yoy_p1': -0.261163454589, u'ted_spread_p0': 1.96391841957}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 0.00823171721478, u'realgpd_yoy_p1': 2.44100020902e-07, u'ted_spread_p0': 2.02057150048e-07}&lt;/td&gt;
      &lt;td&gt;8.231717e-03&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p0': 1.51138715628, u'realgpd_yoy_p1': 1.63739288792, u'ted_spread_p0': 1.13076824898}&lt;/td&gt;
      &lt;td&gt;1.637393&lt;/td&gt;
      &lt;td&gt;0.538396&lt;/td&gt;
      &lt;td&gt;0.479249&lt;/td&gt;
      &lt;td&gt;0.692753&lt;/td&gt;
      &lt;td&gt;0.766970&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p1 ted_spread_p0 spindex_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 0.65706188261, u'spindex_yoy_p3': -0.0278317743804, u'const': -4.80448757293, u'ted_spread_p0': 2.33803685851}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 2.74601006446e-06, u'spindex_yoy_p3': 1.52047871931e-07, u'ted_spread_p0': 3.75314669611e-09}&lt;/td&gt;
      &lt;td&gt;2.746010e-06&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p1': 1.11830677895, u'spindex_yoy_p3': 1.30113118522, u'ted_spread_p0': 1.17735336332}&lt;/td&gt;
      &lt;td&gt;1.301131&lt;/td&gt;
      &lt;td&gt;0.534091&lt;/td&gt;
      &lt;td&gt;0.467292&lt;/td&gt;
      &lt;td&gt;0.695975&lt;/td&gt;
      &lt;td&gt;0.775725&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.496427591994, u'crea_hp_yoy_p3': -4.04408099734, u'const': -2.11272790298, u'ted_spread_p0': 2.98218905816}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 2.53868889258e-06, u'crea_hp_yoy_p3': 0.013182986019, u'ted_spread_p0': 8.44964879018e-12}&lt;/td&gt;
      &lt;td&gt;1.318299e-02&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 2.14915809275, u'crea_hp_yoy_p3': 2.15508279604, u'ted_spread_p0': 1.31551155593}&lt;/td&gt;
      &lt;td&gt;2.155083&lt;/td&gt;
      &lt;td&gt;0.530881&lt;/td&gt;
      &lt;td&gt;0.487799&lt;/td&gt;
      &lt;td&gt;0.698369&lt;/td&gt;
      &lt;td&gt;0.760648&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 ted_spread_p0 spindex_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': -0.0268613344048, u'cap_rate_p0': 0.637203038203, u'const': -4.58585413988, u'ted_spread_p0': 2.10847588993}&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': 5.48924336311e-07, u'cap_rate_p0': 4.08336221776e-06, u'ted_spread_p0': 9.57175247542e-08}&lt;/td&gt;
      &lt;td&gt;4.083362e-06&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': 1.34011381199, u'cap_rate_p0': 1.13890662152, u'ted_spread_p0': 1.20436511897}&lt;/td&gt;
      &lt;td&gt;1.340114&lt;/td&gt;
      &lt;td&gt;0.529975&lt;/td&gt;
      &lt;td&gt;0.481577&lt;/td&gt;
      &lt;td&gt;0.699043&lt;/td&gt;
      &lt;td&gt;0.765254&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 realgpd_yoy_p2 crea_hp_yoy_p2&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': -5.23054020025, u'const': -0.213910926506, u'realgpd_yoy_p2': -0.300526775383, u'ted_spread_p0': 2.54263066825}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 4.73746805923e-05, u'realgpd_yoy_p2': 2.90405468424e-11, u'ted_spread_p0': 4.65498962823e-10}&lt;/td&gt;
      &lt;td&gt;4.737468e-05&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 1.14885263493, u'realgpd_yoy_p2': 1.07915752568, u'ted_spread_p0': 1.20209011854}&lt;/td&gt;
      &lt;td&gt;1.202090&lt;/td&gt;
      &lt;td&gt;0.528894&lt;/td&gt;
      &lt;td&gt;0.490617&lt;/td&gt;
      &lt;td&gt;0.699847&lt;/td&gt;
      &lt;td&gt;0.758552&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.3392041554, u'const': -1.48765136041, u'realgpd_yoy_p1': -0.183029785519, u'ted_spread_p0': 2.53186775198}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.0371576508071, u'realgpd_yoy_p1': 0.0268215589596, u'ted_spread_p0': 4.5194564047e-09}&lt;/td&gt;
      &lt;td&gt;3.715765e-02&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 5.6016017398, u'realgpd_yoy_p1': 4.82586299937, u'ted_spread_p0': 1.36892246555}&lt;/td&gt;
      &lt;td&gt;5.601602&lt;/td&gt;
      &lt;td&gt;0.524097&lt;/td&gt;
      &lt;td&gt;0.448446&lt;/td&gt;
      &lt;td&gt;0.703401&lt;/td&gt;
      &lt;td&gt;0.789328&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 corp_profit_p1 bbb_spread_p4&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': -0.0137172688584, u'const': -2.37417449176, u'bbb_spread_p4': 0.524233697796, u'ted_spread_p0': 3.14985768412}&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 2.02303842995e-07, u'bbb_spread_p4': 9.78700091729e-09, u'ted_spread_p0': 6.29018807036e-12}&lt;/td&gt;
      &lt;td&gt;2.023038e-07&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 1.09331461883, u'bbb_spread_p4': 1.37836343887, u'ted_spread_p0': 1.42051444169}&lt;/td&gt;
      &lt;td&gt;1.420514&lt;/td&gt;
      &lt;td&gt;0.523007&lt;/td&gt;
      &lt;td&gt;0.343924&lt;/td&gt;
      &lt;td&gt;0.704206&lt;/td&gt;
      &lt;td&gt;0.860876&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p2 corp_profit_p2&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.861897957176, u'corp_profit_p2': -0.0139592553872, u'const': -5.98901672449, u'ted_spread_p0': 2.01153180982}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 2.3561392043e-09, u'corp_profit_p2': 7.3640978496e-08, u'ted_spread_p0': 8.20990655485e-08}&lt;/td&gt;
      &lt;td&gt;8.209907e-08&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.01956965532, u'corp_profit_p2': 1.03727640573, u'ted_spread_p0': 1.05720340499}&lt;/td&gt;
      &lt;td&gt;1.057203&lt;/td&gt;
      &lt;td&gt;0.518564&lt;/td&gt;
      &lt;td&gt;0.346343&lt;/td&gt;
      &lt;td&gt;0.707478&lt;/td&gt;
      &lt;td&gt;0.859287&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p2 spindex_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.623635734378, u'spindex_yoy_p3': -0.0288247874575, u'const': -4.63667583576, u'ted_spread_p0': 2.40681253901}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.38801807046e-05, u'spindex_yoy_p3': 8.44057937049e-08, u'ted_spread_p0': 2.69534162048e-09}&lt;/td&gt;
      &lt;td&gt;1.388018e-05&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.11243360337, u'spindex_yoy_p3': 1.28413160743, u'ted_spread_p0': 1.17710523955}&lt;/td&gt;
      &lt;td&gt;1.284132&lt;/td&gt;
      &lt;td&gt;0.517091&lt;/td&gt;
      &lt;td&gt;0.428256&lt;/td&gt;
      &lt;td&gt;0.708559&lt;/td&gt;
      &lt;td&gt;0.803645&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 crea_hp_yoy_p2&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.60361226282, u'crea_hp_yoy_p2': -2.36021166288, u'const': -2.44208114633, u'ted_spread_p0': 2.90459106331}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.15389476089e-10, u'crea_hp_yoy_p2': 0.0792156318557, u'ted_spread_p0': 3.35277155454e-11}&lt;/td&gt;
      &lt;td&gt;7.921563e-02&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.45111260067, u'crea_hp_yoy_p2': 1.31847516127, u'ted_spread_p0': 1.30031898369}&lt;/td&gt;
      &lt;td&gt;1.451113&lt;/td&gt;
      &lt;td&gt;0.513970&lt;/td&gt;
      &lt;td&gt;0.494352&lt;/td&gt;
      &lt;td&gt;0.710845&lt;/td&gt;
      &lt;td&gt;0.755766&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 corp_profit_p2&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.582278194028, u'corp_profit_p2': -0.00457269604228, u'const': -2.42447060517, u'ted_spread_p0': 2.73939873183}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 4.46517244848e-09, u'corp_profit_p2': 0.103032476656, u'ted_spread_p0': 1.65472591821e-10}&lt;/td&gt;
      &lt;td&gt;1.030325e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.69554846895, u'corp_profit_p2': 1.39827155759, u'ted_spread_p0': 1.26336327403}&lt;/td&gt;
      &lt;td&gt;1.695548&lt;/td&gt;
      &lt;td&gt;0.511588&lt;/td&gt;
      &lt;td&gt;0.404442&lt;/td&gt;
      &lt;td&gt;0.712585&lt;/td&gt;
      &lt;td&gt;0.820210&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 crea_hp_yoy_p2 bbb_spread_p4&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': -6.70808022288, u'const': -2.42877868756, u'bbb_spread_p4': 0.607734054772, u'ted_spread_p0': 3.4961201786}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 6.05151670014e-07, u'bbb_spread_p4': 1.48369689826e-10, u'ted_spread_p0': 1.03002090634e-12}&lt;/td&gt;
      &lt;td&gt;6.051517e-07&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 1.15154216378, u'bbb_spread_p4': 1.37674369466, u'ted_spread_p0': 1.55319251249}&lt;/td&gt;
      &lt;td&gt;1.553193&lt;/td&gt;
      &lt;td&gt;0.511203&lt;/td&gt;
      &lt;td&gt;0.561489&lt;/td&gt;
      &lt;td&gt;0.712866&lt;/td&gt;
      &lt;td&gt;0.703807&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -2.69122158056, u'const': -0.311597133369, u'realgpd_yoy_p1': -0.271003198186, u'ted_spread_p0': 2.36903437474}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 0.160540939231, u'realgpd_yoy_p1': 1.68115665371e-05, u'ted_spread_p0': 1.73940931661e-08}&lt;/td&gt;
      &lt;td&gt;1.605409e-01&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 2.92635266698, u'realgpd_yoy_p1': 2.51416529656, u'ted_spread_p0': 1.28272242148}&lt;/td&gt;
      &lt;td&gt;2.926353&lt;/td&gt;
      &lt;td&gt;0.510855&lt;/td&gt;
      &lt;td&gt;0.496386&lt;/td&gt;
      &lt;td&gt;0.713120&lt;/td&gt;
      &lt;td&gt;0.754245&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 bbb_spread_p4&lt;/td&gt;
      &lt;td&gt;{u'const': -0.833411322829, u'realgpd_yoy_p1': -0.284642070828, u'bbb_spread_p4': 0.157377817458, u'ted_spread_p0': 2.41688117752}&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p1': 6.46698989093e-07, u'bbb_spread_p4': 0.167778133568, u'ted_spread_p0': 3.11105712134e-08}&lt;/td&gt;
      &lt;td&gt;1.677781e-01&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p1': 1.99595132113, u'bbb_spread_p4': 2.51669899622, u'ted_spread_p0': 1.392766576}&lt;/td&gt;
      &lt;td&gt;2.516699&lt;/td&gt;
      &lt;td&gt;0.510479&lt;/td&gt;
      &lt;td&gt;0.431894&lt;/td&gt;
      &lt;td&gt;0.713393&lt;/td&gt;
      &lt;td&gt;0.801084&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p1 ted_spread_p0 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.544850450153, u'cap_rate_p1': 0.272985200716, u'const': -3.98664801529, u'ted_spread_p0': 2.60924536243}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.44198483922e-06, u'cap_rate_p1': 0.121325501399, u'ted_spread_p0': 2.7102689375e-09}&lt;/td&gt;
      &lt;td&gt;1.213255e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 2.34776209124, u'cap_rate_p1': 1.88813529519, u'ted_spread_p0': 1.36428733329}&lt;/td&gt;
      &lt;td&gt;2.347762&lt;/td&gt;
      &lt;td&gt;0.510130&lt;/td&gt;
      &lt;td&gt;0.428608&lt;/td&gt;
      &lt;td&gt;0.713648&lt;/td&gt;
      &lt;td&gt;0.803397&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 realgpd_yoy_p2 crea_hp_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -5.59792480702, u'const': -0.330017028638, u'realgpd_yoy_p2': -0.215888553711, u'ted_spread_p0': 2.58561014684}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 0.000296441873851, u'realgpd_yoy_p2': 1.87381427588e-05, u'ted_spread_p0': 1.09664873088e-09}&lt;/td&gt;
      &lt;td&gt;2.964419e-04&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.77986311362, u'realgpd_yoy_p2': 1.51489441528, u'ted_spread_p0': 1.26112101248}&lt;/td&gt;
      &lt;td&gt;1.779863&lt;/td&gt;
      &lt;td&gt;0.509685&lt;/td&gt;
      &lt;td&gt;0.447184&lt;/td&gt;
      &lt;td&gt;0.713972&lt;/td&gt;
      &lt;td&gt;0.790230&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;cap_rate_p0 ted_spread_p0 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.538791932676, u'cap_rate_p0': 0.269149311217, u'const': -3.90589431381, u'ted_spread_p0': 2.51233917448}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 3.69381353544e-06, u'cap_rate_p0': 0.128373725201, u'ted_spread_p0': 3.88957741361e-08}&lt;/td&gt;
      &lt;td&gt;1.283737e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 2.51413269277, u'cap_rate_p0': 1.99928074526, u'ted_spread_p0': 1.52794519095}&lt;/td&gt;
      &lt;td&gt;2.514133&lt;/td&gt;
      &lt;td&gt;0.509631&lt;/td&gt;
      &lt;td&gt;0.434051&lt;/td&gt;
      &lt;td&gt;0.714011&lt;/td&gt;
      &lt;td&gt;0.799561&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 crea_hp_yoy_p3 bbb_spread_p4&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -7.124071928, u'const': -1.81674640673, u'bbb_spread_p4': 0.402048103596, u'ted_spread_p0': 3.28772335045}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 7.40618262123e-07, u'bbb_spread_p4': 1.99672792352e-05, u'ted_spread_p0': 4.0450297686e-12}&lt;/td&gt;
      &lt;td&gt;1.996728e-05&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.43640456697, u'bbb_spread_p4': 1.55605617691, u'ted_spread_p0': 1.46835089361}&lt;/td&gt;
      &lt;td&gt;1.556056&lt;/td&gt;
      &lt;td&gt;0.508998&lt;/td&gt;
      &lt;td&gt;0.470157&lt;/td&gt;
      &lt;td&gt;0.714472&lt;/td&gt;
      &lt;td&gt;0.773637&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 crea_hp_yoy_p2 realgpd_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p3': -0.311113715762, u'crea_hp_yoy_p2': -9.309071086, u'const': -0.103457190105, u'ted_spread_p0': 2.84913168832}&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p3': 1.92976213253e-10, u'crea_hp_yoy_p2': 5.13571840644e-10, u'ted_spread_p0': 6.5891420931e-11}&lt;/td&gt;
      &lt;td&gt;5.135718e-10&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p3': 1.19851914035, u'crea_hp_yoy_p2': 1.30632930877, u'ted_spread_p0': 1.28561708211}&lt;/td&gt;
      &lt;td&gt;1.306329&lt;/td&gt;
      &lt;td&gt;0.508293&lt;/td&gt;
      &lt;td&gt;0.518727&lt;/td&gt;
      &lt;td&gt;0.714985&lt;/td&gt;
      &lt;td&gt;0.737325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 spindex_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.525123157535, u'spindex_yoy_p3': -0.0106469798442, u'const': -2.29476615365, u'ted_spread_p0': 2.80601709969}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 3.20465565732e-05, u'spindex_yoy_p3': 0.152675572385, u'ted_spread_p0': 7.93900361033e-11}&lt;/td&gt;
      &lt;td&gt;1.526756e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 3.02253841133, u'spindex_yoy_p3': 2.82820562708, u'ted_spread_p0': 1.26016903124}&lt;/td&gt;
      &lt;td&gt;3.022538&lt;/td&gt;
      &lt;td&gt;0.508117&lt;/td&gt;
      &lt;td&gt;0.429300&lt;/td&gt;
      &lt;td&gt;0.715113&lt;/td&gt;
      &lt;td&gt;0.802911&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p0 ted_spread_p0 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p0': -0.0822791822097, u'const': -2.17908597925, u'bbb_spread_p3': 0.540487885674, u'ted_spread_p0': 2.63164243642}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 5.73922047576e-06, u'realgpd_yoy_p0': 0.153193960743, u'ted_spread_p0': 1.95668256146e-09}&lt;/td&gt;
      &lt;td&gt;1.531940e-01&lt;/td&gt;
      &lt;td&gt;{u'realgpd_yoy_p0': 2.14853765757, u'bbb_spread_p3': 2.63888992404, u'ted_spread_p0': 1.35238585586}&lt;/td&gt;
      &lt;td&gt;2.638890&lt;/td&gt;
      &lt;td&gt;0.508087&lt;/td&gt;
      &lt;td&gt;0.458403&lt;/td&gt;
      &lt;td&gt;0.715134&lt;/td&gt;
      &lt;td&gt;0.782170&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 cap_rate_p4&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.615821291858, u'cap_rate_p4': 0.208708505931, u'const': -3.8335303104, u'ted_spread_p0': 2.74915454136}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 6.60288226921e-11, u'cap_rate_p4': 0.166847470536, u'ted_spread_p0': 1.67384077652e-10}&lt;/td&gt;
      &lt;td&gt;1.668475e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 1.44211057025, u'cap_rate_p4': 1.16255484012, u'ted_spread_p0': 1.26230863366}&lt;/td&gt;
      &lt;td&gt;1.442111&lt;/td&gt;
      &lt;td&gt;0.507353&lt;/td&gt;
      &lt;td&gt;0.402521&lt;/td&gt;
      &lt;td&gt;0.715668&lt;/td&gt;
      &lt;td&gt;0.821532&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 crea_hp_yoy_p3 realgpd_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': -8.76409349446, u'realgpd_yoy_p3': -0.180879352626, u'const': -0.318947340435, u'ted_spread_p0': 2.86354149694}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 5.84134461416e-10, u'realgpd_yoy_p3': 2.43206207395e-05, u'ted_spread_p0': 6.41362452225e-11}&lt;/td&gt;
      &lt;td&gt;2.432062e-05&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p3': 1.26977569699, u'realgpd_yoy_p3': 1.05558827276, u'ted_spread_p0': 1.29286084071}&lt;/td&gt;
      &lt;td&gt;1.292861&lt;/td&gt;
      &lt;td&gt;0.506863&lt;/td&gt;
      &lt;td&gt;0.458546&lt;/td&gt;
      &lt;td&gt;0.716024&lt;/td&gt;
      &lt;td&gt;0.782067&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 cap_rate_p2 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.225756509972, u'const': -3.78840359933, u'bbb_spread_p3': 0.570654122306, u'ted_spread_p0': 2.67079906481}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 0.193180231382, u'bbb_spread_p3': 2.30746281702e-07, u'ted_spread_p0': 9.08484906408e-10}&lt;/td&gt;
      &lt;td&gt;1.931802e-01&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p2': 1.75920678376, u'bbb_spread_p3': 2.17026703782, u'ted_spread_p0': 1.31977926124}&lt;/td&gt;
      &lt;td&gt;2.170267&lt;/td&gt;
      &lt;td&gt;0.506110&lt;/td&gt;
      &lt;td&gt;0.411367&lt;/td&gt;
      &lt;td&gt;0.716570&lt;/td&gt;
      &lt;td&gt;0.815428&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 realgpd_yoy_p2 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': -0.00960849919163, u'const': -0.407772979121, u'realgpd_yoy_p2': -0.257333465025, u'ted_spread_p0': 2.29032216488}&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 0.000448333783238, u'realgpd_yoy_p2': 4.97703208541e-08, u'ted_spread_p0': 7.49817339718e-09}&lt;/td&gt;
      &lt;td&gt;4.483338e-04&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 1.23841713376, u'realgpd_yoy_p2': 1.22668430577, u'ted_spread_p0': 1.11609747523}&lt;/td&gt;
      &lt;td&gt;1.238417&lt;/td&gt;
      &lt;td&gt;0.505281&lt;/td&gt;
      &lt;td&gt;0.321448&lt;/td&gt;
      &lt;td&gt;0.717171&lt;/td&gt;
      &lt;td&gt;0.875497&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 cap_rate_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.201637275061, u'bbb_spread_p3': 0.593429505205, u'const': -3.71870866222, u'ted_spread_p0': 2.71903806154}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.217238393496, u'bbb_spread_p3': 7.84889309862e-09, u'ted_spread_p0': 3.23824482993e-10}&lt;/td&gt;
      &lt;td&gt;2.172384e-01&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.4724277304, u'bbb_spread_p3': 1.80756321736, u'ted_spread_p0': 1.27957899111}&lt;/td&gt;
      &lt;td&gt;1.807563&lt;/td&gt;
      &lt;td&gt;0.505135&lt;/td&gt;
      &lt;td&gt;0.405297&lt;/td&gt;
      &lt;td&gt;0.717277&lt;/td&gt;
      &lt;td&gt;0.819622&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 spindex_yoy_p3 cap_rate_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.594228769802, u'spindex_yoy_p3': -0.031124114991, u'const': -4.49069384587, u'ted_spread_p0': 2.48354562619}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 4.50370245447e-05, u'spindex_yoy_p3': 8.36225082073e-09, u'ted_spread_p0': 1.64539790007e-09}&lt;/td&gt;
      &lt;td&gt;4.503702e-05&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.06865169504, u'spindex_yoy_p3': 1.22753770126, u'ted_spread_p0': 1.18199596216}&lt;/td&gt;
      &lt;td&gt;1.227538&lt;/td&gt;
      &lt;td&gt;0.504430&lt;/td&gt;
      &lt;td&gt;0.408088&lt;/td&gt;
      &lt;td&gt;0.717788&lt;/td&gt;
      &lt;td&gt;0.817696&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 realgpd_yoy_p2 bbb_spread_p3&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.517442462306, u'const': -2.1307227209, u'realgpd_yoy_p2': -0.0864549354609, u'ted_spread_p0': 2.66817475933}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.000504611426207, u'realgpd_yoy_p2': 0.249265257706, u'ted_spread_p0': 1.27380241683e-09}&lt;/td&gt;
      &lt;td&gt;2.492653e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 4.29026413402, u'realgpd_yoy_p2': 3.6616378528, u'ted_spread_p0': 1.34053982558}&lt;/td&gt;
      &lt;td&gt;4.290264&lt;/td&gt;
      &lt;td&gt;0.504018&lt;/td&gt;
      &lt;td&gt;0.388464&lt;/td&gt;
      &lt;td&gt;0.718086&lt;/td&gt;
      &lt;td&gt;0.831140&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 spindex_yoy_p3&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': -0.00697418929236, u'const': -0.358295064272, u'realgpd_yoy_p1': -0.287950006036, u'ted_spread_p0': 2.24832163495}&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': 0.396355057686, u'realgpd_yoy_p1': 4.80031372299e-05, u'ted_spread_p0': 2.97028063227e-08}&lt;/td&gt;
      &lt;td&gt;3.963551e-01&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p3': 3.44623710801, u'realgpd_yoy_p1': 3.17299042455, u'ted_spread_p0': 1.18476784624}&lt;/td&gt;
      &lt;td&gt;3.446237&lt;/td&gt;
      &lt;td&gt;0.503737&lt;/td&gt;
      &lt;td&gt;0.457973&lt;/td&gt;
      &lt;td&gt;0.718290&lt;/td&gt;
      &lt;td&gt;0.782481&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 crea_hp_yoy_p2&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': -1.0795826604, u'const': -0.272715216966, u'realgpd_yoy_p1': -0.318271064261, u'ted_spread_p0': 2.22651917837}&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 0.450477095771, u'realgpd_yoy_p1': 3.13546780517e-10, u'ted_spread_p0': 2.90322487555e-08}&lt;/td&gt;
      &lt;td&gt;4.504771e-01&lt;/td&gt;
      &lt;td&gt;{u'crea_hp_yoy_p2': 1.48055472983, u'realgpd_yoy_p1': 1.40383603833, u'ted_spread_p0': 1.15790564414}&lt;/td&gt;
      &lt;td&gt;1.480555&lt;/td&gt;
      &lt;td&gt;0.502876&lt;/td&gt;
      &lt;td&gt;0.479863&lt;/td&gt;
      &lt;td&gt;0.718912&lt;/td&gt;
      &lt;td&gt;0.766517&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 spindex_yoy_p4&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p4': -0.00287623611266, u'const': -0.307424934206, u'realgpd_yoy_p1': -0.319272637088, u'ted_spread_p0': 2.2027800319}&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p4': 0.630438613957, u'realgpd_yoy_p1': 1.06155602716e-08, u'ted_spread_p0': 4.3262669625e-08}&lt;/td&gt;
      &lt;td&gt;6.304386e-01&lt;/td&gt;
      &lt;td&gt;{u'spindex_yoy_p4': 1.90330721432, u'realgpd_yoy_p1': 1.77597233856, u'ted_spread_p0': 1.16353394534}&lt;/td&gt;
      &lt;td&gt;1.903307&lt;/td&gt;
      &lt;td&gt;0.500930&lt;/td&gt;
      &lt;td&gt;0.450725&lt;/td&gt;
      &lt;td&gt;0.720318&lt;/td&gt;
      &lt;td&gt;0.787695&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;ted_spread_p0 bbb_spread_p3 spindex_yoy_p2&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 0.601012948771, u'spindex_yoy_p2': -0.00542467267777, u'const': -2.49819167847, u'ted_spread_p0': 2.8074946861}&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 5.53590432482e-08, u'spindex_yoy_p2': 0.387126006585, u'ted_spread_p0': 1.07989092515e-10}&lt;/td&gt;
      &lt;td&gt;3.871260e-01&lt;/td&gt;
      &lt;td&gt;{u'bbb_spread_p3': 2.12232941161, u'spindex_yoy_p2': 1.94673426734, u'ted_spread_p0': 1.26539793559}&lt;/td&gt;
      &lt;td&gt;2.122329&lt;/td&gt;
      &lt;td&gt;0.500684&lt;/td&gt;
      &lt;td&gt;0.429745&lt;/td&gt;
      &lt;td&gt;0.720496&lt;/td&gt;
      &lt;td&gt;0.802598&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p1 ted_spread_p0 corp_profit_p1&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': -0.00111510712547, u'const': -0.295544468835, u'realgpd_yoy_p1': -0.322134804647, u'ted_spread_p0': 2.16984747298}&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 0.746943635339, u'realgpd_yoy_p1': 7.85857215941e-08, u'ted_spread_p0': 3.07526883363e-08}&lt;/td&gt;
      &lt;td&gt;7.469436e-01&lt;/td&gt;
      &lt;td&gt;{u'corp_profit_p1': 2.09799265347, u'realgpd_yoy_p1': 2.09768958589, u'ted_spread_p0': 1.09854672985}&lt;/td&gt;
      &lt;td&gt;2.097993&lt;/td&gt;
      &lt;td&gt;0.500194&lt;/td&gt;
      &lt;td&gt;0.446912&lt;/td&gt;
      &lt;td&gt;0.720849&lt;/td&gt;
      &lt;td&gt;0.790424&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;realgpd_yoy_p0 ted_spread_p0 cap_rate_p3&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 0.639727648052, u'realgpd_yoy_p0': -0.253032450789, u'const': -4.28034401961, u'ted_spread_p0': 1.90847482334}&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.18825887769e-05, u'realgpd_yoy_p0': 1.23019019096e-08, u'ted_spread_p0': 3.86554982571e-07}&lt;/td&gt;
      &lt;td&gt;1.188259e-05&lt;/td&gt;
      &lt;td&gt;{u'cap_rate_p3': 1.04954092806, u'realgpd_yoy_p0': 1.04901230219, u'ted_spread_p0': 1.04189905245}&lt;/td&gt;
      &lt;td&gt;1.049541&lt;/td&gt;
      &lt;td&gt;0.500114&lt;/td&gt;
      &lt;td&gt;0.505297&lt;/td&gt;
      &lt;td&gt;0.720907&lt;/td&gt;
      &lt;td&gt;0.747542&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class="language-python"&gt;all_combination
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;(5, 14, 18)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;best_reg_result.summary()
&lt;/code&gt;&lt;/pre&gt;
&lt;table class="simpletable"&gt;
&lt;caption&gt;OLS Regression Results&lt;/caption&gt;
&lt;tr&gt;
  &lt;th&gt;Dep. Variable:&lt;/th&gt;       &lt;td&gt;cum_pd_num&lt;/td&gt;    &lt;th&gt;  R-squared:         &lt;/th&gt; &lt;td&gt;   0.615&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Model:&lt;/th&gt;                   &lt;td&gt;OLS&lt;/td&gt;       &lt;th&gt;  Adj. R-squared:    &lt;/th&gt; &lt;td&gt;   0.601&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Method:&lt;/th&gt;             &lt;td&gt;Least Squares&lt;/td&gt;  &lt;th&gt;  F-statistic:       &lt;/th&gt; &lt;td&gt;   46.24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Date:&lt;/th&gt;             &lt;td&gt;Mon, 17 Apr 2017&lt;/td&gt; &lt;th&gt;  Prob (F-statistic):&lt;/th&gt; &lt;td&gt;5.77e-18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Time:&lt;/th&gt;                 &lt;td&gt;21:56:19&lt;/td&gt;     &lt;th&gt;  Log-Likelihood:    &lt;/th&gt; &lt;td&gt; -85.467&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;No. Observations:&lt;/th&gt;      &lt;td&gt;    91&lt;/td&gt;      &lt;th&gt;  AIC:               &lt;/th&gt; &lt;td&gt;   178.9&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Residuals:&lt;/th&gt;          &lt;td&gt;    87&lt;/td&gt;      &lt;th&gt;  BIC:               &lt;/th&gt; &lt;td&gt;   189.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Df Model:&lt;/th&gt;              &lt;td&gt;     3&lt;/td&gt;      &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Covariance Type:&lt;/th&gt;      &lt;td&gt;nonrobust&lt;/td&gt;    &lt;th&gt;                     &lt;/th&gt;     &lt;td&gt; &lt;/td&gt;   
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class="simpletable"&gt;
&lt;tr&gt;
         &lt;td&gt;&lt;/td&gt;           &lt;th&gt;coef&lt;/th&gt;     &lt;th&gt;std err&lt;/th&gt;      &lt;th&gt;t&lt;/th&gt;      &lt;th&gt;P&gt;|t|&lt;/th&gt; &lt;th&gt;[95.0% Conf. Int.]&lt;/th&gt; 
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;const&lt;/th&gt;          &lt;td&gt;   -5.8248&lt;/td&gt; &lt;td&gt;    0.770&lt;/td&gt; &lt;td&gt;   -7.569&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;   -7.354    -4.295&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;ted_spread_p0&lt;/th&gt;  &lt;td&gt;    2.9978&lt;/td&gt; &lt;td&gt;    0.340&lt;/td&gt; &lt;td&gt;    8.805&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    2.321     3.675&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;cap_rate_p3&lt;/th&gt;    &lt;td&gt;    0.8444&lt;/td&gt; &lt;td&gt;    0.120&lt;/td&gt; &lt;td&gt;    7.055&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;    0.607     1.082&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;crea_hp_yoy_p3&lt;/th&gt; &lt;td&gt;   -9.7555&lt;/td&gt; &lt;td&gt;    1.111&lt;/td&gt; &lt;td&gt;   -8.784&lt;/td&gt; &lt;td&gt; 0.000&lt;/td&gt; &lt;td&gt;  -11.963    -7.548&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;table class="simpletable"&gt;
&lt;tr&gt;
  &lt;th&gt;Omnibus:&lt;/th&gt;       &lt;td&gt; 3.899&lt;/td&gt; &lt;th&gt;  Durbin-Watson:     &lt;/th&gt; &lt;td&gt;   2.125&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Prob(Omnibus):&lt;/th&gt; &lt;td&gt; 0.142&lt;/td&gt; &lt;th&gt;  Jarque-Bera (JB):  &lt;/th&gt; &lt;td&gt;   3.893&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Skew:&lt;/th&gt;          &lt;td&gt; 0.210&lt;/td&gt; &lt;th&gt;  Prob(JB):          &lt;/th&gt; &lt;td&gt;   0.143&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;th&gt;Kurtosis:&lt;/th&gt;      &lt;td&gt; 3.922&lt;/td&gt; &lt;th&gt;  Cond. No.          &lt;/th&gt; &lt;td&gt;    107.&lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;</content><category term="python"></category><category term="python"></category><category term="statsmodels"></category></entry><entry><title>jupyter and pandas display</title><link href="/pages/2017/04/02/jupyter-and-pandas-display/" rel="alternate"></link><published>2017-04-02T00:00:00-05:00</published><updated>2017-04-02T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-04-02:/pages/2017/04/02/jupyter-and-pandas-display/</id><summary type="html">&lt;p&gt;tips on ipython display and pandas options&lt;/p&gt;</summary><content type="html">&lt;h3&gt;1. show all the rows or columns from a DataFrame in Jupyter QTConcole&lt;/h3&gt;
&lt;p&gt;if the df has a lot of rows or columns, then when you try to show the df, pandas will auto detect the size of the displaying area and automatically hide some part of the data by replacing with &lt;code&gt;...&lt;/code&gt;. To show the full data without any hiding, you can use &lt;code&gt;pd.set_option('display.max_rows', 500)&lt;/code&gt; and &lt;code&gt;pd.set_option('display.max_rows', 500)&lt;/code&gt; to change the max number of rows or max number of columns to display.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.random(200).reshape(2, 100))
df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170402_01.PNG"&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;# set up display area to show dataframe in jupyter qtconsole

pd.set_option('display.height', 1000)
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

df
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170402_02.PNG"&gt;&lt;/p&gt;
&lt;h3&gt;2. display all text in a cell without truncation&lt;/h3&gt;
&lt;p&gt;pandas will automatically truncate the long string to display by default. Taking the example below, the string_x is long so by default it will not display the full string. However the full text is wanted. &lt;code&gt;pd.set_option('display.max_colwidth', -1)&lt;/code&gt; will help to show all the text strings in the column.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;string_x = &amp;quot;if the df has a lot of rows or columns, then when you try to show the df, pandas will auto detect \
the size of the displaying area and automatically hide some part of the data by replacing with&amp;quot;

pd.DataFrame({'string_x': string_x}, index = [0])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;string_x&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;if the df has a lot of rows or columns, then w...&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class="language-python"&gt;pd.options.display.max_rows
pd.set_option('display.max_colwidth', -1)

pd.DataFrame({'string_x': string_x}, index = [0])
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;string_x&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;if the df has a lot of rows or columns, then when you try to show the df, pandas will auto detect the size of the displaying area and automatically hide some part of the data by replacing with&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3&gt;3. display html in Jupyter&lt;/h3&gt;
&lt;p&gt;If the dataframe is shown in html, then it is easier to copy the splited data to excel. Otherwise data copied to excel will be in one row. To display in html in jupyter, use &lt;code&gt;from IPython.display import display, HTML&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from IPython.display import display, HTML
display(HTML(' &amp;lt;span style=&amp;quot;color:red&amp;quot;&amp;gt;the title is: &amp;lt;h1&amp;gt;Hello, world!&amp;lt;/h1&amp;gt; &amp;lt;/span&amp;gt;  '))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;span style="color:red"&gt;the title is: &lt;h1&gt;Hello, world!&lt;/h1&gt; &lt;/span&gt;  &lt;/p&gt;
&lt;h3&gt;4. jupyter run magic command&lt;/h3&gt;
&lt;p&gt;Jupyter has some magic command making your job easier. Some useful commands like &lt;code&gt;%lsmagic %env %ls %pwd %cd&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;%lsmagic
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;Available line magics:
%alias  %alias_magic  %autocall  %automagic  %autosave  %bookmark  %cat  %cd  %clear  %colors  %config  %connect_info  %cp  %debug  %dhist  %dirs  %doctest_mode  %ed  %edit  %env  %gui  %hist  %history  %killbgscripts  %ldir  %less  %lf  %lk  %ll  %load  %load_ext  %loadpy  %logoff  %logon  %logstart  %logstate  %logstop  %ls  %lsmagic  %lx  %macro  %magic  %man  %matplotlib  %mkdir  %more  %mv  %notebook  %page  %pastebin  %pdb  %pdef  %pdoc  %pfile  %pinfo  %pinfo2  %popd  %pprint  %precision  %profile  %prun  %psearch  %psource  %pushd  %pwd  %pycat  %pylab  %qtconsole  %quickref  %recall  %rehashx  %reload_ext  %rep  %rerun  %reset  %reset_selective  %rm  %rmdir  %run  %save  %sc  %set_env  %store  %sx  %system  %tb  %time  %timeit  %unalias  %unload_ext  %who  %who_ls  %whos  %xdel  %xmode

Available cell magics:
%%!  %%HTML  %%SVG  %%bash  %%capture  %%debug  %%file  %%html  %%javascript  %%js  %%latex  %%perl  %%prun  %%pypy  %%python  %%python2  %%python3  %%ruby  %%script  %%sh  %%svg  %%sx  %%system  %%time  %%timeit  %%writefile

Automagic is ON, % prefix IS NOT needed for line magics.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;5. run shell command&lt;/h3&gt;
&lt;p&gt;jupyter also provides the facility to run linux shell command in the way &lt;code&gt;!shall_command&lt;/code&gt;. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;! ls
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;20160625_deploy_flask_on_heroku.txt
20170318_pandas_multiple_agg_summary.ipynb
20170402_dl_notes.ipynb
20170402_jupyter_pandas_display.ipynb
build_by_pelican.md
figures
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;6. list function definiton&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;??&lt;/code&gt; gives the facility to check the difinicion of a function. &lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;??pd.merge
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;7. download the server files&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;FileLink&lt;/code&gt; will return a link to the file on your remote server in your browser. You can then click on it to download the file.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;from IPython.display import FileLink
FileLink('build_by_pelican.md')
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;a href='build_by_pelican.md' target='_blank'&gt;build_by_pelican.md&lt;/a&gt;&lt;br&gt;&lt;/p&gt;
&lt;h2&gt;Reference&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://stackoverflow.com/questions/11707586/python-pandas-how-to-widen-output-display-to-see-more-columns"&gt;Python pandas, how to widen output display to see more columns?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.dominodatalab.com/lesser-known-ways-of-using-notebooks/"&gt;Advanced Jupyter Notebook Tricks — Part I&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="https://blog.dominodatalab.com/interactive-dashboards-in-jupyter/"&gt;Advanced Jupyter Notebook Tricks — Part II&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://pynash.org/2013/03/06/timing-and-profiling/"&gt;Timing and Profiling in IPython&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="pandas"></category></entry><entry><title>weighted avarage, aggrefated function with apply and agg</title><link href="/pages/2017/03/18/weighted-avarage-aggrefated-function-with-apply-and-agg/" rel="alternate"></link><published>2017-03-18T00:00:00-05:00</published><updated>2017-03-18T00:00:00-05:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-03-18:/pages/2017/03/18/weighted-avarage-aggrefated-function-with-apply-and-agg/</id><summary type="html">&lt;p&gt;suppose we want to calculate the weighted average probability of default of all risk rating weighted by the number of borrowers on each pd risk rating(or a little more, grouped by industry or portfolio), we need this  weighted average function on each group. &lt;code&gt;Pandas&lt;/code&gt; has &lt;code&gt;groupby&lt;/code&gt; to split data, and then apply function to calculate and summarize.&lt;/p&gt;</summary><content type="html">&lt;pre&gt;&lt;code class="language-python"&gt;import numpy as np
import pandas as pd
from IPython.display import display
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;1. weighted average&lt;/h3&gt;
&lt;p&gt;Let's denote &lt;code&gt;x = [x_1, ..., x_n]&lt;/code&gt;. The weight &lt;code&gt;w&lt;/code&gt; is denoted as &lt;code&gt;w = [w_1, ..., w_n]&lt;/code&gt;. The weighted average of &lt;code&gt;x&lt;/code&gt; by &lt;code&gt;w&lt;/code&gt; is &lt;span class="math"&gt;\(\frac{ \sum_{i=1}^{n} x_i * w_i } { \sum_{i=1}^{n} w_i}\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;numpy&lt;/code&gt; provides a function called &lt;code&gt;np.average()&lt;/code&gt; to calculate the weighted average. An example of calculate by hand and by the &lt;code&gt;np.average&lt;/code&gt; is given below:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
df = pd.DataFrame(np.random.random(20).reshape(10, 2), columns = ['val1', 'wt'])

# if we want to calcuate the weighted average of val1 weighted by wt, that is:
sum(df.val1 * df.wt) / sum(df.wt)    # 0.42105022244260254

# np.average() function
np.average(df.val1, weights = df.wt, axis = 0)    # 0.42105022244260254
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.42105022244260254
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;2. calculate weighted average within each group&lt;/h3&gt;
&lt;p&gt;The above example is very simple. A more common situation is there are different groups, and we need to calculate the weighted average within each group. The following is an example:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;there are two groups, called 'id'&lt;/li&gt;
&lt;li&gt;we want to calculate the weighted average for data in group 1(id == 1) and group 2(id == 2)&lt;/li&gt;
&lt;li&gt;calculate the weighted average of var1 and var2 by wt in group 1, and group 2 seperately&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;so,  &lt;code&gt;0.339688030253 = sum(df1.val1 * df1.wt) / df1.wt.sum()&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
df = pd.DataFrame(np.random.random(20).reshape(10, 2), columns = ['val1', 'val2'])
df['id'] = np.repeat([1, 2], 5)
df['wt'] = [1, 2] * 5

print df

print '\n' + '-'*20 + ' weighted average of val1 and val2 by wt in each group ' + '-'*20

wtavg = lambda x: np.average(x.ix[:, :2], weights = x.wt, axis = 0)
dfwavg = df.groupby('id').apply(wtavg)
display(dfwavg)
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;       val1      val2  id  wt
0  0.823389  0.218603   1   1
1  0.031461  0.545926   1   2
2  0.154456  0.186963   1   1
3  0.422348  0.605552   1   2
4  0.492354  0.644119   1   1
5  0.913049  0.771104   2   2
6  0.288361  0.415441   2   1
7  0.331490  0.366327   2   2
8  0.312195  0.580446   2   1
9  0.112952  0.325875   2   2

-------------------- weighted average of val1 and val2 by wt in each group --------------------



id
1    [0.339688030253, 0.478948919665]
2     [0.41444248036, 0.490312314239]
dtype: object
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;## verified by manual calculation
df1 = df.query('id == 1')
sum(df1.val1 * df1.wt) / df1.wt.sum()    ## 0.33968803025301908
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;0.33968803025301908
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Difference between &lt;code&gt;apply&lt;/code&gt; and &lt;code&gt;agg&lt;/code&gt;: &lt;code&gt;apply&lt;/code&gt; will apply the funciton on the data frame of each group, while &lt;code&gt;agg&lt;/code&gt; will aggregate each column of each group. So the arguments in the apply function is a dataframe.&lt;/p&gt;
&lt;p&gt;The following is an example from pandas docs. The arguments in function f0 is a dataframe in each id group.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;def f0(group): 
    return pd.DataFrame({'original': group, 'group - mean': group - group.mean()})

df.groupby('id')['val1'].apply(f0)
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;group - mean&lt;/th&gt;
      &lt;th&gt;original&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;0.438588&lt;/td&gt;
      &lt;td&gt;0.823389&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;-0.353340&lt;/td&gt;
      &lt;td&gt;0.031461&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;-0.230346&lt;/td&gt;
      &lt;td&gt;0.154456&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.037546&lt;/td&gt;
      &lt;td&gt;0.422348&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.107553&lt;/td&gt;
      &lt;td&gt;0.492354&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;0.521440&lt;/td&gt;
      &lt;td&gt;0.913049&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;-0.103248&lt;/td&gt;
      &lt;td&gt;0.288361&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;-0.060119&lt;/td&gt;
      &lt;td&gt;0.331490&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;-0.079414&lt;/td&gt;
      &lt;td&gt;0.312195&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;-0.278658&lt;/td&gt;
      &lt;td&gt;0.112952&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3&gt;multiple functions&lt;/h3&gt;
&lt;h4&gt;1. different function for different column&lt;/h4&gt;
&lt;p&gt;One condition is you want to apply different function on different columns in the dataframe. For example, you want to apply sum on one column, and stdev on another column. &lt;/p&gt;
&lt;p&gt;if you want to apply multiple functions to aggregate, then you need to put them in the &lt;strong&gt;list&lt;/strong&gt; or &lt;strong&gt;dict&lt;/strong&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
snapdf = pd.DataFrame(np.random.random(3000).reshape(-1, 3), \
           columns = ['wt_var', 'var_to_sum', &amp;quot;var_to_stdev&amp;quot;])
snapdf['stage'] = np.repeat([1, 2], 500)

f = {&amp;quot;var_to_sum&amp;quot;: np.sum, &amp;quot;var_to_stdev&amp;quot;: np.std}
stage_s1 = snapdf.ix[:, ['stage', 'var_to_sum', 'var_to_stdev']].groupby('stage').agg(f).reset_index()

stage_s1
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;stage&lt;/th&gt;
      &lt;th&gt;var_to_sum&lt;/th&gt;
      &lt;th&gt;var_to_stdev&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;245.666111&lt;/td&gt;
      &lt;td&gt;0.288775&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;247.715539&lt;/td&gt;
      &lt;td&gt;0.284739&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h4&gt;2. multiple functions on one column&lt;/h4&gt;
&lt;p&gt;Another condition is you want to apply multiple functions on each column. For example, you want to calcualte both mean and stdev for eachc column. Then your function is a dict of dicts: first key is the column(variable) to be applied by the functions. second key is the functions.&lt;/p&gt;
&lt;p&gt;An example is like:&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
snapdf = pd.DataFrame(np.random.random(3000).reshape(-1, 3), \
           columns = ['wt_var', 'var1', &amp;quot;var2&amp;quot;])
snapdf['stage'] = np.repeat([1, 2], 500)

snapdf.groupby('stage').agg({np.sum, np.mean, np.std})
&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;wt_var&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;var1&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;var2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sum&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sum&lt;/th&gt;
      &lt;th&gt;std&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;sum&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;stage&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.27400&lt;/td&gt;
      &lt;td&gt;0.484682&lt;/td&gt;
      &lt;td&gt;242.341116&lt;/td&gt;
      &lt;td&gt;0.287491&lt;/td&gt;
      &lt;td&gt;0.491332&lt;/td&gt;
      &lt;td&gt;245.666111&lt;/td&gt;
      &lt;td&gt;0.288775&lt;/td&gt;
      &lt;td&gt;0.506056&lt;/td&gt;
      &lt;td&gt;253.028070&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.29702&lt;/td&gt;
      &lt;td&gt;0.507317&lt;/td&gt;
      &lt;td&gt;253.658370&lt;/td&gt;
      &lt;td&gt;0.282293&lt;/td&gt;
      &lt;td&gt;0.495431&lt;/td&gt;
      &lt;td&gt;247.715539&lt;/td&gt;
      &lt;td&gt;0.284739&lt;/td&gt;
      &lt;td&gt;0.501093&lt;/td&gt;
      &lt;td&gt;250.546638&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
snapdf = pd.DataFrame(np.random.random(3000).reshape(-1, 3), \
           columns = ['wt_var', 'var_to_wt_avg_1', 'var_to_wt_avg_2'])
snapdf['stage'] = np.repeat([1, 2], 500)

wtavg = lambda x: np.average(x, weights = snapdf.loc[x.index, &amp;quot;wt_var&amp;quot;])

f2 = {'var_to_wt_avg_1': {'mean':'mean', 'wt_avg': wtavg}, 'var_to_wt_avg_2':{'mean':'mean', 'wt_avg':wtavg}}
stage_s2 = snapdf.ix[:, ['stage', 'var_to_wt_avg_1', 'var_to_wt_avg_2']].groupby('stage').agg(f2).reset_index()

# stage_s2.columns = stage_s2.columns.droplevel(0)

stage_s2
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170320_pandas_agg_dict.PNG"&gt;&lt;/p&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;stage&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;var_to_wt_avg_2&lt;/th&gt;
      &lt;th colspan="2" halign="left"&gt;var_to_wt_avg_1&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;wt_avg&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
      &lt;th&gt;wt_avg&lt;/th&gt;
      &lt;th&gt;mean&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.498908&lt;/td&gt;
      &lt;td&gt;0.506056&lt;/td&gt;
      &lt;td&gt;0.486139&lt;/td&gt;
      &lt;td&gt;0.491332&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.517657&lt;/td&gt;
      &lt;td&gt;0.501093&lt;/td&gt;
      &lt;td&gt;0.499796&lt;/td&gt;
      &lt;td&gt;0.495431&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;h3&gt;3. weighted from stacked data: unstack data&lt;/h3&gt;
&lt;p&gt;Data in the previous are stored in horizontial way: variables are in different columns. But sometimes data are in vertical way: in the following example, the 3 ratings(I/S/P) are stacked in vertical  line. We want to get the weighted average of I/S/P. So, the first step is to unstak the data. Then calculate the weighted average.&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;np.random.seed(9999)
df = pd.DataFrame(np.random.random(72).reshape(36, 2), columns = [&amp;quot;var1&amp;quot;, &amp;quot;var2&amp;quot;])
df[&amp;quot;rating&amp;quot;] = np.repeat(['I', 'S', 'P'], 12)
df['qtrs'] = range(1, 13) * 3

df = df.set_index(['rating', 'qtrs']).unstack(level = 0)
df['cnts', 'I'] = 100
df['cnts', 'P'] = 200
df['cnts', &amp;quot;S&amp;quot;] = 300

display(df.head())

df['wt_var_1'] = np.average(df.var1, weights = df.cnts, axis = 1)
df['wt_var_2'] = np.average(df.var2, weights = df.cnts, axis = 1)

df.reset_index().ix[:, [&amp;quot;qtrs&amp;quot;, &amp;quot;wt_var_1&amp;quot;, &amp;quot;wt_var_2&amp;quot;]]

&lt;/code&gt;&lt;/pre&gt;
&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;var1&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;var2&lt;/th&gt;
      &lt;th colspan="3" halign="left"&gt;cnts&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;rating&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;P&lt;/th&gt;
      &lt;th&gt;S&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;P&lt;/th&gt;
      &lt;th&gt;S&lt;/th&gt;
      &lt;th&gt;I&lt;/th&gt;
      &lt;th&gt;P&lt;/th&gt;
      &lt;th&gt;S&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;qtrs&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;0.823389&lt;/td&gt;
      &lt;td&gt;0.154764&lt;/td&gt;
      &lt;td&gt;0.602518&lt;/td&gt;
      &lt;td&gt;0.218603&lt;/td&gt;
      &lt;td&gt;0.904529&lt;/td&gt;
      &lt;td&gt;0.435665&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;0.031461&lt;/td&gt;
      &lt;td&gt;0.365481&lt;/td&gt;
      &lt;td&gt;0.472321&lt;/td&gt;
      &lt;td&gt;0.545926&lt;/td&gt;
      &lt;td&gt;0.580417&lt;/td&gt;
      &lt;td&gt;0.866001&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;0.154456&lt;/td&gt;
      &lt;td&gt;0.494557&lt;/td&gt;
      &lt;td&gt;0.537686&lt;/td&gt;
      &lt;td&gt;0.186963&lt;/td&gt;
      &lt;td&gt;0.824609&lt;/td&gt;
      &lt;td&gt;0.484590&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;0.422348&lt;/td&gt;
      &lt;td&gt;0.503091&lt;/td&gt;
      &lt;td&gt;0.333305&lt;/td&gt;
      &lt;td&gt;0.605552&lt;/td&gt;
      &lt;td&gt;0.670854&lt;/td&gt;
      &lt;td&gt;0.582543&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;0.492354&lt;/td&gt;
      &lt;td&gt;0.446044&lt;/td&gt;
      &lt;td&gt;0.679368&lt;/td&gt;
      &lt;td&gt;0.644119&lt;/td&gt;
      &lt;td&gt;0.050341&lt;/td&gt;
      &lt;td&gt;0.544653&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;300&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;div&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;qtrs&lt;/th&gt;
      &lt;th&gt;wt_var_1&lt;/th&gt;
      &lt;th&gt;wt_var_2&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;rating&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.490078&lt;/td&gt;
      &lt;td&gt;0.555776&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0.363231&lt;/td&gt;
      &lt;td&gt;0.717461&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0.459438&lt;/td&gt;
      &lt;td&gt;0.548325&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0.404741&lt;/td&gt;
      &lt;td&gt;0.615815&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;0.570425&lt;/td&gt;
      &lt;td&gt;0.396460&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;/th&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;0.870236&lt;/td&gt;
      &lt;td&gt;0.845366&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;6&lt;/th&gt;
      &lt;td&gt;7&lt;/td&gt;
      &lt;td&gt;0.446611&lt;/td&gt;
      &lt;td&gt;0.123709&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;/th&gt;
      &lt;td&gt;8&lt;/td&gt;
      &lt;td&gt;0.735676&lt;/td&gt;
      &lt;td&gt;0.185098&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;8&lt;/th&gt;
      &lt;td&gt;9&lt;/td&gt;
      &lt;td&gt;0.378114&lt;/td&gt;
      &lt;td&gt;0.435725&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;9&lt;/th&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.533662&lt;/td&gt;
      &lt;td&gt;0.248189&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;10&lt;/th&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;0.529564&lt;/td&gt;
      &lt;td&gt;0.565223&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;11&lt;/th&gt;
      &lt;td&gt;12&lt;/td&gt;
      &lt;td&gt;0.536358&lt;/td&gt;
      &lt;td&gt;0.297243&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;

&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;</content><category term="Python"></category><category term="python"></category><category term="pandas"></category></entry><entry><title>tensorflow简介--04</title><link href="/pages/2017/03/05/tensorflowjian-jie-04/" rel="alternate"></link><published>2017-03-05T10:03:00-06:00</published><updated>2017-03-05T10:03:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-03-05:/pages/2017/03/05/tensorflowjian-jie-04/</id><summary type="html">&lt;p&gt;前面几章我们讨论了给定一些features，比如房屋面积，怎么通过TF的线性回归来预测结果，比如说房屋价格。下面会讨论logistic regression，它会通过输入的features来实现分类。比如说通过输入的图片，来进行0-9的分类。&lt;/p&gt;</summary><content type="html">&lt;h2&gt;Logistic Regression回顾&lt;/h2&gt;
&lt;p&gt;我们已经学习了给定一些features，比如房屋面积，怎么通过TF的线性回归来预测数值型的结果，比如说房屋价格。&lt;/p&gt;
&lt;p&gt;但是有时候我们需要进行分类，而不是预测一个数值。比如：给定一些数字的图片，我们希望能识别出0，1，...9.或者给了一首歌，我们希望能识别这是摇滚音乐，乡村音乐或者其他。这种分类的模型识别的结果我们叫&lt;em&gt;class&lt;/em&gt;.我们可以用TF中的logistic regression来做分类。&lt;/p&gt;
&lt;p&gt;下面我们会讨论用logistic regression来做数字图形识别，也就是分类成0，1，...9.&lt;/p&gt;
&lt;h2&gt;logistic regression详细介绍&lt;/h2&gt;
&lt;p&gt;好消息是我们在前面线性回归提到的好多概念在logistic regression继续适用。我们可以使用改进的&lt;em&gt;y=W.x + b&lt;/em&gt;。 下面我们看看这两个回归的详细比较&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_01.png"&gt;&lt;/p&gt;
&lt;h5&gt;不一样的地方&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;输出的结果(y)：线性回归里面这是一个标量值，比如$50K。在logistic regression里面，这是一个关于类的整数，比如0， 1， 2等等。&lt;/li&gt;
&lt;li&gt;features(x)： 线性回归里面每个feature对应列向量的一个元素。在logistic regression的2-维图像里面，这是一个2维的向量，向量的每个元素表示图片的一个像素。每个像素有一个0-255之间的值，表示对应的灰度：0表示黑色，255表示白色。旁边的值表示不同的灰度。&lt;/li&gt;
&lt;li&gt;损失函数(cost):线性回归里面我们用的真实值和预测值的平方和。logistic regression里面将会是一个预测正确或者错误的函数。&lt;/li&gt;
&lt;/ul&gt;
&lt;h5&gt;相似的地方&lt;/h5&gt;
&lt;ul&gt;
&lt;li&gt;training：两个回归的目的都是找出权重(W)和截距(b， biases)的值&lt;/li&gt;
&lt;li&gt;输出结果:两个回归的结果都是用找出的W和b来预测结果(y)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;协调logistic和linear regression&lt;/h2&gt;
&lt;p&gt;要让&lt;em&gt;y = W.B + x&lt;/em&gt;在ligistic regression也工作，我们需要做一些变换来协调上面提到的不同。&lt;/p&gt;
&lt;h3&gt;Feature (x)的变换&lt;/h3&gt;
&lt;p&gt;我们可以把logistic regression里的二维的图片的features通过这个办法转化为一维的：把图片的像素矩阵的每一行都粘接到第一行的后面。如下图所示&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_02.png"&gt;&lt;/p&gt;
&lt;p&gt;（译注：实际这儿指的是矩阵变成一个行向量，numpy中如果x是一个矩阵，x.flatten（）就得到我们要的结果）&lt;/p&gt;
&lt;h3&gt;y的变换&lt;/h3&gt;
&lt;p&gt;logistic regression中我们不能把y当成标量，因为标量的预测是连续整数，而我们的结果的类是[0, 1, 2， ...]&lt;/p&gt;
&lt;p&gt;要克服这一点，y会被变换为一个列向量（下图为了节省地方，表示成行向量），列向量的每个元素表示logistic regression模型认为结果是那个给定的类的得分。在下面的例子中，预测的结果是类1，因为1的得分最高，为33.&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_03.png"&gt;&lt;/p&gt;
&lt;p&gt;对给定的图片，要得到得分向量，图片的每一个像素，根据它的灰度值，都会被算出一系列的得分（对每个类)用来表示这个像素认为图片被分类到某个类的可能性。对每个类，每个像素的得分的和就是预测向量。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_04.png"&gt;&lt;/p&gt;
&lt;h3&gt;损失函数的变换&lt;/h3&gt;
&lt;p&gt;因为结果不是数值型的值，我们不能用衡量预测值和真实值的距离的那种数值型的损失函数。这种损失函数，对图像‘1’，会认为预测结果为‘7’（7-1=6）比预测结果为‘2’（2-1=1）更错的严重，尽管实际上两个都是错的。&lt;/p&gt;
&lt;p&gt;我们要用的损失函数&lt;em&gt;cross entropy(H)&lt;/em&gt;包含下面几步：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;把真实的图像的类（y‘）转换为一个one-hot向量，这个向量是一个概率分布&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把预测向量（y）转换为一个概率分布&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用cross entropy函数来算出损失，这个损失是两个概率分布的距离&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;stwp 1. one-hot向量&lt;/h4&gt;
&lt;p&gt;因为我们已经把预测值（y）转换为得分向量了，我们需要把真实值（y）也转换为向量。列向量的每一个元素都是0，只有一个元素是1，这个元素就是真实的类所在的那个位置。这就是one-hot向量，下面是0-9的one-hot向量&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_05.png"&gt;&lt;/p&gt;
&lt;p&gt;（译注：其实就是产生了一个dummy向量，sklearn的两个变换，其中一个就是one-hot变换）&lt;/p&gt;
&lt;p&gt;假设真实的（y’）图片为1，那么one-hot向量就是[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], 预测向量 (y) 为 [1.3, 33, 2, 1.2, 3.2, 0.5, 3, 9.2, 1], 把它们画出来比较一下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_06.png"&gt;&lt;/p&gt;
&lt;h4&gt;step 2. 概率分布和softmax&lt;/h4&gt;
&lt;p&gt;要数学化的比较两个图片的相似度，cross entropy是一个很好的比较尺度。（&lt;a href="https://colah.github.io/posts/2015-09-Visual-Information/"&gt;这儿&lt;/a&gt;有一个很好的详细的例子）&lt;/p&gt;
&lt;p&gt;要使用cross entropy，我们需要把真实结果向量（y'）和预测结果向量(y)都转换为概率分布。这儿的概率分布指&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个类的概率值在0和1之间&lt;/li&gt;
&lt;li&gt;对所有类的概率和为1&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;真实结果向量（y'）是one-hot向量，已经满足这两个条件。&lt;/p&gt;
&lt;p&gt;对预测值（y），我们可以用softmax把它转换为概率分布&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_07.png"&gt;&lt;/p&gt;
&lt;p&gt;这是一个简单的两步过程（下面S1 和 S2），预测向量（y）的每个值先取指数值，然后除以指数值的总和。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_08.png"&gt;&lt;/p&gt;
&lt;p&gt;注意softmax（y）的图像跟预测值（y）的图像很相似，只是最大值更大，最小值更小&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_09.png"&gt;&lt;/p&gt;
&lt;h4&gt;setp 3. cross entropy&lt;/h4&gt;
&lt;p&gt;我们现在可以来计算预测向量（y）的概率分布和真实向量（y'）的cross entropy(H)
了。&lt;/p&gt;
&lt;p&gt;cross entropy的公式为&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_10.png"&gt;&lt;/p&gt;
&lt;p&gt;要快速理解这个复杂的公式，我们把它分为三部分。注意下面的公式H中，我们用y_i来表示‘y向量的第i个元素’&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_11.png"&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;蓝色： 真实结果向量，y_i'&lt;/li&gt;
&lt;li&gt;红色： 预测向量的概率分布(softmax(y_i))的log值&lt;/li&gt;
&lt;li&gt;绿色： 对每个图像的分类i，i=0到9，上面红色和蓝色的乘积的和&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面的例子可以用来帮助理解。&lt;/p&gt;
&lt;p&gt;蓝色的部分是ont-hot真实结果(y')向量&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_12.png"&gt;&lt;/p&gt;
&lt;p&gt;红色部分是从预测向量y的推导得到，也就是从softmax(y)到-log(softmax(y))&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_13.png"&gt;&lt;/p&gt;
&lt;p&gt;如果你想进一步了解为什么用-log(softmax(y))来代替softmax(y)，请看这个&lt;a href="https://www.youtube.com/watch?v=F8g_6TXKlxw"&gt;视频&lt;/a&gt; 或者&lt;a href="http://www.slideshare.net/KhorSoonHin/gentlest-introduction-to-tensorflow-part-3"&gt;文件&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;cross entropy(H)，下图中的绿色部分，是蓝色和红色部分的乘积，然后把它们加起来&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_14.png"&gt;&lt;/p&gt;
&lt;p&gt;因为蓝色部分是one-hot向量，只有一个对应真实分类的元素是1，别的元素都是0.所以H可以简化为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cross Entropy (H) = -log(softmax(y_i))
Where:
- y_i: Predicted score/probability for correct image class
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;归纳&lt;/h2&gt;
&lt;p&gt;根据上面的三个变换，我们可以使用线性模型的技术来做logistic regression。下面的代码逐行比较了&lt;a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-03/"&gt;线性模型&lt;/a&gt;部分和logistic regression部分的区别&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_15.png"&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Feature(x)变为一维的feature&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;预测向量(y_)和真实向量(y)变为one-hot向量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;损失函数从均方误差损失变为cross entropy&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;下面是不同的地方的总结&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170305_tensorflow_intro_p4_16.png"&gt;&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;给定几个feature的时候，线性模型可以用来预测数值型的结果。logistic regression可以用来分类。&lt;/p&gt;
&lt;p&gt;我们用例子演示了怎么通过三个改变，从线性回归y = W.x + b做logistic regression。三个改变为: (1) feature向量 (2)预测值/真实结果向量 (3)损失函数&lt;/p&gt;
&lt;p&gt;通过上面介绍的one-hot向量，softmax，已经cross entropy，下面你可以很轻松的阅读google官方的关于图片分类的&lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;简易教程&lt;/a&gt;了.&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-4-logistic-regression-2afd0cabc54#.6erqpbxxl"&gt;Gentlest Intro to Tensorflow #4: Logistic Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>tensorflow简介--03</title><link href="/pages/2017/02/26/tensorflowjian-jie-03/" rel="alternate"></link><published>2017-02-26T22:03:00-06:00</published><updated>2017-02-26T22:03:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-02-26:/pages/2017/02/26/tensorflowjian-jie-03/</id><summary type="html">&lt;p&gt;经过前面介绍的单feature的线性模型，损失函数，梯度下降（part 1），epoch， learn-rate, 可变梯度下降(part 2)，我们可以继续介绍TF下面多特征(&lt;em&gt;multi-feature&lt;/em&gt;)的线性回归。&lt;/p&gt;</summary><content type="html">&lt;h2&gt;快速回顾&lt;/h2&gt;
&lt;p&gt;前面两章的前提是：给定房屋面积作为feature，我们想预测房屋价格:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;我们找到一条直线(线性回归)来最好的拟合数据点。最好的线性回归拟合的意思是：真实值(灰色点)和预测值(灰色点在回归直线上的插值)的距离最小化。也就是那些蓝色线的值最小化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用这条回归直线，对给定的房屋面积，我们可以预测对应的价格&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_01.png"&gt;&lt;/p&gt;
&lt;h2&gt;多特征(multi-feature)线性回归回顾&lt;/h2&gt;
&lt;p&gt;实际当中，预测通常依赖于多个特征。所以我们要从单特征模型拓展到多特征模型。不失一般性，我们选择两个特征模型来可视化。这样既能保持简单，又可以推广到更多特征的情形。&lt;/p&gt;
&lt;p&gt;我们引入一个新的特征，房间的数目(房屋里面房间的数目)。在收集数据的时候，我们需要新的特征房间数目，以及已经存在的特征，房屋面积，还有相应的房屋价格。&lt;/p&gt;
&lt;p&gt;图形就是3-维的：&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_02.png"&gt;&lt;/p&gt;
&lt;p&gt;我们的目标是给定房间数目，房屋面积，来预测房屋价格。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_03.png"&gt;&lt;/p&gt;
&lt;p&gt;在单特征假设下，我们要用线性回归来产生一条直线来预测房屋面积的结果。在2-维的情形下，我们同样可以做线性回归，只不过变成一个平面（而不是一条直线）。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_04.png"&gt;&lt;/p&gt;
&lt;h2&gt;多特征线性回归模型&lt;/h2&gt;
&lt;p&gt;回忆一下单特征模型（下图左图），线性模型的结果(y)有一个系数(W),一个叫房屋面积的特征的placeholder(x)，以及截距b。&lt;/p&gt;
&lt;p&gt;对一个2个特征的模型（下图右图），我们有一个新的系数(W2)，以及另外一个placeholder叫x2，用来表示房间的数量这个特征。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_05.png"&gt;&lt;/p&gt;
&lt;p&gt;当我们进行线性回归的时候，梯度下降帮助我们知道W2的值。&lt;/p&gt;
&lt;h2&gt;TF实现多特征线性回归模型&lt;/h2&gt;
&lt;h3&gt;快速回顾&lt;/h3&gt;
&lt;p&gt;单特征的TF代码包含三个部分（见下图）：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;建立模型（蓝色部分）&lt;/li&gt;
&lt;li&gt;构建基于模型的损失函数（红色部分）&lt;/li&gt;
&lt;li&gt;用梯度下降来最小化损失函数（绿色部分）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_06.png"&gt;&lt;/p&gt;
&lt;h3&gt;TF求解两个特征的线性模型&lt;/h3&gt;
&lt;p&gt;上面解释的两个特征的线性模型的TF代码表示成红色的颜色：&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_07.png"&gt;&lt;/p&gt;
&lt;p&gt;请注意用这种办法来添加新的feature比较低效。随着特征的数量的增长，变量的数量（W）和placeholder的数量也相应的增长。现实当中的模型会有很多features，导致问题更加严重。我们怎么才能更高效的表示特征？&lt;/p&gt;
&lt;h2&gt;使用矩阵来表示&lt;/h2&gt;
&lt;p&gt;首先，我们推广一下从两个feature到n个feature的表示：&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_08.png"&gt;&lt;/p&gt;
&lt;p&gt;用矩阵可以简化n个feature的表达式，TF内嵌了矩阵：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据可以表达成多维，这正好是我们需要的：把一个数据点用n个feature（下图左）和一个有n个系数（下图右，系数矩阵）的模型表达出来。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_09.png"&gt;&lt;/p&gt;
&lt;p&gt;TF中，它们可以写为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;x = tf.placeholder(tf.float, [1,n])
W = tf.Variable(tf.zeros[n,1])
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;注意，我们用&lt;em&gt;tf.zeros&lt;/em&gt;来表示W，也就是说把W=(W1, W2, ..., Wn)全部初始化为0.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数学上矩阵相乘是对应的向量点的乘积的和。所以自然的，特征（下图中间）和系数（下图右边）的矩阵相乘就是我们想要的结果（下图左边）。这也就是上面描述的n个特征的线性回归表达式的第一部分（缺少了截距）.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_10.png"&gt;&lt;/p&gt;
&lt;p&gt;TF当中，这个相乘可以表示为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;y = tf.matmul(x, W)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;一个多行的feature矩阵(每一行表示一个数据点的n个feature)乘以系数矩阵就得到多行的结果(每行表示一个数据点的结果)。所以通过矩阵相乘我们可以把线性回归模型表达成数据点的矩阵乘以系数矩阵。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;注意： feature矩阵的x的表达式会更加复杂，比如我们用x11，x12而不是x1 x2，这是因为feature矩阵(下图中间)已经从一个数据点的n个feature(1行乘以n列)的变成了m个数据点的n个feature(m行乘以n列)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_11.png"&gt;&lt;/p&gt;
&lt;p&gt;TF中的表达形式为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;x = tf.placeholder(tf.float, [m, n])
W = tf.Variable(tf.zeros[n,1])
y = tf.matmul(x, W)
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;最后，在矩阵的每一行加一个常数项，就会在结果矩阵中加入常数项&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在TF中，x和W已经表达成矩阵，不管有多少个feature或者多少个数据点，都可以简化为&lt;/p&gt;
&lt;pre&gt;&lt;code class="language-python"&gt;b = tf.Variable(tf.zeros[1])
y = tf.matmul(x, W) + b
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Tensorflow多特征备忘录&lt;/h2&gt;
&lt;p&gt;我们做了一个比较的表格来总结从单feature到多feature的线性回归模型的变化&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p3_12.png"&gt;&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;我们演示了多feature线性回归的概念，以及从单feature到多feature的TF代码。最后用一个速查表做了总结来展示怎么在TF中做多feature回归。&lt;/p&gt;
&lt;h3&gt;下一步&lt;/h3&gt;
&lt;p&gt;下一章会讨论logtistic回归，交叉熵(cross-entropy)，softmax，这样我们就能更好的明白Tensorflow关于MNIST的&lt;a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/"&gt;官方初级教程&lt;/a&gt;。&lt;/p&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c#.r0w6hcjn2"&gt;Gentlest Intro to TensorFlow #3: Matrices &amp;amp; Multi-feature Linear Regression&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>tensorflow简介--02</title><link href="/pages/2017/02/26/tensorflowjian-jie-02/" rel="alternate"></link><published>2017-02-26T21:30:00-06:00</published><updated>2017-02-26T21:30:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-02-26:/pages/2017/02/26/tensorflowjian-jie-02/</id><summary type="html">&lt;p&gt;上一章讨论了怎么在TF里面训练机器学习模型，以及基本的tensorflow代码。这为接下来讨论各种训练变化比如stochastic/mini-batch/batch, 以及adaptive learning rate gradient descent预铺了道路.&lt;/p&gt;</summary><content type="html">&lt;p&gt;上一篇文章，我们使用tensorflow(TF)来建立了一个单feature的线性回归模型。给定feature的值（house size），我们可以预测出房屋价格。&lt;/p&gt;
&lt;p&gt;我们回顾一下上一章怎么做的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;我们有房屋面积和房屋价格的数据（图中灰色原点）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;用线性回归来建模（红色虚线）&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;通过最小化损失函数（图中蓝线的长度的平方和）来找出最优的模型的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;给定房屋面积，应用建立的线性模型来预测房屋价格（图中蓝色虚线对应的点）&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_01.png"&gt;&lt;/p&gt;
&lt;p&gt;在机器学习的论文中，我们经常会遇到‘training’这个词。我们首先来看看在TF中它的含义。&lt;/p&gt;
&lt;h2&gt;线性回归模型&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;Linear Model (in TF notation): y = tf.matmul(x,W) + b
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;线性回归的目的是找出&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;,对给定的feature值(x),当我们把&lt;code&gt;W&lt;/code&gt;,&lt;code&gt;b&lt;/code&gt;和&lt;code&gt;x&lt;/code&gt;带入模型，我们可以得到&lt;strong&gt;prediction&lt;/strong&gt;(y)。&lt;/p&gt;
&lt;p&gt;要得到&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;，我们需要用给定的数据(真实的feature值x和真实的结果y_， 注意y_的下划线)来训练(&lt;strong&gt;train&lt;/strong&gt;)模型。&lt;/p&gt;
&lt;h2&gt;Training Illustrated&lt;/h2&gt;
&lt;p&gt;要找到最优的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;，我们需要定义一个损失函数用来衡量对&lt;strong&gt;某一个给定的feature（x）值&lt;/strong&gt;，预测值（y）和真实值(y_)之间的差异。为了简化，我们用最小平方差的和(MSE)作为损失函数。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Cost function (in TF notation): tf.reduce_mean(tf.square(y_ - y))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;通过最小化损失函数，我们可以得到好的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的值。&lt;/p&gt;
&lt;p&gt;用来做训练的程序代码非常简单，我们用[A,B,C,D]来标记各个部分。下面将会用到。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# ... (snip) Variable/Constants declarations (snip) ...
# [A] TF.Graph
y = tf.matmul(x,W) + b
cost = tf.reduce_mean(tf.square(y_-y))
# [B] Train with fixed 'learn_rate'
learn_rate = 0.1
train_step =
    tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)
for i in range(steps):
  # [C] Prepare datapoints
  # ... (snip) Code to prepare datapoint as xs, and ys (snip) ...
  # [D] Feed Data at each step/epoch into 'train_step'
  feed = { x: xs, y_: ys }
  sess.run(train_step, feed_dict=feed)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;A当中的线性模型和损失函数可以被表示成下面的TF流程图&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_02.png"&gt;&lt;/p&gt;
&lt;p&gt;下一步，我们要选择一个C当中的数据(x, y_)，输入到D当中来得到预测值(y)和损失函数。TF流程图如下：&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_03.png"&gt;&lt;/p&gt;
&lt;p&gt;要得到更好的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;，我们在B中通过&lt;em&gt;tf.train.GradientDescentOptimizer&lt;/em&gt;做梯度下降来得到损失函数。更简易的说法是：给定当前的损失值，根据图中别的变量（&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;）所得到的损失函数的值，优化器会对&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;做一个小变化（增加/减少），从而使得对那一个数据点的预测值更好。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_04.png"&gt;&lt;/p&gt;
&lt;p&gt;最后一步就是更新调整过的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的值。注意这一个周期(cycle)在机器学习的文档中也叫&lt;strong&gt;epoch&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_05.png"&gt;&lt;/p&gt;
&lt;p&gt;在下一个&lt;strong&gt;epoch&lt;/strong&gt;，重复以上步骤，但是用一个&lt;strong&gt;不同的数据点(x, y_)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_06.png"&gt;&lt;/p&gt;
&lt;p&gt;使用不同的数据点来建模，所得到的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;就可以用来预测任意的feature。注意&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;大部分时候，更多的数据点得到的模型会更好&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果你的epochs比你的数据点还多，你可以重复利用数据点，这是没有问题的。梯度下降的优化器总是使用数据点和损失函数(从相应的epoch中的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;和数据点计算出来)来调整&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;。优化器可能已经用过这个数据点，但是因为损失函数不同，所以它仍然会得到新的信息，也会得到新的调整过的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;你可以训练你的模型给定次数的epoch，或者直到损失函数低于某个给定的值。&lt;/p&gt;
&lt;h2&gt;Training variation&lt;/h2&gt;
&lt;h3&gt;stochastic，mini-batch, batch&lt;/h3&gt;
&lt;p&gt;在上面的模型训练中，我们每次输入一个数据点。这叫做随机梯度下降(&lt;em&gt;stochastic gradient descent&lt;/em&gt;)。我们也可以在每个epoch中输入一批数据点，这叫做&lt;em&gt;mini-batch gradient descent&lt;/em&gt;。甚至我们可以在每个epoch中输入所有的数据点，这叫做&lt;em&gt;batch gradient descent&lt;/em&gt;。请看下面的比较图示。注意这三个图有两个不同&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个epoch中输入到TF.Graph的数据点(右上角的数据点)不同&lt;/li&gt;
&lt;li&gt;用来调整&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的梯度下降优化器所用到的数据点（右下角）也不相同&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_07.png"&gt;
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_08.png"&gt;
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_09.png"&gt;&lt;/p&gt;
&lt;p&gt;在每个epoch中使用到的数据点有两层含义。如果每个epoch使用更多的数据点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;计算损失函数和梯度下降的计算资源(加减乘)会减少&lt;/li&gt;
&lt;li&gt;模型学习的速度会提高&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;使用stochastic, mini-batch, batch gradient descent 的优缺点见下图&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_p2_10.png"&gt;&lt;/p&gt;
&lt;p&gt;要在stochastic, mini-batch, batch gradient descent这三个办法中切换，我们只需要在下面的[C]当中设置不同的batch大小，然后根据batch大小把对应的batch的数据点输入到模型训练[D]中。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# * all_xs: All the feature values
# * all_ys: All the outcome values
# datapoint_size: Number of points/entries in all_xs/all_ys
# batch_size: Configure this to:
#             1: stochastic mode
#             integer &amp;lt; datapoint_size: mini-batch mode
#             datapoint_size: batch mode
# i: Current epoch number
if datapoint_size == batch_size:
  # Batch mode so select all points starting from index 0
  batch_start_idx = 0
elif datapoint_size &amp;lt; batch_size:
  # Not possible
  raise ValueError(“datapoint_size: %d, must be greater than         
                    batch_size: %d” % (datapoint_size, batch_size))
else:
  # stochastic/mini-batch mode: Select datapoints in batches
  #                             from all possible datapoints
  batch_start_idx = (i * batch_size) % (datapoint_size — batch_size)
  batch_end_idx = batch_start_idx + batch_size
  batch_xs = all_xs[batch_start_idx:batch_end_idx]
  batch_ys = all_ys[batch_start_idx:batch_end_idx]


# Get batched datapoints into xs, ys, which is fed into
# 'train_step'
xs = np.array(batch_xs)
ys = np.array(batch_ys)
&lt;/code&gt;&lt;/pre&gt;
&lt;h3&gt;Learn Rate的变化&lt;/h3&gt;
&lt;p&gt;学习速度(Learn Rate)指的是梯度下降来调整&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的这时候，每次增加或者减小的大小。当learn rate比较小的时候，前进速度会很慢，但是会渐渐收敛到最小损失函数值。当learn rate比较大的时候，达到最小损失函数的速度会比较快，但是有可能会走过头，导致找不到最小值。&lt;/p&gt;
&lt;p&gt;要克服这一点，肯多ML的实际操作是开始的时候用比较大的learn rate（假设初始的损失函数离最小值比较远），然后每个epoch渐渐减小learn rate以防止走过头。&lt;/p&gt;
&lt;p&gt;TF提供了两个办法。这个&lt;a href="http://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer"&gt;StackOverflow thread&lt;/a&gt;讲的非常棒。总结如下:&lt;/p&gt;
&lt;h3&gt;使用变化的learn rate的梯度下降优化器&lt;/h3&gt;
&lt;p&gt;TF提供了不同的支持learn rate变化的梯度下降优化器，比如&lt;a href="https://www.tensorflow.org/api_guides/python/train#AdagradOptimizer"&gt;tf.train.AdagradientOptimizer&lt;/a&gt;和&lt;a href="https://www.tensorflow.org/api_guides/python/train#AdamOptimizer"&gt;tf.train.AdamOptimizer&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;使用tf.placeholder建Learn Rate&lt;/h3&gt;
&lt;p&gt;如前所述，我们在&lt;em&gt;tf.train.GradientDescentOptimizer&lt;/em&gt;当中用&lt;em&gt;tf.placeholder&lt;/em&gt;来申明&lt;em&gt;Learn Rate&lt;/em&gt;的时候，我们可以在每个epoch中输入不同的值。这类似于在每个epoch中我们用&lt;em&gt;tf.placeholders&lt;/em&gt;来输入不同的数据点到x, y_。&lt;/p&gt;
&lt;p&gt;这样做需要两个小的改动:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Modify [B] to make 'learn_rate' a 'tf.placeholder'
# and supply it to the 'learning_rate' parameter name of
# tf.train.GradientDescentOptimizer
learn_rate = tf.placeholder(tf.float32, shape=[])
train_step = tf.train.GradientDescentOptimizer(
    learning_rate=learn_rate).minimize(cost)
# Modify [D] to include feed a 'learn_rate' value,
# which is the 'initial_learn_rate' divided by
# 'i' (current epoch number)
# NOTE: Oversimplified. For example only.
feed = { x: xs, y_: ys, learn_rate: initial_learn_rate/i }
sess.run(train_step, feed_dict=feed)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;我们演示了机器学习的训练是什么意思，以及在Tensorflow中怎么定义模型和损失函数。然后通过输入数据点到梯度下降优化器，进行循环来得到&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的值。我们还讨论了训练过程中两个常见的变化：每个epoch中使用不同的数据集的大小，以及使用不同的learn rate。&lt;/p&gt;
&lt;h3&gt;下一章&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;设置Tensor Board来可视化TF的执行，这样可以检测到我们模型，损失函数和梯度下降的问题&lt;/li&gt;
&lt;li&gt;展示一个多特征(features)的线性回归模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f#.9cxp5xvf0"&gt;Gentlest Introduction to Tensorflow #2&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry><entry><title>tensorflow简介--01</title><link href="/pages/2017/02/26/tensorflowjian-jie-01/" rel="alternate"></link><published>2017-02-26T11:35:00-06:00</published><updated>2017-02-26T11:36:00-06:00</updated><author><name>Huiming Song</name></author><id>tag:None,2017-02-26:/pages/2017/02/26/tensorflowjian-jie-01/</id><summary type="html">&lt;p&gt;python的群里有人询问深度学习的基本概念，正好最近看到一篇非常简洁明了的介绍TF的基本概念的文章，尝试把它翻译成中文。以下为原文翻译：Tensorflow(TF)是google开源的深度学习框架。它有&lt;a href="https://www.tensorflow.org/get_started/mnist/beginners"&gt;初级&lt;/a&gt;和&lt;a href="https://www.tensorflow.org/get_started/mnist/pros"&gt;高级&lt;/a&gt;两个教程实例。然后那两个例子把ML和TF放在一起解决一个多feature的问题--图像识别。本文尝试从最简单的线性模型来介绍一个单feature的问题，然后从这儿开始扩展下去。&lt;/p&gt;</summary><content type="html">&lt;h2&gt;介绍&lt;/h2&gt;
&lt;p&gt;我们将会用一个非常简单非实际的问题来帮助理解机器学习和tensorflow(TF)。我们使用一个feature(变量)房屋面积来预测房屋价格。这样将避免讨论高维数据，让我们可以集中理解TF中定义模型，应用模型和训练模型。&lt;/p&gt;
&lt;p&gt;(原文中都写的是machine learning，机器学习。尽管现在大家都管TF为深度学习框架，我仍然翻译成机器学习。)&lt;/p&gt;
&lt;h2&gt;机器学习简介&lt;/h2&gt;
&lt;p&gt;我们从下面收集的一组数据开始。下图每一个点表示两个值--一个&lt;strong&gt;outcome&lt;/strong&gt;（房屋价格）和一个影响&lt;strong&gt;feature&lt;/strong&gt;(房屋面积)。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_01.png"&gt;&lt;/p&gt;
&lt;p&gt;然而，我们不知道那些不在图上的features点对应的价格。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_02.png"&gt;&lt;/p&gt;
&lt;p&gt;我们可以使用机器学习来发现两个之间的关系，这样的话给定一个feature点，我们可以精确的预测出结果。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_03.png"&gt;&lt;/p&gt;
&lt;h2&gt;第一步，选择模型&lt;/h2&gt;
&lt;h3&gt;模型类型&lt;/h3&gt;
&lt;p&gt;要进行预测，我们需要选择一个模拟可以最好的拟合我们上面收集的数据。&lt;/p&gt;
&lt;p&gt;我们可以选择一个线性模型，然后通过调整斜率和截距来拟合那些点。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_04.png"&gt;&lt;/p&gt;
&lt;p&gt;我们也可以选择一个指数模型，然后通过调整曲率和位置来拟合那些点。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_05.png"&gt;&lt;/p&gt;
&lt;h3&gt;损失函数&lt;/h3&gt;
&lt;p&gt;要比较哪个模型拟合更好，我们定义最佳拟合为损失函数，我们要最小化这个损失函数。一个损失函数的例子是真实值和预测值的差的绝对值的和（数学上也叫L1）。也就是下图中那些蓝色线的和就是。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_06.png"&gt;&lt;/p&gt;
&lt;p&gt;注意：更通常的损失函数是距离的平方和，也就是最小二乘。&lt;/p&gt;
&lt;h3&gt;线性模型简介&lt;/h3&gt;
&lt;p&gt;为了简单化问题，我们用线性模型来拟合数据。一个线性模型何以数学的表达为&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;y = W.x + b
Where:
x: house size, in sqm
y: predicted house price, in $
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;要调整参数来拟合数据，我们可以&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;调整&lt;code&gt;W&lt;/code&gt;来改变线性模型的梯度
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_07.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;调整&lt;code&gt;b&lt;/code&gt;来改变位置
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_08.png"&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过选择不同的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;，我们可以最终选者一个最合适的模型来最小化损失函数。除了随机的选择&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;，我们有没有更好的办法来尝试&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;？&lt;/p&gt;
&lt;h3&gt;梯度下降&lt;/h3&gt;
&lt;p&gt;如果你在山上的一个平台上，当你想下降到最低点的时候，你的视野是这样的
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_09.png"&gt;&lt;/p&gt;
&lt;p&gt;下降的方向是不明了的！这样最好的办法就是梯度下降(gradient descent)：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在当前位置找到最陡峭的方向最为梯度方向&lt;/li&gt;
&lt;li&gt;沿着选择的方向走一步，步长为&lt;code&gt;X&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;重复上面的办法，这就是&lt;strong&gt;training&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;最小化损失函数也相似。下图中，损失函数就像那个山一样起伏，我们的目的是找到最低点。通过梯度下降，我们可以做到这一点。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_10.png"&gt;&lt;/p&gt;
&lt;p&gt;有了线性模型，损失函数，和梯度下降这些概念，我们可以进一步学习TF了。&lt;/p&gt;
&lt;h2&gt;第二步：TF中建模&lt;/h2&gt;
&lt;h3&gt;TF中线性模型&lt;/h3&gt;
&lt;p&gt;TF的两个基本概念是：&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. placeholder&lt;/strong&gt;: 表示进行梯度下降算法的时候，输入模型中的实际值。在上面的例子中，也就是房屋面积(x)和房屋价格(y_)。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_11.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. variable&lt;/strong&gt;：表示损失函数中我们要寻找最佳值的那些变量，也就是上例中的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_12.png"&gt;&lt;/p&gt;
&lt;p&gt;在TF中线性模型&lt;code&gt;y = W.x + b&lt;/code&gt;表示为
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_13.png"&gt;&lt;/p&gt;
&lt;h3&gt;TF中的损失函数&lt;/h3&gt;
&lt;p&gt;类似于把真实房屋价格输入到模型中，我们建立一个placeholder:
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_14.png"&gt;&lt;/p&gt;
&lt;p&gt;我们的最小二乘损失函数可以表示为
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_15.png"&gt;&lt;/p&gt;
&lt;h3&gt;数据&lt;/h3&gt;
&lt;p&gt;因为我们没有实际的房屋价格(y_)和房屋面积(x)，我们模拟出它们的值
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_16.png"&gt;&lt;/p&gt;
&lt;p&gt;模拟的时候，我们假设房屋价格是房屋面积的2倍。&lt;/p&gt;
&lt;h3&gt;梯度下降&lt;/h3&gt;
&lt;p&gt;有了线性模型，损失函数，数据，我们就可以用梯度下降来最小化损失函数，寻找到最优的W和b。&lt;/p&gt;
&lt;p&gt;&lt;img alt="png" src="/figures/20170225_tensorflow_intro_17.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;0.00001&lt;/code&gt;就是梯度下降中的沿着最陡峭方向的步长，也叫&lt;strong&gt;learning rate&lt;/strong&gt;。&lt;/p&gt;
&lt;h2&gt;第三步：训练模型&lt;/h2&gt;
&lt;p&gt;训练模型包括按照预先设置的迭代次数进行梯度下降，或者直到损失函数小于一个预先给定的值。&lt;/p&gt;
&lt;h3&gt;TF Quirks&lt;/h3&gt;
&lt;p&gt;所有的variable都要给定初始值，否则的话他们可能保持上次迭代的值
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_18.png"&gt;&lt;/p&gt;
&lt;h3&gt;TF Session&lt;/h3&gt;
&lt;p&gt;尽管TF是一个python库，python是一个解释型的语言，但是因为性能原因，TF默认却不能执行解释运算。所以上面的&lt;code&gt;init&lt;/code&gt;并没有执行。TF在&lt;strong&gt;session&lt;/strong&gt;里面执行，建立一个session(sess)然后执行的话使用&lt;code&gt;sess.run()&lt;/code&gt;
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_19.png"&gt;&lt;/p&gt;
&lt;p&gt;类似的，我们通过调用&lt;code&gt;sess.run()&lt;/code&gt;里的循环来执行&lt;em&gt;train_step&lt;/em&gt;.
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_20.png"&gt;&lt;/p&gt;
&lt;p&gt;你需要把实际值输入到x，y_组成的&lt;em&gt;feed&lt;/em&gt;里面去，这是因为TF把解出来的&lt;em&gt;train_step&lt;/em&gt;放入他的dependencies里：
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_21.png"&gt;&lt;/p&gt;
&lt;p&gt;dependencies的底部就是x, y_的placeholders.我们已经知道，&lt;strong&gt;tf.placeholders&lt;/strong&gt;是用来输入实际的房屋价格(y_)和房屋面积(x)的。&lt;/p&gt;
&lt;h3&gt;结果&lt;/h3&gt;
&lt;p&gt;我们打印出循环里的值来显示TF怎么找到最佳的&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的。
&lt;img alt="png" src="/figures/20170225_tensorflow_intro_22.png"&gt;&lt;/p&gt;
&lt;h2&gt;总结&lt;/h2&gt;
&lt;p&gt;我们学习了机器学习的最简单的模型，用一个feature来预测结果。为了简化问题，我们使用了线性模型来拟合数据。然后定义一个损失函数来找到最佳拟合。最后通过梯度下降来找到variable&lt;code&gt;W&lt;/code&gt;和&lt;code&gt;b&lt;/code&gt;的最佳估计。&lt;/p&gt;
&lt;h3&gt;下一步&lt;/h3&gt;
&lt;p&gt;在下一篇文章，我们将会&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;设置Tensor Board来可视化TF的执行，这样可以检测到我们模型，损失函数和梯度下降的问题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;分批(batches)输入数据来训练模型(而不是每次输入一个数据点)，并且展示这样会怎么影响结果&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Reference&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224#.7ptf8ybid"&gt;Gentlest Introduction to Tensorflow #1&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="Python"></category><category term="python"></category><category term="tensorflow"></category><category term="deep learning"></category></entry></feed>