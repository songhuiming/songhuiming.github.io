<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Build Neural Network from Scratch &mdash; pydata: Huiming's learning notes</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><header>
  <h1><a href="/">pydata: Huiming's learning notes</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</header>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  // 允许 $...$ 和 \( ... \) 作为行内公式
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  // 允许 $$...$$ 和 \[ ... \] 作为块级公式
    processEscapes: true,  // 允许在公式中使用转义符，如 \$ 表示美元符号
    processEnvironments: true  // 允许解析 \begin{equation} ... \end{equation} 等数学环境
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],  // 跳过某些 HTML 标签，防止错误解析
    renderActions: {
      addMenu: []  // 移除右键菜单
    }
  }
};

window.addEventListener('load', () => {
  document.querySelectorAll("mjx-container").forEach(x => {
    x.parentElement.classList.add('has-jax');  // 使用 classList.add() 避免字符串拼接错误
  });
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></header>
  <nav role="navigation">

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Build Neural Network from Scratch</h1>
    <p class="meta">
<time datetime="2017-08-12T15:08:00-05:00" pubdate>Sat 12 August 2017</time>    </p>
</header>

  <div class="entry-content"><h2>How to build Neural Network from scratch</h2>
<p>This post will introduce how to build a neural network from stratch: </p>
<ol>
<li>the forward-propagation: from input data to activations on each layer</li>
<li>the backpropagation: using chain rules to calculate the deravatives of cost function to weights(parameters) on each layer</li>
<li>minimize the cost function by SGD with the derivatives from step 2.</li>
</ol>
<h3>Example 1.</h3>
<p>First I will introduce a very simple example. Suppose we have data like the following. <span class="math">\(X\)</span> is a 4x3 array. <span class="math">\(Y\)</span> is a 4x1 vector. The task is to predict the output <span class="math">\(Y\)</span> based on the input <span class="math">\(X\)</span>. We will show how to build a neural network to do this job.</p>
<table cellspacing="10" width="400">
  <tr>
    <td colspan="3"  align="center" valign="middle">Inputs: X</td>
    <td  align="center" valign="middle">Output: Y</td>
  </tr>

  <tr>
    <td align="center" valign="middle">0 </td>
    <td align="center" valign="middle">0 </td>
    <td align="center" valign="middle">1 </td>
    <td align="center" valign="middle">0 </td>
  </tr>
  <tr>
    <td align="center" valign="middle">0</td>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">1</td>
  </tr>
  <tr>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">0</td>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">1</td>
  </tr> 
  <tr>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">1</td>
    <td align="center" valign="middle">0</td>
  </tr> 
</table>

<p>Our NN will have 2 layers. The first layer is from input <span class="math">\(X\)</span> (we will call it <span class="math">\(a_1\)</span>) to activations <span class="math">\(a_2\)</span> with the sigmoid activation function. The shape of the weights for the first layer (<span class="math">\(\Theta_1^{T}\)</span>) is 4x3. So the shape of layer 1 is 4x4.</p>
<p>The layer 1 (<span class="math">\(a_2\)</span>) will be the input for the second layer(<span class="math">\(a_3\)</span>) with weights parameter <span class="math">\(\Theta_2^{T}\)</span> and sigmoid activation function. The shape of <span class="math">\(\Theta_2^{T}\)</span> is 4x1 (the shape of input data to layer 2 is 4x4 and the shape of <span class="math">\(Y\)</span> is 4x1, these two will determine the shape of <span class="math">\(\Theta_2\)</span> because layer 2 is the output layer here).</p>
<h2>Preparation</h2>
<h3>1.1. Model Structure</h3>
<p>Our prediction Neural Network will have two layers: the first layer a linear combination between input <span class="math">\(X\)</span> and parameter <span class="math">\(\theta_1\)</span>. Then this will be the input to the <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a> function, which is the second layer. </p>
<div class="math">\begin{aligned}
    a_1 &amp;= X \\
    z_1 &amp;= a_1 \cdot \Theta_1^{T}  \\
    a_2 &amp;= \mbox{sigmoid}(z_1) = \frac{1}{1 + e^{- z_1}}  \qquad \mbox{the first activation layer} \\
    z_2 &amp;= a_2 \cdot \Theta_2^{T} \\
    a_3 &amp;= \mbox{sigmoid}(z_2) = \frac{1}{1 + e^{- z_2}}  \qquad \mbox{the second activation layer}
\end{aligned}</div>
<p>The symbal <span class="math">\(\cdot\)</span> above means <a href="https://en.wikipedia.org/wiki/Dot_product">dot product</a> of the vector or the matrix. We also need the [elementwise product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices) denoted as <span class="math">\(\odot\)</span>.</p>
<h3>1.2. Cost Function</h3>
<p>Cost function generally measures the distance between the predictions and the true value. </p>
<ol>
<li>
<p><strong><a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">Cross Entropy</a> Loss function</strong>:
<div class="math">\begin{aligned}
     C = -\frac{1}{n}\sum_{x}\left[y \log(a_3) + (1 - y)\log(1 - a_3) \right]  
\end{aligned}</div>
</p>
</li>
<li>
<p><strong>L2 cost function</strong>:</p>
</li>
</ol>
<div class="math">\begin{aligned}
    C = \frac{1}{2}||y - a_3||^2  
\end{aligned}</div>
<h3>1.3. Derivatives / Gradient</h3>
<p>Suppose the Lost function is
</p>
<div class="math">\begin{aligned}
    C = \frac{1}{2} ||y - a_3||^2   
\end{aligned}</div>
<p>First, let's denote
</p>
<div class="math">\begin{aligned}
    d_3 &amp;= (a_3 - y) \odot (a_3 \odot (1 - a_3))  ,\; \; \mbox{ or call it } a_3\_delta \\
    d_2 &amp;= d_3 \cdot \Theta_2 \odot (a_2 \odot (1 - a_2)) 
\end{aligned}</div>
<p>The partival derivative of Loss function to parameter <span class="math">\(\Theta_2\)</span> using chain rule is:
</p>
<div class="math">\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_2}} &amp; = \frac{\partial{C}}{\partial{a_3}} \frac{\partial{a_3}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{\Theta_2}} \\
    &amp; = (a_3 - y) \left(a_3 (1 - a_3)\right) a_2  \; \; (\mbox{if all are scalers}) \\
    &amp; =  \left[(a_3 - y) \odot \left(a_3 \odot (1 - a_3)\right)\right]^{T} \cdot a_2 \\
    &amp;  \triangleq   d_3^{T} \cdot a_2  
\end{aligned}</div>
<p>where  <span class="math">\(d_3 \triangleq (a_3 - y) \odot \left(a_3 \odot (1 - a_3)\right)\)</span> . <span class="math">\(\odot\)</span>  maens element-wise product, <span class="math">\(\cdot\)</span> means dot product.</p>
<p>The partival derivative of Loss function to parameter <span class="math">\(\Theta_1\)</span> using chain rule is:
</p>
<div class="math">\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_1}} &amp; = \frac{\partial{C}}{\partial{a_3}} \frac{\partial{a_3}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{a_2}} \frac{\partial{a_2}}{\partial{z_1}} \frac{\partial{z_1}}{\partial{\Theta_1}}  \\
    &amp; \propto (a_3 - y) \left(a_3 (1 - a_3)\right) \Theta_2 (a_2 (1 - a_2)) a_1 \\
    &amp; \propto \left[(a_3 - y) \odot \left(a_3 (1 - a_3)\right) \cdot \Theta_2 \odot (a_2 \odot (1 - a_2))\right]^{T} a_1 \\
    &amp;  = d_2^{T} \cdot a_1
\end{aligned}</div>
<p>
where <span class="math">\(d_2 \triangleq d_3 \cdot \Theta_2 \odot \left(a_2 \odot (1 - a_2) \right)\)</span> .</p>
<p>To find the value of <span class="math">\(\Theta_1\)</span> and <span class="math">\(\Theta_2\)</span>, we need to iteratively adjust their value until their gradient is close to 0. That is, to iterate:</p>
<div class="math">\begin{aligned}
    \Theta_2^{i + 1} &amp;= \Theta_2^{i} + \lambda  \frac{\partial {C}}{\partial {\Theta_2^{i}}}  \\
    \Theta_1^{i + 1} &amp;= \Theta_1^{i} + \lambda  \frac{\partial {C}}{\partial {\Theta_1^{i}}} 
\end{aligned}</div>
<h3>4. Code</h3>
<pre><code class="language-python">import numpy as np
np.random.seed(10)
x = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
y = np.array([0, 1, 1, 0]).reshape(4, -1)

a1 = x
theta1 = np.random.random(12).reshape(4, 3) * 2 - 1
theta2 = np.random.random(4).reshape(1, 4) * 2 - 1

learning_rate = 1

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

for i in range(100000):
    # forward propagation: including the first layer and the second layer
    z1 = a1.dot(theta1.T)
    a2 = sigmoid(z1) 
    z2 = a2.dot(theta2.T)
    a3 = sigmoid(z2) 

    # backpropagation: chain rules of the partial derivatives
    a3_error = y - a3
    theta2_grad = np.dot(np.transpose(a3_error * (a3 * (1 - a3))), a2) 
    theta1_grad = np.dot(np.transpose(np.dot(a3_error * (a3 * (1 - a3)),  theta2) * (a2 * (1 - a2))), a1)

    # update the parameters until the gradient converges to 0
    theta2 += learning_rate * theta2_grad
    theta1 += learning_rate * theta1_grad
</code></pre>
<pre><code class="language-python">theta2
</code></pre>
<pre><code>array([[ -3.16174322,  10.29352444,  11.40818558,  -4.45945196]])
</code></pre>
<pre><code class="language-python">theta1
</code></pre>
<pre><code>array([[ 3.35916278, -3.32786847,  1.87094082],
       [ 5.84130712, -5.98365309, -3.07635286],
       [-6.45870855,  6.34717945, -3.30437575],
       [-3.91182875,  3.95761648,  2.17740367]])
</code></pre>
<pre><code class="language-python">a3
</code></pre>
<pre><code>array([[ 0.00276657],
       [ 0.99710763],
       [ 0.99718615],
       [ 0.00243296]])
</code></pre>
<pre><code class="language-python"># The prediction is
a3 &gt; 0.5
</code></pre>
<pre><code>array([[False],
       [ True],
       [ True],
       [False]], dtype=bool)
</code></pre>
<p>The above is a simple example to introduce the insides of a neural network: how to calculate the forward propagation from input data to the prediction output and the cost function, how to calcualte the back propagatin of the partial derivatives with chain rules, and how to update the parameters until the gradients converging to zero, although in fact neural network is not necessary for this simple example since there are only 4 data points but we have introduced 16 parameters(parameters in <span class="math">\(\Theta_1\)</span> and <span class="math">\(\Theta_2\)</span>).</p>
<h2>Example 2</h2>
<p>The following example is a more real data question: I collect 60 verifacation code graphs. Each graph has 4 characters. The user needs to recognize these 4 characters correctly to get correct input verification.</p>
<p>Each graph has 4 characters. An example of the picture is like</p>
<p><img alt="alt text" src="/figures/20170812_nn_from_scratch_01.png" title="Logo Title Text 1"></p>
<p>The corresponding characters are: 'k' 'n' 'n' 'p'. </p>
<h3>2.1. Model</h3>
<p>We will build a three layers neural network. </p>
<div class="math">\begin{aligned}
    a_1 &amp;= X \\
    z_1 &amp;= a_1 \cdot \Theta_1^{T}  \\
    a_2 &amp;= \mbox{sigmoid}(z_1) = \frac{1}{1 + e^{- z_1}}  \qquad \mbox{the first activation layer} \\
    z_2 &amp;= a_2 \cdot \Theta_2^{T} \\
    a_3 &amp;= \mbox{sigmoid}(z_2) = \frac{1}{1 + e^{- z_2}}  \qquad \mbox{the second activation layer} \\
    z_3 &amp;= a_3 \cdot \Theta_3^{T}  \\
    a_4 &amp;= \mbox{sigmoid}(z_3) = \frac{1}{1 + e^{- z_3}}  \qquad \mbox{the third activation layer} 
\end{aligned}</div>
<h3>2.2. Derivatives</h3>
<p>Suppose the Lost function is
</p>
<div class="math">\begin{aligned}
    C = -\frac{1}{n}\sum_{x}\left[y \log(a_3) + (1 - y)\log(1 - a_3) \right] 
\end{aligned}</div>
<p>First, let's denote
</p>
<div class="math">\begin{aligned}
    d_4 &amp;= a_4 - y   ,\; \; \mbox{ or call it } a_4\_delta \\
    d_3 &amp;= d_4 \cdot \Theta_3 \odot (a_3 \odot (1 - a_3))  ,\; \; \mbox{ or call it } a_3\_delta \\
    d_2 &amp;= d_3 \cdot \Theta_2 \odot (a_2 \odot (1 - a_2)) 
\end{aligned}</div>
<p>The partival derivative of Loss function to parameter <span class="math">\(\Theta_3\)</span> using chain rule is:</p>
<div class="math">\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_3}} &amp; = \frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{\Theta_3}} \\
    &amp; = - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) a_3 = ( a_4 - y) a_3  \; \; (\mbox{if all are scalers}) \\
    &amp; =  \left[(a_4 - y)\right]^{T} \cdot a_3 \\
    &amp;  \triangleq   d_4^{T} \cdot a_3  
\end{aligned}</div>
<p>The partival derivative of Loss function to parameter <span class="math">\(\Theta_2\)</span> using chain rule is:</p>
<div class="math">\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_2}} &amp; =\frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{a_3}} \frac{\partial a_3}{\partial z_3} \frac{\partial z_3}{\partial \Theta_2} \\
    &amp; \propto - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) \Theta_3 (a_3(1-a_3)) a_2  \; \; (\mbox{if all are scalers})  \\
    &amp; =  \left[d_4 \cdot \Theta_3 \odot \left(a_3 \odot (1 - a_3)\right)\right]^{T} \cdot a_2 \\
    &amp;  \triangleq   d_3^{T} \cdot a_2  
\end{aligned}</div>
<p>The partival derivative of Loss function to parameter <span class="math">\(\Theta_1\)</span> using chain rule is:
</p>
<div class="math">\begin{aligned}
    \frac{\partial {C}}{\partial {\Theta_1}} &amp; = \frac{\partial{C}}{\partial{a_4}} \frac{\partial{a_4}}{\partial{z_4}} \frac{\partial{z_4}}{\partial{a_3}} \frac{\partial a_3}{\partial z_3} \frac{\partial z_3}{\partial a_2} \frac{\partial{a_2}}{\partial{z_2}} \frac{\partial{z_2}}{\partial{\Theta_1}}  \\
    &amp; \propto - \left[ \frac{y}{a_4} - \frac{1 - y}{1 - a_4}  \right] \left(a_4 (1 - a_4)\right) \Theta_3 (a_3(1-a_3)) \Theta_2 (a_2(1-a_2)) \cdot a_1 \; \; (\mbox{if all are scalers}) \\
    &amp; = \left[(a_3 - y) \odot \left(a_3 (1 - a_3)\right) \cdot \Theta_2 \odot (a_2 \odot (1 - a_2))\right]^{T} a_1 \\
    &amp;  = d_2^{T} \cdot a_1
\end{aligned}</div>
<h3>2.3. Details</h3>
<p>The overall steps will include these:</p>
<ol>
<li>from pic to data</li>
<li>set up activation function / sigmoid</li>
<li>calc gradient descent function</li>
<li>loss function</li>
<li>predict</li>
<li>put all together</li>
</ol>
<h4>Shape of the data</h4>
<pre><code>Theta1.T: 261 x 200
Theta2.T: 201 x 200
Theta3.T: 201 x 36

X: 240 * 260
a1: 240 * 261
z2: 240 * 200
a2: 240 * 200 -&gt; 240 * 201
z3: 240 * 201
a3: 240 * 200 -&gt; 240 * 201
z4: 240 * 36
a4: 240 * 36

y_matirx: 240 * 36
</code></pre>
<pre><code class="language-python">import scipy.io
import os
import scipy.signal
import pandas as pd
import numpy as np
from PIL import Image
import scipy.optimize as opt
</code></pre>
<pre><code class="language-python">word = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\
        'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']

mypath = r'data/yanzhengma/'
train_data = pd.read_pickle(mypath + 'train_data.pkl')

</code></pre>
<pre><code class="language-python">
def sigmoid(x, derivative = False):
    if derivative == True:
        return x * (1 - x)
    else:
        return 1 / (1 + np.exp(-x))

def rand_init_weights(layer_in, layer_out):
    epsilon_init = 0.1
    return np.random.rand(layer_out, 1 + layer_in) * 2 * epsilon_init - epsilon_init

def nn_gradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param):
    '''
    theta1: 200x261
    theta2: 200x201
    theta3: 36x201
    a1: 240x261
    z2: 240x200
    a2: 240x200 -&gt; 240x201
    z3: 240x200
    a3: 240x2-- -&gt; 240x201
    z4: 240x36
    a4: 240x36
    d4: 240x36
    d3: 240x201 -&gt; 240x200
    d2: 240x201 -&gt; 240x200
    theta1_grad: 200x261
    theta2_grad: 200x201
    theta3_grad: 36x201
    final output: 200x261 + 200x201 + 36x201 = 99636
    '''
    theta1 = nn_params[0:hidden_layer_size * (input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[(hidden_layer_size * (input_layer_size + 1)) : \
                       (hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    n = np.shape(X)[0]
    a1 = np.append(np.ones((n, 1)), X, axis = 1)
    z2 = a1.dot(theta1.T)
    a2 = sigmoid(z2)
    a2 = np.append(np.ones((n, 1)), a2, axis = 1)
    z3 = a2.dot(theta2.T)
    a3 = sigmoid(z3)
    a3 = np.append(np.ones((n, 1)), a3, axis = 1)
    z4 = a3.dot(theta3.T)
    a4 = sigmoid(z4)

    y_matrix = np.zeros((n, num_labels))
    for i in range(num_labels):
        y_matrix[:, i] = (y == (i + 1)).reshape(-1)

    d4 = a4 - y_matrix
    d3 = d4.dot(theta3) * a3 * (1 - a3)   
    d3 = d3[:, 1:]
    d2 = d3.dot(theta2) * a2 * (1 - a2)  
    d2 = d2[:, 1:]
    theta1_grad = 1 / n * (d2.T.dot(a1))
    theta2_grad = 1 / n * (d3.T.dot(a2)) 
    theta3_grad = 1 / n * (d4.T.dot(a3))  
    return np.append(np.append(theta1_grad.flatten(), theta2_grad.flatten(), axis = 0), theta3_grad.flatten(), axis = 0)

def nn_cost(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param):
    theta1 = nn_params[0:(hidden_layer_size * (input_layer_size + 1))].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[(hidden_layer_size * (input_layer_size + 1)) : \
                       (hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    n = np.shape(X)[0]
    a1 = np.append(np.ones((n, 1)), X, axis = 1)
    z2 = a1.dot(theta1.T)
    a2 = sigmoid(z2)
    a2 = np.append(np.ones((n, 1)), a2, axis = 1)
    z3 = a2.dot(theta2.T)
    a3 = sigmoid(z3)
    a3 = np.append(np.ones((n, 1)), a3, axis = 1)
    z4 = a3.dot(theta3.T)
    a4 = sigmoid(z4)

    y_matrix = np.zeros((n, num_labels))
    for i in range(num_labels):
        y_matrix[:, i] = (y == (i + 1)).reshape(-1)

    return -(1 / n) * (np.sum(np.sum(y_matrix * np.log(a4))) + np.sum(np.sum((1 - y_matrix) * np.log(1 - a4)))) + \
        lambda_param / ((2 * n) * np.sum(np.sum(theta1[:, 1:]**2)) + np.sum(np.sum(theta2[:, 1:]**2)) + \
                        np.sum(np.sum(theta3[:, 1:]**2)))

def predict(theta1, theta2, theta3, X):
    n = np.shape(X)[0]
    p = np.zeros((n, 1))
    h1 = sigmoid(np.append(np.ones((n, 1)), X, 1).dot(theta1.T))
    h2 = sigmoid(np.append(np.ones((n, 1)), h1, 1).dot(theta2.T))
    h3 = sigmoid(np.append(np.ones((n, 1)), h2, 1).dot(theta3.T))
    p = np.ndarray.argmax(h3, axis = 1).reshape([p.shape[0], 1])
    return p + 1

def test(train_data = train_data):
    data = train_data
    X = train_data['X']
    y = train_data['y']
    input_layer_size = 260
    hidden_layer_size = 200
    num_labels = 36

    initial_theta1 = rand_init_weights(input_layer_size, hidden_layer_size)
    initial_theta2 = rand_init_weights(hidden_layer_size, hidden_layer_size)
    initial_theta3 = rand_init_weights(hidden_layer_size, num_labels)

    initial_nn_params = np.append(np.append(initial_theta1.flatten(), initial_theta2.flatten()), initial_theta3.flatten())
    lambda_param = 0.1
    result = opt.minimize(fun = nn_cost, x0 = initial_nn_params, \
                          args = (input_layer_size, hidden_layer_size, num_labels, X, y, lambda_param), \
                          method = &quot;CG&quot;, jac = nn_gradient, options = {&quot;maxiter&quot;: 100})
    nn_params = result.x
    theta1 = nn_params[0:hidden_layer_size*(input_layer_size + 1)].reshape(hidden_layer_size, input_layer_size + 1)
    theta2 = nn_params[hidden_layer_size*(input_layer_size + 1) : \
                       (hidden_layer_size*(input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1))].reshape(hidden_layer_size, hidden_layer_size + 1)
    theta3 = nn_params[(hidden_layer_size * (input_layer_size + 1) + \
                        hidden_layer_size * (hidden_layer_size + 1)):].reshape(num_labels, hidden_layer_size + 1)
    pred = predict(theta1, theta2, theta3, X)
    print(&quot;Training Accuracy: &quot;, np.mean(pred == y) * 100, &quot;%&quot;)
    return

test()

</code></pre>
<pre><code>('Training Accuracy: ', 100.0000000, '%')
</code></pre>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-08-12T15:08:00-05:00" pubdate>Sat 12 August 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/deep-learning.html">deep learning</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/02/23/deepseek-v3-learning-notes/">DeepSeek V3 learning notes</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2024/04/21/prediction-in-decoder-and-kv-cache/">Prediction in decoder and KV-Cache</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/agi.html">AGI</a>,    <a href="/tag/gpt.html">GPT</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/08/12/build-neural-network-from-scratch/';
    var disqus_url = '/pages/2017/08/12/build-neural-network-from-scratch/';
    var disqus_title = 'Build Neural Network from Scratch';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>