<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Build Recurrent Neural Network from Scratch &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Build Recurrent Neural Network from Scratch</h1>
    <p class="meta">
<time datetime="2017-08-20T16:08:00-05:00" pubdate>Sun 20 August 2017</time>    </p>
</header>

  <div class="entry-content"><h2>0. Introduction</h2>
<p>The <a href="http://songhuiming.github.io/pages/2017/08/12/build-neural-network-from-scratch/">previous blog</a>  shows how to build a neural network manualy from scratch in numpy with matrix/vector multiply and add. Although there are many packages can do this easily and quickly with a few lines of scripts, it is still a good idea to understand the logic behind the packages. This part is from <a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">a good blog</a> which use an example predicitng the words in the sentence to explain how to build RNN manually. RNN is a little more complicated than the neural network in the previous blog because the current time status and ourput in RNN will depends on the status in the previous time. So the Backpropagation part will be more complicated. I try to give the details in mathematic formula about how to get the gradients recursively in the partial derivatives.</p>
<p>Recurrent neural network is one of the most popular neural networks for language modeling(based on existed words to predict next word) or automatic input like the automatic complete in the mobile input(based on existed character to predict next character).</p>
<p>For example, when we build a RNN for language, that means: the training data is a list of sentences. Each sentence is a seris of words(tokenized words). For each sentence, from the first word, we will predict the second word. From the first and the second word, we will predict the third word, etc. Recurrent neural network means when it predict time order t, it will remember the information from time order 0 to time order t.</p>
<p>Let's denote the sentence having <span class="math">\(t+1\)</span> words as <span class="math">\(x=[x_0, x_1, \cdots, x_t]\)</span>. We start from <span class="math">\(x_0\)</span> to status <span class="math">\(s_0 = \tanh(Ux_0 + Ws_{-1})\)</span>, where <span class="math">\(s_{-1}\)</span> is the initialization of status initialized as 0. The output <span class="math">\(o_0 = \mathrm{softmax}(Vs_0)\)</span>. Then when we go to next word <span class="math">\(x_1\)</span> we will have updated status <span class="math">\(s_1 = \tanh(Ux_1 + Ws_0)\)</span> and the corresponding output <span class="math">\(o_1 = \mathrm{softmax}(Vs_1)\)</span>. You will see at time order <span class="math">\(t=1\)</span> it not only depends on input <span class="math">\(x_1\)</span> but also depends on the previous status <span class="math">\(s_0\)</span>. The equation for the RNN used in this tutorial is:</p>
<div class="math">\begin{aligned}
s_t &amp;= \tanh(Ux_t + Ws_{t-1}) \\
o_t &amp;= \mathrm{softmax}(Vs_t)
\end{aligned}</div>
<p>If we plot the logic of RNN and the corresponding forward propagation, it is like
<img alt="alt text" src="/figures/20170826_rnn_scratch_01_rnn.jpg" title="Logo Title Text 1" /></p>
<p>The training data is 79,170 sentences coming from 15,000 reddit comments(one comment may has multiple sentences). The vocabulary consists of the 8,000 most common words. For the words not included in the vocabulary list are replaced by UNKNOWN_TOKEN.</p>
<p>The words in each sentences are mapped to the index of the order of these words in the vocabulary list. So the training data <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are both integers rather than strings. Since the purpose here is to predict the next word, so <span class="math">\(Y\)</span> is just <span class="math">\(X\)</span> shifted by one leading position.</p>
<h2>1. Build RNN</h2>
<h3>1.0. Model and Data Structure, Initialization</h3>
<p>As introduced before, the model structure of RNN used here is:
</p>
<div class="math">\begin{aligned}
s_t &amp;= \tanh(Ux_t + Ws_{t-1}) \\
o_t &amp;= \mathrm{softmax}(Vs_t)
\end{aligned}</div>
<p>The vocabulary size <span class="math">\(C=8,000\)</span> and the hidden layer size <span class="math">\(H=100\)</span>. So the size of W is <span class="math">\(100 \times 100\)</span>.</p>
<p>Let's assume one sentence has 10 words, for the corresponding mapped <span class="math">\(x\)</span>, we can treat it in two equal ways: 1. it is a python list by index of the words in the sentence. Then its length is the same as the number of words in that sentence, which is 10. we call it x_expression_1;   2. it is the one-hot transformation of these 10 words in the 8,000 vocabulary list. so its shape is [10, 8000]. For each row, there is one 1 in the word index position and the other 7999 positions will be all 0. we call it x_expression_2. So for each word of these 10 words, the matrix dot multiply <span class="math">\(U.\)</span>dot<span class="math">\((x\_expression\_2[t])\)</span> is the same as <span class="math">\(U[:, x\_expression\_1[t]]\)</span> for <span class="math">\(t = \mathrm{range(10)}\)</span>.</p>
<p>If use numpy to explain above, it is <code>np.arange(1, 21).reshape(4, 5).dot([0,1,0,0,0])</code> == <code>np.arange(1, 21).reshape(4, 5)[:, 1]</code>. This will save a lot of time because numpy indexing is much faster than matrix dot multiply.</p>
<p>The dimension of all the data used is(below <span class="math">\(x_t\)</span> is index position in the vocabulary list for the <span class="math">\(t_{th}\)</span> word in the sentence, and <span class="math">\(x\)</span> is a sentence with <span class="math">\(l\)</span> words):</p>
<div class="math">\begin{aligned}
x_t &amp;\in \mathbb{R}^{8000}  \Longleftrightarrow x \in \mathbb{R}^{l \times 8000} \\
o_t &amp;\in \mathbb{R}^{8000}  \Longleftrightarrow o \in \mathbb{R}^{l \times 8000} \\
s_t &amp;\in \mathbb{R}^{100}   \Longleftrightarrow s \in \mathbb{R}^{l \times 100} \\
U &amp;\in \mathbb{R}^{100 \times 8000} \\
V &amp;\in \mathbb{R}^{8000 \times 100} \\
W &amp;\in \mathbb{R}^{100 \times 100} \\
\end{aligned}</div>
<p>When we iterate to update the parameters, we need to set up their initial values. It is very important to set up suitable initializations to make RNN gradients work well. The initizalition of the parameters is initialized as from random uniform distribution <span class="math">\(U\left(-\frac{1}{\sqrt{n}}, \frac{1}{\sqrt{n}} \right)\)</span> :</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">import</span> <span class="nn">operator</span>
<span class="kn">from</span> <span class="nn">datetime</span> <span class="kn">import</span> <span class="n">datetime</span>
<span class="kn">import</span> <span class="nn">sys</span>

<span class="n">vocabulary_size</span> <span class="o">=</span> <span class="mi">8000</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;X_train.npy&#39;</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;y_train.npy&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span class="c1">## initialize parameters</span>
<span class="k">class</span> <span class="nc">RNNNumpy</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word_dim</span><span class="p">,</span> <span class="n">hidden_dim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">bptt_truncate</span> <span class="o">=</span> <span class="mi">4</span><span class="p">):</span>
        <span class="c1"># assign instance variable</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span> <span class="o">=</span> <span class="n">word_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span> <span class="o">=</span> <span class="n">hidden_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bptt_truncate</span> <span class="o">=</span> <span class="n">bptt_truncate</span>
        <span class="c1"># random initiate the parameters</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">word_dim</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">word_dim</span><span class="p">),</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">word_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">hidden_dim</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">hidden_dim</span><span class="p">),</span> <span class="p">(</span><span class="n">word_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">hidden_dim</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mf">1.</span><span class="o">/</span><span class="n">hidden_dim</span><span class="p">),</span> <span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="p">))</span>
</pre></div>


<p>Pay attention: the matrix <span class="math">\(U, V, W\)</span> are not initizalized as matrix with value 0 but random numbers. If you initilize them as 0, then you will get everythin 0 and they will not change in the loop. </p>
<h3>1.1. Forward-Propagation</h3>
<p>For a given sentence <span class="math">\(x = (x_0, \cdots, x_{T-1})\)</span> having <span class="math">\(T\)</span> words, we will start from <span class="math">\(x_0\)</span> with initialized <span class="math">\(s_{-1} = 0\)</span> to calculate <span class="math">\(s_0\)</span> and <span class="math">\(o_0\)</span>, then from <span class="math">\(s_0\)</span> together with <span class="math">\(x_1\)</span> to get <span class="math">\(s_1\)</span> and <span class="math">\(o_1\)</span>, and so on.</p>
<div class="highlight"><pre><span class="c1">## 1. forward propagation</span>

<span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">xt</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">xt</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward_propagation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># total num of time steps, len of vector x</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># during forward propagation, save all hidden stages in s, S_t = U .dot x_t + W .dot s_{t-1}</span>
    <span class="c1"># we also need the initial state of s, which is set to 0</span>
    <span class="c1"># each time step is saved in one row in sï¼Œeach row in s is s[t] which corresponding to an rnn internal loop time</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">))</span>
    <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_dim</span><span class="p">)</span>
    <span class="c1"># output at each time step saved as o, save them for later use</span>
    <span class="n">o</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">T</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dim</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="c1"># we are indexing U by x[t]. it is the same as multiplying U with a one-hot vector</span>
        <span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="p">[:,</span> <span class="n">x</span><span class="p">[</span><span class="n">t</span><span class="p">]]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">o</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]))</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">o</span><span class="p">,</span> <span class="n">s</span><span class="p">]</span>

<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">forward_propagation</span> <span class="o">=</span> <span class="n">forward_propagation</span>
</pre></div>


<p>Each <span class="math">\(o_t\)</span> here is a vector of prob representing the word in the vocabulary list. All we want is the next word with the predicted prob, we call it predict</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">o</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">predict</span> <span class="o">=</span> <span class="n">predict</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNNNumpy</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">forward_propagation</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">o</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">o</span><span class="p">)</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">predictions</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">predictions</span><span class="p">)</span> 
</pre></div>


<div class="highlight"><pre>(45, 8000)
[[ 0.00012408  0.0001244   0.00012603 ...,  0.00012515  0.00012488
   0.00012508]
 [ 0.00012536  0.00012582  0.00012436 ...,  0.00012482  0.00012456
   0.00012451]
 [ 0.00012387  0.0001252   0.00012474 ...,  0.00012559  0.00012588
   0.00012551]
 ..., 
 [ 0.00012414  0.00012455  0.0001252  ...,  0.00012487  0.00012494
   0.0001263 ]
 [ 0.0001252   0.00012393  0.00012509 ...,  0.00012407  0.00012578
   0.00012502]
 [ 0.00012472  0.0001253   0.00012487 ...,  0.00012463  0.00012536
   0.00012665]]
(45,)
[1284 5221 7653 7430 1013 3562 7366 4860 2212 6601 7299 4556 2481  238 2539
   21 6548  261 1780 2005 1810 5376 4146  477 7051 4832 4991  897 3485   21
 7291 2007 6006  760 4864 2182 6569 2800 2752 6821 4437 7021 7875 6912 3575]
</pre></div>


<h3>1.2. Lost Function</h3>
<p>We will use <a href="https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression">cross entropy</a> loss function here. If we have <span class="math">\(N\)</span> training examples(words in the text) and <span class="math">\(C\)</span> classes(the size of the vocabulary list), then the lost function with respect to the prediction <span class="math">\(o\)</span> and the true lable <span class="math">\(y\)</span> is:</p>
<div class="math">\begin{aligned}
L(y, o) = - \frac{1}{N}\sum_{n \in N}y_n \log o_n
\end{aligned}</div>
<p>How do we know this loss function makes sense? </p>
<p>If everything is predicted correctly, that is: for the <span class="math">\(y_i = 1\)</span>, the corresponding <span class="math">\(o_i = 1\)</span>, then <span class="math">\(1 \times \log(1) = 1 \times 0 = 0\)</span>. For the rest <span class="math">\(y_i = 0\)</span>, the multiplied value <span class="math">\(0 \times \log(o_i) = 0\)</span>. So the loss will be 0 for the perfect function.</p>
<p>On the other way, if the model have no prediction power, then all <span class="math">\(o_i = 1/C\)</span> then we will have <span class="math">\(L(y, o) = - \frac{1}{N}\sum_{n \in N} \log \frac{1}{C} = \log(C)\)</span>.</p>
<div class="highlight"><pre><span class="c1">## 2. calculate the loss</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">the loss is defined as</span>
<span class="sd">L(y, o) = -\frac{1}{N} \sum_{n \in N} y_n log(o_n)</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">calculate_total_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="c1"># for each sentence ...</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
        <span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="c1"># we only care about our prediction of the &quot;correct&quot; words</span>
        <span class="n">correct_word_predictions</span> <span class="o">=</span> <span class="n">o</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])),</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
        <span class="c1"># add to the loss based on how off we were</span>
        <span class="n">L</span> <span class="o">+=</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">correct_word_predictions</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">L</span>

<span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># divide the total loss by the number of training examples</span>
    <span class="n">N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span> <span class="k">for</span> <span class="n">y_i</span> <span class="ow">in</span> <span class="n">y</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_total_loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">/</span><span class="n">N</span>

<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">calculate_total_loss</span> <span class="o">=</span> <span class="n">calculate_total_loss</span>
<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">calculate_loss</span> <span class="o">=</span> <span class="n">calculate_loss</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Expected Loss for random prediction: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;Actual loss: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="n">X_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]))</span>
</pre></div>


<div class="highlight"><pre>Expected Loss for random prediction: 8.987197
Actual loss: 8.987440
</pre></div>


<h3>1.3. Model Training with Backward-Propagation</h3>
<p>Next we need to <strong>find the value of <span class="math">\(U, V, W\)</span> to minimize the loss function</strong>. SGD is the common way to do this. The idad behind SGD is: we iterate over all the training examples and during each iteration <strong>we nudge the parameters into a direction that reduces the error</strong>. <strong>The direction is given by the gradient of th loss <span class="math">\(\frac{\partial{L}}{\partial{U}}, \frac{\partial{L}}{\partial{V}}, \frac{\partial{L}}{\partial{W}}\)</span></strong>. SGD also needs <strong>learning rate</strong>, which defines how big of a step we want to make in each iteration. You can refer to <a href="http://cs231n.github.io/optimization-1/">here</a> for SGD introduction. SGD is a common method used in many numeric solutions to optimize the function.</p>
<p>Now the question becomes how to calculate the gradients <span class="math">\(\frac{\partial{L}}{\partial{U}}, \frac{\partial{L}}{\partial{V}}, \frac{\partial{L}}{\partial{W}}\)</span>. In the regular fully connected neural network, we use backpropagation to calculate it. In RNN it is a little more complicated because of the hidden status which links the current time step with the historical time step. So we need to calculate the gradients through the time. Thus we call this algorithm <strong>backpropagation through time</strong>(BPTT). The parameters are shared in all the time steps, the gradients at each output will not only depends on the current time step, but also the previous time steps. For general introduction of backpropagation, please read <a href="http://colah.github.io/posts/2015-08-Backprop/">this</a> and <a href="http://cs231n.github.io/optimization-2/">this</a>. Here I will give some detailed math formula about the BPTT used in this post. </p>
<p>Let's look at the graph of RNN below. Suppose now we are at time step <span class="math">\(t=3\)</span>, we want to calculate the gradients.</p>
<p><img alt="alt text" src="/figures/20170826_rnn_scratch_02_bptt_with_gradients.png" title="Logo Title Text 1" /></p>
<p>To make it clear, I will write the forward propagation explitly:
</p>
<div class="math">\begin{aligned}
&amp; s_0 = tanh(U x_0 + W s_{-1}) \\
&amp; z_0 = V s_0   \\
&amp; o_0 \triangleq \hat{y}_{0} = sigmoid(z_0) \\\\
&amp; s_1 = tanh(U x_1 + W s_0) \\
&amp; z_1 = V s_1   \\
&amp; o_1 \triangleq \hat{y}_{1} = sigmoid(z_1) \\\\
&amp; s_2 = tanh(U x_2 + W s_1) \\
&amp; z_2 = V s_2   \\
&amp; o_2 \triangleq \hat{y}_{2} = sigmoid(z_2) \\\\
&amp; s_3 = tanh(U x_3 + W s_2) \\
&amp; z_3 = V s_3   \\
&amp; o_3 \triangleq \hat{y}_{3} = sigmoid(z_3) \\
\end{aligned}</div>
<p>Also, for simplification, I will treat everything as scale rather then vectors or matrix. This will make the partial derivative easy to understand. After we understand this, we only need tiny modification to replace some multiplication by the array dot multiply.</p>
<p>First, let's denote
</p>
<div class="math">\begin{aligned}
&amp; d_3 \triangleq \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \\
&amp; d_2 \triangleq d_3 \cdot W \cdot \big(1 - s_2 ^ 2 \big) \\
&amp; d_1 \triangleq d_2 \cdot W \cdot \big(1 - s_1 ^ 2 \big) \\
&amp; d_0 \triangleq d_1 \cdot W \cdot \big(1 - s_0 ^ 2 \big) \\
\end{aligned}</div>
<h4>1.3.1. Calculate of partial derivative of error <span class="math">\(E_3\)</span> to <span class="math">\(U\)</span>: <span class="math">\(\frac{\partial{L}}{\partial{U}}\)</span></h4>
<p>We have already know each historical hidden status will be used to calculate current status. The parameters are used in each status. So we need to calculate all the hidden status to the parameter <span class="math">\(U\)</span>.</p>
<div class="math">\begin{aligned}
\frac{\partial{s_0}}{\partial{U}} &amp;= \big(1 - s_0 ^ 2 \big) \left(x_0 + \frac{\partial{s_{-1}}}{\partial{U}} \right) \\
&amp;= \big(1 - s_0 ^ 2 \big) \cdot x_0 \\\\
\frac{\partial{s_1}}{\partial{U}} &amp;= \big(1 - s_1 ^ 2 \big) \left(x_1 + W \cdot \frac{\partial{s_{0}}}{\partial{U}} \right) \\
&amp;= \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\\\
\frac{\partial{s_2}}{\partial{U}} &amp;= \big(1 - s_2 ^ 2 \big) \left(x_2 + W \cdot \frac{\partial{s_{1}}}{\partial{U}} \right) \\
&amp;= \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)\\\\
\frac{\partial{s_3}}{\partial{U}} &amp;= \big(1 - s_3 ^ 2 \big) \left(x_3 + W \cdot \frac{\partial{s_{2}}}{\partial{U}} \right) \\
&amp;= \big(1 - s_3 ^ 2 \big) \\
&amp; \bigg(x_3 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)  \bigg)\\\\
\end{aligned}</div>
<p>After we get this, we can calculate the partial derivative of error <span class="math">\(E3\)</span> to <span class="math">\(U\)</span>:</p>
<div class="math">\begin{aligned}
\frac{\partial{E_3}}{\partial{U}} &amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{s_3}} \frac{\partial{s_3}}{\partial{U}}  \\
&amp;= \left(\frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \right) \cdot \frac{\partial{z_3}}{\partial{s_3}} \cdot \frac{\partial{s_3}}{\partial{U}}  \\
&amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \frac{\partial{s_3}}{\partial{U}}  \\
&amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \bigg(x_3 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)  \bigg)\\
&amp; \triangleq d_3 \big[x_3 + W \cdot  \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big)   \big]\\
&amp;= d_3 x_3 + d_3 W \cdot \big(1 - s_2 ^ 2 \big) \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \Big) \\
&amp; \triangleq d_3 x_3 + d_2 \Big(x_2 + W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big)\Big) \\
&amp;= d_3 x_3 + d_2 x_2 + d_2 W \cdot \big(1 - s_1 ^ 2 \big) \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\
&amp; \triangleq d_3 x_3 + d_2 x_2 + d_1 \big(x_1 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0 \big) \\
&amp;= d_3 x_3 + d_2 x_2 + d_1 x_1 + d_1 W \cdot \big(1 - s_0 ^ 2 \big) \cdot x_0  \\
&amp; \triangleq d_3 x_3 + d_2 x_2 + d_1 x_1 + d_0 \cdot x_0  \\
\end{aligned}</div>
<h4>1.3.2. Calculate of partial derivative of error <span class="math">\(E_3\)</span> to <span class="math">\(W\)</span>: <span class="math">\(\frac{\partial{L}}{\partial{W}}\)</span></h4>
<p>First is each hidden status to <span class="math">\(W\)</span>:</p>
<div class="math">\begin{aligned}
\frac{\partial{s_0}}{\partial{W}} &amp;= \big(1 - s_0 ^ 2 \big) \left(s_{-1} + \frac{\partial{s_{-1}}}{\partial{W}} \right) \\
&amp;= \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \\\\
\frac{\partial{s_1}}{\partial{W}} &amp;= \big(1 - s_1 ^ 2 \big) \left(s_0 + W \cdot \frac{\partial{s_{0}}}{\partial{W}} \right) \\
&amp;= \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\\\
\frac{\partial{s_2}}{\partial{W}} &amp;= \big(1 - s_2 ^ 2 \big) \left(s_1 + W \cdot \frac{\partial{s_{1}}}{\partial{W}} \right) \\
&amp;= \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)\\\\
\frac{\partial{s_3}}{\partial{W}} &amp;= \big(1 - s_3 ^ 2 \big) \left(s_2 + W \cdot \frac{\partial{s_{2}}}{\partial{W}} \right) \\
&amp;= \big(1 - s_3 ^ 2 \big) \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)\\\\
\end{aligned}</div>
<p>Then we can write the partial derivative of error <span class="math">\(E3\)</span> to <span class="math">\(W\)</span>:</p>
<div class="math">\begin{aligned}
\frac{\partial{E_3}}{\partial{W}} &amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{s_3}} \frac{\partial{s_3}}{\partial{W}}  \\
&amp;= \left(\frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \right) \cdot \frac{\partial{z_3}}{\partial{s_3}} \cdot \frac{\partial{s_3}}{\partial{W}}  \\
&amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \frac{\partial{s_3}}{\partial{W}}  \\
&amp;= \big(\hat{y}_3 - y_3 \big) \cdot V \cdot \big(1 - s_3 ^ 2 \big) \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)\\
&amp; \triangleq d_3    \bigg(s_2 + W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big)  \bigg)   \\
&amp;= d_3 s_2 + d_3 W \cdot \big(1 - s_2 ^ 2 \big) \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big) \\
&amp; \triangleq d_3 s_2 + d_2 \Big(s_1 + W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \Big) \\
&amp;= d_3 s_2 + d_2 s_1 + d_2 W \cdot \big(1 - s_1 ^ 2 \big) \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\
&amp; \triangleq d_3 s_2 + d_2 s_1 + d_1 \big(s_0 + W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1} \big) \\
&amp;= d_3 s_2 + d_2 s_1 + d_1 s_0 + d_1 W \cdot \big(1 - s_0 ^ 2 \big) \cdot s_{-1}  \\
&amp; \triangleq d_3 s_2 + d_2 s_1 + d_1 s_0 + d_0 \cdot s_{-1}  \\
\end{aligned}</div>
<h4>1.3.3. Calculate of partial derivative of error <span class="math">\(E_3\)</span> to <span class="math">\(V\)</span>: <span class="math">\(\frac{\partial{L}}{\partial{V}}\)</span></h4>
<p>This will be easier than the two above:
</p>
<div class="math">\begin{aligned}
\frac{\partial{E_3}}{\partial{V}} &amp;= \frac{\partial{E_3}}{\partial{\hat{y}_3}} \frac{\partial{\hat{y}_3}}{\partial{z_3}} \frac{\partial{z_3}}{\partial{V}} \\
&amp;= (\hat{y}_{3} - y_3)  s_3
\end{aligned}</div>
<p>This simplifies the equations from vector and matrix multiply to scale multiply. But with this it can easily revert back to vertor and matrix, just need to pay attention of the orders, shape, transformation and dot multiply.</p>
<p>From the derivatives formula above, we can easily write the BPTT in python:</p>
<div class="highlight"><pre><span class="c1">## 3. BPTT</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">1. we nudge the parameters into a direction that reduces the error. the direction is given by the gradient of the loss: \frac{\partial L}{\partial U}, </span>
<span class="sd">\frac{\partial L}{\partial V}, \frac{\partial L}{\partial W}</span>
<span class="sd">2. we also need learning rate: which indicated how big of a step we want to make in each direction</span>
<span class="sd">Q: how to optimize SGD using batching, parallelism and adaptive learning rates.</span>

<span class="sd">RNN BPTT: because the parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the</span>
<span class="sd">current time step, but also the previous time steps.</span>
<span class="sd">&#39;&#39;&#39;</span>

<span class="k">def</span> <span class="nf">bptt</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c1"># perform forward propagation</span>
    <span class="n">o</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward_propagation</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># we will accumulate the gradients in these variables</span>
    <span class="n">dLdU</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">U</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dLdV</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">dLdW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">delta_o</span> <span class="o">=</span> <span class="n">o</span>
    <span class="n">delta_o</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>   <span class="c1"># it is y_hat - y</span>
    <span class="c1"># for each output backwards ...</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
        <span class="n">dLdV</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_o</span><span class="p">[</span><span class="n">t</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>    <span class="c1"># at time step t, shape is word_dim * hidden_dim</span>
        <span class="c1"># initial delta calculation</span>
        <span class="n">delta_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">V</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta_o</span><span class="p">[</span><span class="n">t</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">^</span> <span class="mi">2</span><span class="p">))</span>
        <span class="c1"># backpropagation through time (for at most self.bptt_truncate steps)</span>
        <span class="c1"># given time step t, go back from time step t, to t-1, t-2, ...</span>
        <span class="k">for</span> <span class="n">bptt_step</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">bptt_truncate</span><span class="p">),</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="c1"># print(&quot;Backprogation step t=%d bptt step=%d&quot; %(t, bptt_step))</span>
            <span class="n">dLdW</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">delta_t</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="n">bptt_step</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span>
            <span class="n">dLdU</span><span class="p">[:,</span> <span class="n">x</span><span class="p">[</span><span class="n">bptt_step</span><span class="p">]]</span> <span class="o">+=</span> <span class="n">delta_t</span>
            <span class="c1"># update delta for next step</span>
            <span class="n">dleta_t</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">delta_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">s</span><span class="p">[</span><span class="n">bptt_step</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">^</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dLdU</span><span class="p">,</span> <span class="n">dLdV</span><span class="p">,</span> <span class="n">dLdW</span><span class="p">]</span>

<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">bptt</span> <span class="o">=</span> <span class="n">bptt</span>
</pre></div>


<p>When we implement the backpropagaton it is good idea to also implement gradient checking. The idad behind gradient checking is that derivative of a parameter is equal to the slope at that point, which we can approximate by slighyly changing the parameter and then dividing by the change:</p>
<div class="math">\begin{aligned}
\frac{\partial{L}}{\partial{\theta}} = \lim_{h \rightarrow 0} \frac{L(\theta + h) - L(\theta - h)}{2h}
\end{aligned}</div>
<p>We will compare the calculate gradient using the limitation approaching formula above with the gradients from the derivatives. They should be very close. The approximation needs to calculate the total loss for every parameter, so the gradient checking is very expensive. So it is a good idea to perform it on a model with a smaller vocabulary.</p>
<div class="highlight"><pre>
</pre></div>


<div class="highlight"><pre><span class="c1">### 3.1 gradient checking</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">verify the gradient by its definition:</span>
<span class="sd">\frac{\partial{L}}{\partial{\theta}} = \lim_{h \propto 0} \frac{J(\theta + h) - J(\theta - h)}{2h}</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">gradient_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="mf">0.001</span><span class="p">,</span> <span class="n">error_threshold</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">):</span>
    <span class="c1"># calculate the gradient using backpropagation</span>
    <span class="n">bptt_gradients</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bptt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># list of all params we want to check</span>
    <span class="n">model_parameters</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;U&quot;</span><span class="p">,</span> <span class="s2">&quot;V&quot;</span><span class="p">,</span> <span class="s2">&quot;W&quot;</span><span class="p">]</span>
    <span class="c1"># gradient check for each parameter</span>
    <span class="k">for</span> <span class="n">pidx</span><span class="p">,</span> <span class="n">pname</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model_parameters</span><span class="p">):</span>
        <span class="c1"># get the actual parameter value from model, e.g. model.W</span>
        <span class="n">parameter</span> <span class="o">=</span> <span class="n">operator</span><span class="o">.</span><span class="n">attrgetter</span><span class="p">(</span><span class="n">pname</span><span class="p">)(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;performing gradient check for parameter </span><span class="si">%s</span><span class="s2"> with size </span><span class="si">%d</span><span class="s2">. &quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">pname</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">shape</span><span class="p">)))</span>
        <span class="c1"># iterate over each element of the parameter matrix, e.g. (0,0), (0,1)...</span>
        <span class="n">it</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nditer</span><span class="p">(</span><span class="n">parameter</span><span class="p">,</span> <span class="n">flags</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;multi_index&#39;</span><span class="p">],</span> <span class="n">op_flags</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;readwrite&#39;</span><span class="p">])</span>
        <span class="k">while</span> <span class="ow">not</span> <span class="n">it</span><span class="o">.</span><span class="n">finished</span><span class="p">:</span>
            <span class="n">ix</span> <span class="o">=</span> <span class="n">it</span><span class="o">.</span><span class="n">multi_index</span>
            <span class="c1"># save the original value so we can reset it later</span>
            <span class="n">original_value</span> <span class="o">=</span> <span class="n">parameter</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span>
            <span class="c1"># estimate the gradient using (f(x+h) - f(x-h))/2h</span>
            <span class="n">parameter</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_value</span> <span class="o">+</span> <span class="n">h</span>
            <span class="n">gradplus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_total_loss</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>
            <span class="n">parameter</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_value</span> <span class="o">-</span> <span class="n">h</span>
            <span class="n">gradminus</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_total_loss</span><span class="p">([</span><span class="n">x</span><span class="p">],</span> <span class="p">[</span><span class="n">y</span><span class="p">])</span>
            <span class="n">estimated_gradient</span> <span class="o">=</span> <span class="p">(</span><span class="n">gradplus</span> <span class="o">-</span> <span class="n">gradminus</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
            <span class="c1"># reset parameter to the original value</span>
            <span class="n">parameter</span><span class="p">[</span><span class="n">ix</span><span class="p">]</span> <span class="o">=</span> <span class="n">original_value</span>
            <span class="c1"># the gradient for this parameter calculated using backpropagation</span>
            <span class="n">backprop_gradient</span> <span class="o">=</span> <span class="n">bptt_gradients</span><span class="p">[</span><span class="n">pidx</span><span class="p">][</span><span class="n">ix</span><span class="p">]</span>
            <span class="c1"># calculate the relative error (|x - y|)/(|x|+|y|)</span>
            <span class="n">relative_error</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">backprop_gradient</span> <span class="o">-</span> <span class="n">estimated_gradient</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">backprop_gradient</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">estimated_gradient</span><span class="p">))</span>
            <span class="c1"># if the error is too large fail the gradient check</span>
            <span class="k">if</span> <span class="n">relative_error</span> <span class="o">&lt;</span> <span class="n">error_threshold</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Gradient check error: parameter = </span><span class="si">%s</span><span class="s2"> ix = </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">pname</span><span class="p">,</span> <span class="n">ix</span><span class="p">))</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;+h Loss: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">gradplus</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;-h Loss: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">gradminus</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Estimated gradient: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">estimated_gradient</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Backpropagation gradient: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">backprop_gradient</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Relative error: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">relative_error</span><span class="p">)</span>
                <span class="k">return</span>
            <span class="n">it</span><span class="o">.</span><span class="n">iternext</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="s2">&quot;Gradient check for parameter </span><span class="si">%s</span><span class="s2"> passed. &quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">pname</span><span class="p">))</span>

<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">gradient_check</span> <span class="o">=</span> <span class="n">gradient_check</span>

<span class="n">grad_check_vocab_size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNNNumpy</span><span class="p">(</span><span class="n">grad_check_vocab_size</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bptt_truncate</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">gradient_check</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">])</span>
</pre></div>


<div class="highlight"><pre>performing gradient check for parameter U with size 1000. 
Gradient check error: parameter = U ix = (0, 3)
+h Loss: 18.307080
-h Loss: 18.307296
Estimated gradient: -0.108091
Backpropagation gradient: -0.108091
Relative error: 0.000000
</pre></div>


<h2>2. SGD Implementation</h2>
<p>Now that we can calculate the gradients for our parameters we can implement SGD in two steps: 1. A function <em>sgd_step</em> that calculate the gradients and performs the updates for one batch(here one batch is one sentence). 2. An output loop that iterates through the training data and adjust the learning rate.</p>
<div class="highlight"><pre><span class="c1">## 4. SGD implementation</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd">two step:</span>
<span class="sd">1. calculate the gradients and perform the updates for one batch</span>
<span class="sd">2. loop through the training set and adjust the learning rate</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="c1">### 4.1. perform one step of SGD</span>
<span class="k">def</span> <span class="nf">numpy_sgd_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="n">dLdU</span><span class="p">,</span> <span class="n">dLdV</span><span class="p">,</span> <span class="n">dLdW</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bptt</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">U</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dLdU</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">V</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dLdV</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">dLdW</span>
<span class="n">RNNNumpy</span><span class="o">.</span><span class="n">sgd_step</span> <span class="o">=</span> <span class="n">numpy_sgd_step</span>

<span class="c1">### 4.2. outer SGD loop</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="sd"> - model: </span>
<span class="sd"> - X_train:</span>
<span class="sd"> - y_train:</span>
<span class="sd"> - learning_rate:</span>
<span class="sd"> - nepoch:</span>
<span class="sd"> - evaluate loss_after:</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">def</span> <span class="nf">train_with_sgd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.005</span><span class="p">,</span> <span class="n">nepoch</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="n">evaluate_loss_after</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="c1"># keep track of the losses so that we can plot them later</span>
    <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">num_examples_seen</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nepoch</span><span class="p">):</span>
        <span class="c1"># optionally evaluate the loss</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">epoch</span> <span class="o">%</span> <span class="n">evaluate_loss_after</span> <span class="o">==</span> <span class="mi">0</span><span class="p">):</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">num_examples_seen</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
            <span class="n">time</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s1">&#39;%Y-%m-</span><span class="si">%d</span><span class="s1"> %H:%M:%S&#39;</span><span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">%s</span><span class="s2">: loss after num_examples_seen=</span><span class="si">%d</span><span class="s2"> epoch=</span><span class="si">%d</span><span class="s2">: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">time</span><span class="p">,</span> <span class="n">num_examples_seen</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">loss</span><span class="p">))</span>
            <span class="c1"># adjust the learning rate if loss increases</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">losses</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">][</span><span class="mi">1</span><span class="p">]):</span>
                <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="mf">0.5</span>
                <span class="k">print</span><span class="p">(</span><span class="s2">&quot;setting learning rate to </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">))</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">flush</span><span class="p">()</span>
        <span class="c1"># for each training example...</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_train</span><span class="p">)):</span>
            <span class="c1"># one sgd step</span>
            <span class="n">model</span><span class="o">.</span><span class="n">sgd_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">learning_rate</span><span class="p">)</span>
            <span class="n">num_examples_seen</span> <span class="o">+=</span> <span class="mi">1</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNNNumpy</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">model</span><span class="o">.</span><span class="n">sgd_step</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="mf">0.005</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>1 loop, best of 3: 175 ms per loop
</pre></div>


<div class="highlight"><pre><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RNNNumpy</span><span class="p">(</span><span class="n">vocabulary_size</span><span class="p">)</span>
<span class="n">losses</span> <span class="o">=</span> <span class="n">train_with_sgd</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">y_train</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">nepoch</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">evaluate_loss_after</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>2017-09-21 02:59:52: loss after num_examples_seen=0 epoch=0: 8.987425
2017-09-21 03:00:04: loss after num_examples_seen=100 epoch=1: 8.974076
2017-09-21 03:00:18: loss after num_examples_seen=200 epoch=2: 8.943971
2017-09-21 03:00:30: loss after num_examples_seen=300 epoch=3: 6.892136
2017-09-21 03:00:40: loss after num_examples_seen=400 epoch=4: 6.351962
2017-09-21 03:00:50: loss after num_examples_seen=500 epoch=5: 6.107587
2017-09-21 03:01:00: loss after num_examples_seen=600 epoch=6: 5.960636
2017-09-21 03:01:11: loss after num_examples_seen=700 epoch=7: 5.858011
2017-09-21 03:01:22: loss after num_examples_seen=800 epoch=8: 5.781543
2017-09-21 03:01:32: loss after num_examples_seen=900 epoch=9: 5.722384
</pre></div>


<p>Since this RNN is implemented in python without code optimization, the running time is pretty long for our 79,170 words in each epoch. But we can try a small sample data and check if the loss actually decreases:</p>
<h3>Reference</h3>
<ol>
<li><a href="http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/">Recurrent Neural Networks Tutorial, Part 2 â€“ Implementing a RNN with Python, Numpy and Theano</a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-08-20T16:08:00-05:00" pubdate>Sun 20 August 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/deep-learning.html">deep learning</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/">Credit Card Fraud Detection / Imbalanced data modeling - Part II: Random Forest</a>
      </li>
      <li class="post">
          <a href="/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/">Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression</a>
      </li>
      <li class="post">
          <a href="/pages/2018/03/18/leetcode-139-word-break/">leetcode 139. Word Break</a>
      </li>
      <li class="post">
          <a href="/pages/2018/03/18/leetcode-140-word-break-ii/">leetcode 140. Word Break II</a>
      </li>
      <li class="post">
          <a href="/pages/2018/03/18/leetcode-174-dungeon-game/">leetcode 174. Dungeon Game</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">Python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2018  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/08/20/build-recurrent-neural-network-from-scratch/';
    var disqus_url = '/pages/2017/08/20/build-recurrent-neural-network-from-scratch/';
    var disqus_title = 'Build Recurrent Neural Network from Scratch';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>