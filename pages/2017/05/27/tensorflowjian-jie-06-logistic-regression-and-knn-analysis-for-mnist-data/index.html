<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Tensorflow简介--06: Logistic regression and KNN analysis for MNIST data &mdash; pydata: Huiming's learning notes</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><header>
  <h1><a href="/">pydata: Huiming's learning notes</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</header>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  // 允许 $...$ 和 \( ... \) 作为行内公式
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  // 允许 $$...$$ 和 \[ ... \] 作为块级公式
    processEscapes: true,  // 允许在公式中使用转义符，如 \$ 表示美元符号
    processEnvironments: true  // 允许解析 \begin{equation} ... \end{equation} 等数学环境
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],  // 跳过某些 HTML 标签，防止错误解析
    renderActions: {
      addMenu: []  // 移除右键菜单
    }
  }
};

window.addEventListener('load', () => {
  document.querySelectorAll("mjx-container").forEach(x => {
    x.parentElement.classList.add('has-jax');  // 使用 classList.add() 避免字符串拼接错误
  });
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></header>
  <nav role="navigation">

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Tensorflow简介--06: Logistic regression and KNN analysis for MNIST data</h1>
    <p class="meta">
<time datetime="2017-05-27T00:00:00-05:00" pubdate>Sat 27 May 2017</time>    </p>
</header>

  <div class="entry-content"><pre><code class="language-python">import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(&quot;MNIST_data/&quot;, one_hot=True)
</code></pre>
<h2>Data Introduction</h2>
<p>The backgroupnd of MNIST data is introduced in <a href="https://www.tensorflow.org/versions/r0.11/tutorials/mnist/beginners/">MNIST For ML Beginners</a></p>
<p>The MNIST data is split into three parts: 55,000 data points of training data (<code>mnist.train</code>), 10,000 points of test data (<code>mnist.test</code>), and 5,000 points of validation data (<code>mnist.validation</code>). This split is very important: it's essential in machine learning that we have separate data which we don't learn from so that we can make sure that what we've learned actually generalizes!</p>
<p>the training images are <code>mnist.train.images</code> and the training labels are <code>mnist.train.labels</code>.</p>
<p>Each image is 28 pixels by 28 pixels. We can interpret this as a big array of numbers. We can flatten this array into a vector of 28x28 = 784 numbers.</p>
<p>The result is that <code>mnist.train.images</code> is a tensor (an n-dimensional array) with a shape of <code>[55000, 784]</code>. The first dimension is an index into the list of images and the second dimension is the index for each pixel in each image. Each entry in the tensor is a pixel intensity between 0 and 1, for a particular pixel in a particular image.</p>
<p>Each image in MNIST has a corresponding label, a number between 0 and 9 representing the digit drawn in the image.</p>
<p>For the purposes of this tutorial, we're going to want our labels as "one-hot vectors". A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the nth digit will be represented as a vector which is 1 in the nth dimension. For example, 3 would be <span class="math">\([0,0,0,1,0,0,0,0,0,0]\)</span>. Consequently, <code>mnist.train.labels</code> is a <code>[55000, 10]</code> array of floats.</p>
<h2>1. Softmax Regressions (multinomial logistic regression)</h2>
<p>Here is an introduction of <a href="http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/">Softmax Regression</a>. It is a multivariate logistic regression.</p>
<div class="math">$$
\begin{aligned}
    &amp; \text{evidence}_i = \sum_j W_{i,~ j} x_j + b_i \\\
    &amp; y = \text{softmax}(\text{evidence})   \\\
    &amp; \text{softmax}(x) = \text{normalize}(\exp(x))   \\\
    &amp; \text{softmax}(x)_i = \frac{\exp(x_i)}{\sum_j \exp(x_j)}
\end{aligned}
$$</div>
<h3>Implementation</h3>
<p>First we will define the input data.</p>
<pre><code class="language-python">init_param = lambda shape: tf.random_normal(shape, dtype=tf.float32)

with tf.name_scope(&quot;IO&quot;):
    inputs = tf.placeholder(tf.float32, [None, 784], name=&quot;X&quot;)
    targets = tf.placeholder(tf.float32, [None, 10], name=&quot;Yhat&quot;)
</code></pre>
<p>As explained above, Input data(training data x) is the array with shape of [55000, 784]. The target variable shape is [55000, 10]. <code>placeholder</code>: a value that we'll input when we ask TensorFlow to run a computation. It will enable you to assemble the graph first without knowing the values needed for computation. In the computation, you can feed the values to placeholders using a dictionary. <code>None</code> means that a dimension can be of any length.</p>
<p>Next we will define the variables in the model. </p>
<p><code>Variable</code> is a modifiable tensor that lives in TensorFlow's graph of interacting operations. <code>tf.Variable</code> is a class, but <code>tf.constant</code> is an operation. <code>tf.Variable</code> also hold some operations lile <code>assign</code>. <code>tf.Variable</code> must be initialized and can be evaluated to get the values.</p>
<pre><code class="language-python">with tf.name_scope(&quot;LogReg&quot;):
    W = tf.Variable(init_param([784, 10]), name=&quot;W&quot;)
    B = tf.Variable(init_param([10]))
    logits = tf.matmul(inputs, W) + B
    y = tf.nn.softmax(logits)
</code></pre>
<p>Then we will define the loss function. The purpose it to optimize(minimize) the loss function to find the value of the parameters defined as variables.</p>
<pre><code class="language-python">with tf.name_scope(&quot;train&quot;):
    learning_rate = tf.Variable(0.5, trainable=False)
    cost_op = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = targets)
    cost_op = tf.reduce_mean(cost_op) 
    train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_op)

    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(targets,1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))*100
</code></pre>
<p>Finally is to loop through the epochs to minimize the loss function. The iteration will stop when it reaches the stoping condition or the max of epoch.</p>
<p>Sometimes the input data will be very big(considering training the pictures with CNN in computer vision / object recognizatio). If dump all data into memory, it will cause the system crashed unless you have lots of ram installed. To avoid this, the best way is to split the input into different batches, then read in and train each batch. Please refer to <a href="http://songhuiming.github.io/pages/2017/02/26/tensorflowjian-jie-02/">tensorflow--02</a> for the details how batch and mini-batch works.</p>
<pre><code class="language-python">tolerance = 1e-4
epochs = 1
last_cost = 0.0
alpha = 0.5
max_epochs = 10
batch_size = 100
costs = []
sess = tf.Session()

with sess.as_default():
    init = tf.global_variables_initializer()
    sess.run(init)
    sess.run(tf.assign(learning_rate, alpha))
    writer = tf.summary.FileWriter(&quot;./tfboard&quot;, sess.graph)
    while True:

        num_batches = int(mnist.train.num_examples/batch_size)
        cost=0
        for _ in range(num_batches):
            batch_xs, batch_ys = mnist.train.next_batch(batch_size)
            tcost, _ = sess.run([cost_op, train_op], feed_dict={inputs: batch_xs, targets: batch_ys})
            cost += tcost
        cost /= num_batches

        tcost = sess.run(cost_op, feed_dict={inputs: mnist.test.images, targets: mnist.test.labels})
        costs.append([cost, tcost])

        if epochs%5==0:
            acc = sess.run(accuracy, feed_dict={inputs: mnist.train.images, targets: mnist.train.labels})
            print (&quot;Epoch: %d - Error: %.4f - Accuracy - %.2f%%&quot; %(epochs, cost, acc))

            if abs(last_cost - cost) &lt; tolerance or epochs &gt; max_epochs:
                break

            last_cost = cost

        epochs += 1

    tcost, taccuracy = sess.run([cost_op, accuracy], feed_dict={inputs: mnist.test.images, targets: mnist.test.labels})
    print (&quot;Test Cost: %.4f - Accuracy: %.2f%% &quot; %(tcost, taccuracy))
    test_plot = sess.run(tf.argmax(y, 1), feed_dict = {inputs: mnist.test.images[:9]})

</code></pre>
<pre><code>Epoch: 5 - Error: 0.4307 - Accuracy - 89.50%
Epoch: 10 - Error: 0.3502 - Accuracy - 91.15%
Epoch: 15 - Error: 0.3164 - Accuracy - 91.91%
Test Cost: 0.3292 - Accuracy: 91.27%
</code></pre>
<pre><code class="language-python">%matplotlib inline
fig = plt.figure(dpi=100, figsize=(9, 6))
labels = np.argmax(mnist.test.labels[:9], axis = 1)
for i in range(9):
    fig.add_subplot(3, 3, i+1)
    plt.imshow(mnist.test.images[i].reshape(28, 28))
    plt.axis(&quot;off&quot;)
    plt.tight_layout()
    plt.title(&quot;pred: &quot; + str(test_plot[i]) + ', actual: ' + str(labels[i]))
    #frame = plt.gca()
    #frame.axes.get_xaxis().set_visible(False)
    #frame.axes.get_yaxis().set_visible(False)

plt.show()
</code></pre>
<p><img alt="png" src="/figures/20170527_tf_06_mnist_logistic_knn_01.png"></p>
<h2>2. KNN</h2>
<p><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">KNN</a> is a non-parametric method for classification and regression. It will measure the distance and group the k nearest data together for classification or regression.</p>
<p>A common used distance is Euclidean distance given by
</p>
<div class="math">$$
d(x, x')=\sqrt{(x_1 − x′_1)^2+(x_2−x′_2)^2+ \cdots +(x_n−x′_n)^2}
$$</div>
<p>More formally, given a positive integer <span class="math">\(K\)</span>, an unseen observation <span class="math">\(x\)</span> and a similarity metric <span class="math">\(d\)</span>, KNN classifier performs the following two steps:</p>
<p>1, It runs through the whole dataset computing <span class="math">\(d\)</span> between <span class="math">\(x\)</span> and each training observation. We’ll call the <span class="math">\(K\)</span> points in the training data that are closest to <span class="math">\(x\)</span> the set <span class="math">\(\mathcal{A}\)</span>. Note that <span class="math">\(K\)</span> is usually odd to prevent tie situations.</p>
<p>2, It then estimates the conditional probability for each class, that is, the fraction of points in <span class="math">\(\mathcal{A}\)</span> with that given class label. (Note <span class="math">\(I(x)\)</span> is the indicator function which evaluates to 1 when the argument <span class="math">\(x\)</span> is true and 0 otherwise)</p>
<div class="math">$$
P(y = j | X = x) = \frac{1}{K} \sum_{i \in \mathcal{A}} I(y^{(i)} = j)
$$</div>
<p>Finally, our input <span class="math">\(x\)</span> gets assigned to the class with the largest probability.</p>
<pre><code class="language-python">sess = tf.Session()


np.random.seed(13)  # set seed for reproducibility
train_size = 2000
test_size = 200
rand_train_indices = np.random.choice(len(mnist.train.images), train_size, replace=False)
rand_test_indices = np.random.choice(len(mnist.test.images), test_size, replace=False)
x_vals_train = mnist.train.images[rand_train_indices]
x_vals_test = mnist.test.images[rand_test_indices]
y_vals_train = mnist.train.labels[rand_train_indices]
y_vals_test = mnist.test.labels[rand_test_indices]

k = 5
batch_size=5

with tf.name_scope(&quot;IO&quot;):
# Placeholders
    x_data_train = tf.placeholder(shape=[None, 784], dtype=tf.float32)
    x_data_test = tf.placeholder(shape=[None, 784], dtype=tf.float32)
    y_target_train = tf.placeholder(shape=[None, 10], dtype=tf.float32)
    y_target_test = tf.placeholder(shape=[None, 10], dtype=tf.float32)

with tf.name_scope(&quot;KNN&quot;):    
    #each train and each test dist
    distance = tf.reduce_sum(tf.abs(tf.subtract(x_data_train, tf.expand_dims(x_data_test,1))), axis=2) 
    # Get min distance index (Nearest neighbor)
    top_k_xvals, top_k_indices = tf.nn.top_k(tf.negative(distance), k=k)
    prediction_indices = tf.gather(y_target_train, top_k_indices)
    # Predict the mode category: k nearest nbrs may result in different preds, pick the pred with highest freq
    count_of_predictions = tf.reduce_sum(prediction_indices, axis=1)
    prediction = tf.argmax(count_of_predictions, axis=1)


num_loops = int(np.ceil(len(x_vals_test)/batch_size))
test_output = []
actual_vals = []

with sess.as_default():
    for i in range(num_loops):
        init = tf.global_variables_initializer()
        sess.run(init)
        min_index = i*batch_size
        max_index = min((i+1)*batch_size,len(x_vals_train))
        x_batch = x_vals_test[min_index:max_index] 
        y_batch = y_vals_test[min_index:max_index]
        predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test: x_batch,
                                             y_target_train: y_vals_train, y_target_test: y_batch})
        test_output.extend(predictions)
        actual_vals.extend(np.argmax(y_batch, axis=1))

    accuracy = sum([1./test_size for i in range(test_size) if test_output[i]==actual_vals[i]])
    print(&quot;Accuracy on test data: %.2f%% &quot; %(accuracy))
</code></pre>
<pre><code>Accuracy on test data: 0.87%
</code></pre>
<pre><code class="language-python">%matplotlib inline
fig = plt.figure(dpi=100, figsize=(9, 6))


for i in range(9):
    fig.add_subplot(3, 3, i+1)
    plt.imshow(x_vals_test[:9][i].reshape(28, 28))
    plt.axis(&quot;off&quot;)
    plt.tight_layout()
    plt.title(&quot;pred: &quot; + str(test_output[:9][i]) + ', actual: ' + str(actual_vals[:9][i]))

plt.show()
</code></pre>
<p><img alt="png" src="/figures/20170527_tf_06_mnist_logistic_knn_02.png"></p>
<h3>broadcasting</h3>
<p>the dimension of xtrain and xtest are different. so the substraction cannot be applied directly. <code>tf.expand_dims(x_vals_test,1)</code> will add one dimension on the data. After expanding the dim, the substraction is fine becasue of broadcasting. It is the same as <code>np.expand_dim</code>. An example is given below.</p>
<pre><code class="language-python">import numpy as np
x = np.arange(100).reshape(20, 5)  # 20x5
y = np.arange(20).reshape(4, 5)    # 4x5
x - y # error

print (np.expand_dims(y, 1).shape)        # 4x1x5
print((x  - np.expand_dims(y, 1)).shape)  # 4x20x5
print(np.sum(np.abs((x  - np.expand_dims(y, 1))), axis = 2).shape)    # 4x20
</code></pre>
<pre><code>(4, 1, 5)
(4, 20, 5)
(4, 20)
</code></pre>
<p><code>tf.reduce_sum(, axis = 2)</code> will calculate the sum on the given axis = 2. It is similar to <code>np.sum(, axis = 2)</code>                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </p>
<pre><code class="language-python">np.random.seed(99)
float_formatter = lambda x: &quot;%.2f&quot; % x

s = np.round(np.random.normal(0, 1, 10), 2) 
#s = np.array(map(float_formatter, s))

print s

print(-s[np.argsort(-s)[::-1][:2]])

with tf.Session() as sess:
    sess.run(init)
    print sess.run(tf.nn.top_k(tf.negative(s), k=2))
</code></pre>
<pre><code>[-0.14  2.06  0.28  1.33 -0.15 -0.07  0.76  0.83 -0.11 -2.37]

[ 2.37  0.15]

TopKV2(values=array([ 2.37,  0.15]), indices=array([9, 4], dtype=int32))
</code></pre>
<h2>Reference</h2>
<ol>
<li>
<p><a href="https://docs.scipy.org/doc/numpy-1.12.0/user/basics.broadcasting.html">Broadcasting</a></p>
</li>
<li>
<p><a href="http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc">Array Broadcasting in numpy</a></p>
</li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-05-27T00:00:00-05:00" pubdate>Sat 27 May 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/tensorflow.html">tensorflow</a>,    <a class="category" href="/tag/deep-learning.html">deep learning</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/04/20/deepseek-expert-parallelism-load-balancer-eplb-code-reading/">DeepSeek Expert Parallelism Load Balancer (EPLB) Code Reading</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/23/deepseek-v3-learning-notes/">DeepSeek V3 learning notes</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2024/04/21/prediction-in-decoder-and-kv-cache/">Prediction in decoder and KV-Cache</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/agi.html">AGI</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/gpt.html">GPT</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/05/27/tensorflowjian-jie-06-logistic-regression-and-knn-analysis-for-mnist-data/';
    var disqus_url = '/pages/2017/05/27/tensorflowjian-jie-06-logistic-regression-and-knn-analysis-for-mnist-data/';
    var disqus_title = 'Tensorflow简介--06: Logistic regression and KNN analysis for MNIST data';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>