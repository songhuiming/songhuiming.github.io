<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>tensorflow简介--02 &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">tensorflow简介--02</h1>
    <p class="meta">
<time datetime="2017-02-26T21:30:00-06:00" pubdate>Sun 26 February 2017</time>    </p>
</header>

  <div class="entry-content"><p>上一篇文章，我们使用tensorflow(TF)来建立了一个单feature的线性回归模型。给定feature的值（house size），我们可以预测出房屋价格。</p>
<p>我们回顾一下上一章怎么做的：</p>
<ol>
<li>
<p>我们有房屋面积和房屋价格的数据（图中灰色原点）</p>
</li>
<li>
<p>用线性回归来建模（红色虚线）</p>
</li>
<li>
<p>通过最小化损失函数（图中蓝线的长度的平方和）来找出最优的模型的<code>W</code>和<code>b</code></p>
</li>
<li>
<p>给定房屋面积，应用建立的线性模型来预测房屋价格（图中蓝色虚线对应的点）</p>
</li>
</ol>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_01.png" /></p>
<p>在机器学习的论文中，我们经常会遇到‘training’这个词。我们首先来看看在TF中它的含义。</p>
<h2>线性回归模型</h2>
<div class="highlight"><pre>Linear Model (in TF notation): y = tf.matmul(x,W) + b
</pre></div>


<p>线性回归的目的是找出<code>W</code>和<code>b</code>,对给定的feature值(x),当我们把<code>W</code>,<code>b</code>和<code>x</code>带入模型，我们可以得到<strong>prediction</strong>(y)。</p>
<p>要得到<code>W</code>和<code>b</code>，我们需要用给定的数据(真实的feature值x和真实的结果y_， 注意y_的下划线)来训练(<strong>train</strong>)模型。</p>
<h2>Training Illustrated</h2>
<p>要找到最优的<code>W</code>和<code>b</code>，我们需要定义一个损失函数用来衡量对<strong>某一个给定的feature（x）值</strong>，预测值（y）和真实值(y_)之间的差异。为了简化，我们用最小平方差的和(MSE)作为损失函数。</p>
<div class="highlight"><pre>Cost function (in TF notation): tf.reduce_mean(tf.square(y_ - y))
</pre></div>


<p>通过最小化损失函数，我们可以得到好的<code>W</code>和<code>b</code>的值。</p>
<p>用来做训练的程序代码非常简单，我们用[A,B,C,D]来标记各个部分。下面将会用到。</p>
<div class="highlight"><pre># ... (snip) Variable/Constants declarations (snip) ...
# [A] TF.Graph
y = tf.matmul(x,W) + b
cost = tf.reduce_mean(tf.square(y_-y))
# [B] Train with fixed &#39;learn_rate&#39;
learn_rate = 0.1
train_step =
    tf.train.GradientDescentOptimizer(learn_rate).minimize(cost)
for i in range(steps):
  # [C] Prepare datapoints
  # ... (snip) Code to prepare datapoint as xs, and ys (snip) ...
  # [D] Feed Data at each step/epoch into &#39;train_step&#39;
  feed = { x: xs, y_: ys }
  sess.run(train_step, feed_dict=feed)
</pre></div>


<p>A当中的线性模型和损失函数可以被表示成下面的TF流程图</p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_02.png" /></p>
<p>下一步，我们要选择一个C当中的数据(x, y_)，输入到D当中来得到预测值(y)和损失函数。TF流程图如下：</p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_03.png" /></p>
<p>要得到更好的<code>W</code>和<code>b</code>，我们在B中通过<em>tf.train.GradientDescentOptimizer</em>做梯度下降来得到损失函数。更简易的说法是：给定当前的损失值，根据图中别的变量（<code>W</code>和<code>b</code>）所得到的损失函数的值，优化器会对<code>W</code>和<code>b</code>做一个小变化（增加/减少），从而使得对那一个数据点的预测值更好。</p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_04.png" /></p>
<p>最后一步就是更新调整过的<code>W</code>和<code>b</code>的值。注意这一个周期(cycle)在机器学习的文档中也叫<strong>epoch</strong>。</p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_05.png" /></p>
<p>在下一个<strong>epoch</strong>，重复以上步骤，但是用一个<strong>不同的数据点(x, y_)</strong></p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_06.png" /></p>
<p>使用不同的数据点来建模，所得到的<code>W</code>和<code>b</code>就可以用来预测任意的feature。注意</p>
<ul>
<li>
<p>大部分时候，更多的数据点得到的模型会更好</p>
</li>
<li>
<p>如果你的epochs比你的数据点还多，你可以重复利用数据点，这是没有问题的。梯度下降的优化器总是使用数据点和损失函数(从相应的epoch中的<code>W</code>和<code>b</code>和数据点计算出来)来调整<code>W</code>和<code>b</code>。优化器可能已经用过这个数据点，但是因为损失函数不同，所以它仍然会得到新的信息，也会得到新的调整过的<code>W</code>和<code>b</code>。</p>
</li>
</ul>
<p>你可以训练你的模型给定次数的epoch，或者直到损失函数低于某个给定的值。</p>
<h2>Training variation</h2>
<h3>stochastic，mini-batch, batch</h3>
<p>在上面的模型训练中，我们每次输入一个数据点。这叫做随机梯度下降(<em>stochastic gradient descent</em>)。我们也可以在每个epoch中输入一批数据点，这叫做<em>mini-batch gradient descent</em>。甚至我们可以在每个epoch中输入所有的数据点，这叫做<em>batch gradient descent</em>。请看下面的比较图示。注意这三个图有两个不同</p>
<ul>
<li>每个epoch中输入到TF.Graph的数据点(右上角的数据点)不同</li>
<li>用来调整<code>W</code>和<code>b</code>的梯度下降优化器所用到的数据点（右下角）也不相同</li>
</ul>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_07.png" />
<img alt="png" src="/figures/20170225_tensorflow_intro_p2_08.png" />
<img alt="png" src="/figures/20170225_tensorflow_intro_p2_09.png" /></p>
<p>在每个epoch中使用到的数据点有两层含义。如果每个epoch使用更多的数据点：</p>
<ul>
<li>计算损失函数和梯度下降的计算资源(加减乘)会减少</li>
<li>模型学习的速度会提高</li>
</ul>
<p>使用stochastic, mini-batch, batch gradient descent 的优缺点见下图</p>
<p><img alt="png" src="/figures/20170225_tensorflow_intro_p2_10.png" /></p>
<p>要在stochastic, mini-batch, batch gradient descent这三个办法中切换，我们只需要在下面的[C]当中设置不同的batch大小，然后根据batch大小把对应的batch的数据点输入到模型训练[D]中。</p>
<div class="highlight"><pre># * all_xs: All the feature values
# * all_ys: All the outcome values
# datapoint_size: Number of points/entries in all_xs/all_ys
# batch_size: Configure this to:
#             1: stochastic mode
#             integer &lt; datapoint_size: mini-batch mode
#             datapoint_size: batch mode
# i: Current epoch number
if datapoint_size == batch_size:
  # Batch mode so select all points starting from index 0
  batch_start_idx = 0
elif datapoint_size &lt; batch_size:
  # Not possible
  raise ValueError(“datapoint_size: %d, must be greater than         
                    batch_size: %d” % (datapoint_size, batch_size))
else:
  # stochastic/mini-batch mode: Select datapoints in batches
  #                             from all possible datapoints
  batch_start_idx = (i * batch_size) % (datapoint_size — batch_size)
  batch_end_idx = batch_start_idx + batch_size
  batch_xs = all_xs[batch_start_idx:batch_end_idx]
  batch_ys = all_ys[batch_start_idx:batch_end_idx]


# Get batched datapoints into xs, ys, which is fed into
# &#39;train_step&#39;
xs = np.array(batch_xs)
ys = np.array(batch_ys)
</pre></div>


<h3>Learn Rate的变化</h3>
<p>学习速度(Learn Rate)指的是梯度下降来调整<code>W</code>和<code>b</code>的这时候，每次增加或者减小的大小。当learn rate比较小的时候，前进速度会很慢，但是会渐渐收敛到最小损失函数值。当learn rate比较大的时候，达到最小损失函数的速度会比较快，但是有可能会走过头，导致找不到最小值。</p>
<p>要克服这一点，肯多ML的实际操作是开始的时候用比较大的learn rate（假设初始的损失函数离最小值比较远），然后每个epoch渐渐减小learn rate以防止走过头。</p>
<p>TF提供了两个办法。这个<a href="http://stackoverflow.com/questions/33919948/how-to-set-adaptive-learning-rate-for-gradientdescentoptimizer">StackOverflow thread</a>讲的非常棒。总结如下:</p>
<h3>使用变化的learn rate的梯度下降优化器</h3>
<p>TF提供了不同的支持learn rate变化的梯度下降优化器，比如<a href="https://www.tensorflow.org/api_guides/python/train#AdagradOptimizer">tf.train.AdagradientOptimizer</a>和<a href="https://www.tensorflow.org/api_guides/python/train#AdamOptimizer">tf.train.AdamOptimizer</a>.</p>
<h3>使用tf.placeholder建Learn Rate</h3>
<p>如前所述，我们在<em>tf.train.GradientDescentOptimizer</em>当中用<em>tf.placeholder</em>来申明<em>Learn Rate</em>的时候，我们可以在每个epoch中输入不同的值。这类似于在每个epoch中我们用<em>tf.placeholders</em>来输入不同的数据点到x, y_。</p>
<p>这样做需要两个小的改动:</p>
<div class="highlight"><pre># Modify [B] to make &#39;learn_rate&#39; a &#39;tf.placeholder&#39;
# and supply it to the &#39;learning_rate&#39; parameter name of
# tf.train.GradientDescentOptimizer
learn_rate = tf.placeholder(tf.float32, shape=[])
train_step = tf.train.GradientDescentOptimizer(
    learning_rate=learn_rate).minimize(cost)
# Modify [D] to include feed a &#39;learn_rate&#39; value,
# which is the &#39;initial_learn_rate&#39; divided by
# &#39;i&#39; (current epoch number)
# NOTE: Oversimplified. For example only.
feed = { x: xs, y_: ys, learn_rate: initial_learn_rate/i }
sess.run(train_step, feed_dict=feed)
</pre></div>


<h2>总结</h2>
<p>我们演示了机器学习的训练是什么意思，以及在Tensorflow中怎么定义模型和损失函数。然后通过输入数据点到梯度下降优化器，进行循环来得到<code>W</code>和<code>b</code>的值。我们还讨论了训练过程中两个常见的变化：每个epoch中使用不同的数据集的大小，以及使用不同的learn rate。</p>
<h3>下一章</h3>
<ul>
<li>设置Tensor Board来可视化TF的执行，这样可以检测到我们模型，损失函数和梯度下降的问题</li>
<li>展示一个多特征(features)的线性回归模型</li>
</ul>
<h3>Reference</h3>
<ol>
<li><a href="https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f#.9cxp5xvf0">Gentlest Introduction to Tensorflow #2</a></li>
</ol></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-02-26T21:30:00-06:00" pubdate>Sun 26 February 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/tensorflow.html">tensorflow</a>,    <a class="category" href="/tag/deep-learning.html">deep learning</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2017/05/20/online-ifrs9cecl-lifetime-credit-loss-calculation-engine/">Online IFRS9/CECL lifetime credit loss calculation engine</a>
      </li>
      <li class="post">
          <a href="/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/">Gradient Descent in solving linear regression and logistic regression</a>
      </li>
      <li class="post">
          <a href="/pages/2017/05/06/tensorflowjian-jie-05-multivariate-regression-with-stochastic-gradient-descent/">Tensorflow简介--05: Multivariate Regression with Stochastic Gradient Descent</a>
      </li>
      <li class="post">
          <a href="/pages/2017/05/04/test-math-formula-in-github-pages/">test math formula in github pages</a>
      </li>
      <li class="post">
          <a href="/pages/2017/04/21/numpy-introduction-02/">Numpy Introduction 02</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2017  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/02/26/tensorflowjian-jie-02/';
    var disqus_url = '/pages/2017/02/26/tensorflowjian-jie-02/';
    var disqus_title = 'tensorflow简介--02';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>