<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Data Engineering and Modeling 01: predict defaults with imbalanced data &mdash; pydata: Huiming's learning notes</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><header>
  <h1><a href="/">pydata: Huiming's learning notes</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</header>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  // 允许 $...$ 和 \( ... \) 作为行内公式
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  // 允许 $$...$$ 和 \[ ... \] 作为块级公式
    processEscapes: true,  // 允许在公式中使用转义符，如 \$ 表示美元符号
    processEnvironments: true  // 允许解析 \begin{equation} ... \end{equation} 等数学环境
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],  // 跳过某些 HTML 标签，防止错误解析
    renderActions: {
      addMenu: []  // 移除右键菜单
    }
  }
};

window.addEventListener('load', () => {
  document.querySelectorAll("mjx-container").forEach(x => {
    x.parentElement.classList.add('has-jax');  // 使用 classList.add() 避免字符串拼接错误
  });
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></header>
  <nav role="navigation">

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Data Engineering and Modeling 01: predict defaults with imbalanced data</h1>
    <p class="meta">
<time datetime="2017-09-23T18:08:00-05:00" pubdate>Sat 23 September 2017</time>    </p>
</header>

  <div class="entry-content"><h2>0. Description</h2>
<p>This data contains the recordings of a series of borrowers(based on id) from 2015-01-01 to 2015-11-02. The target variable is a categorical variable with value 0 or 1. target=1 means the borrower is default, otherwise it means the borrower is active. Totally there are 1168 unique borrower id.</p>
<p>Each borrower will have one recording or no recording everyday. The minimum number of recording is 3 and the maximum number of recording is 304.</p>
<p>For each recording, there are 9 independent variables named as x1 to x9.</p>
<p>The job is to predict the target variable based on the independent variables.</p>
<h2>1. Data View</h2>
<p>The data looks like</p>
<table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>date</th>      <th>id</th>      <th>target</th>      <th>x1</th>      <th>x2</th>      <th>x3</th>      <th>x4</th>      <th>x5</th>      <th>x6</th>      <th>x7</th>      <th>x8</th>      <th>x9</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>2015-01-01</td>      <td>S1F01085</td>      <td>0</td>      <td>215630672</td>      <td>56</td>      <td>0</td>      <td>52</td>      <td>6</td>      <td>407438</td>      <td>0</td>      <td>0</td>      <td>7</td>    </tr>    <tr>      <th>1</th>      <td>2015-01-01</td>      <td>S1F0166B</td>      <td>0</td>      <td>61370680</td>      <td>0</td>      <td>3</td>      <td>0</td>      <td>6</td>      <td>403174</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>2</th>      <td>2015-01-01</td>      <td>S1F01E6Y</td>      <td>0</td>      <td>173295968</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>12</td>      <td>237394</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>3</th>      <td>2015-01-01</td>      <td>S1F01JE0</td>      <td>0</td>      <td>79694024</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>6</td>      <td>410186</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>4</th>      <td>2015-01-01</td>      <td>S1F01R2B</td>      <td>0</td>      <td>135970480</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>15</td>      <td>313173</td>      <td>0</td>      <td>0</td>      <td>3</td>    </tr>    <tr>      <th>5</th>      <td>2015-11-02</td>      <td>Z1F0MA1S</td>      <td>0</td>      <td>18310224</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>10</td>      <td>353705</td>      <td>8</td>      <td>8</td>      <td>0</td>    </tr>    <tr>      <th>6</th>      <td>2015-11-02</td>      <td>Z1F0Q8RT</td>      <td>0</td>      <td>172556680</td>      <td>96</td>      <td>107</td>      <td>4</td>      <td>11</td>      <td>332792</td>      <td>0</td>      <td>0</td>      <td>13</td>    </tr>    <tr>      <th>7</th>      <td>2015-11-02</td>      <td>Z1F0QK05</td>      <td>0</td>      <td>19029120</td>      <td>4832</td>      <td>0</td>      <td>0</td>      <td>11</td>      <td>350410</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>8</th>      <td>2015-11-02</td>      <td>Z1F0QL3N</td>      <td>0</td>      <td>226953408</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>12</td>      <td>358980</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>    <tr>      <th>9</th>      <td>2015-11-02</td>      <td>Z1F0QLC1</td>      <td>0</td>      <td>17572840</td>      <td>0</td>      <td>0</td>      <td>0</td>      <td>10</td>      <td>351431</td>      <td>0</td>      <td>0</td>      <td>0</td>    </tr>  </tbody></table>

<ul>
<li>Target Variable: target is the categorical variable with values 0 and 1</li>
<li>Independent Variables: we can use all the other variables as the independent variables, including x1 to x9, even date and id if with proper transformation.</li>
</ul>
<p><strong>Some Observations</strong>:</p>
<ul>
<li>
<p>From the data we can see <code>x1</code> and <code>x6</code> are like numeric variable while the others are like categorical variable. We will do some analysis later.</p>
</li>
<li>
<p><code>x2</code>, <code>x3</code>, <code>x7</code>, <code>x8</code>, <code>x9</code> has lots of zeros </p>
</li>
<li>
<p>The first 3 characters of id might be useful to distinguish the target variable</p>
</li>
<li>
<p>The variable date might be useful: we can get month information which might be useful, or we can get the time duration from first recording time to positive target time</p>
</li>
</ul>
<p><strong>Some Extras</strong>:</p>
<ul>
<li>Totally there are 124494 rows in the data. But there are only 1168 unique ids.</li>
<li>Overall there are only 106 target=1. That is, the rate is about 0.08%.</li>
<li>Each id only has at most one time target=1.</li>
</ul>
<h3>1.1. Data summary and target info</h3>
<p>There are 124494 rows of data in all but there is only 106 target=1. So the data is very imbalanced.</p>
<pre><code class="language-python">import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import seaborn as sns

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix
from sklearn.cross_validation import KFold
from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import log_loss

import plotly.offline as py
py.init_notebook_mode(connected=True)
import plotly.graph_objs as go
import plotly.tools as tls

%matplotlib inline
#get_ipython().magic('matplotlib inline')
plt.rcParams['figure.figsize'] = (20, 16)

mypath = r'/home/shm/projects/blog_study_draft/data//'
indata = pd.read_csv(mypath + r'defaults.csv')
</code></pre>
<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>

<pre><code class="language-python">print(indata.shape)
print(indata.target.sum())
</code></pre>
<pre><code>(124494, 12)
106
</code></pre>
<h3>1.2. Borrowers defaulted</h3>
<p>Since there are many target=0. We will check how many borrowers defaulted and how many not.</p>
<p>It shows there are 106 borrowers defaulted. The rest 1062 borrowers did not have any default during this observation time period.</p>
<pre><code class="language-python">defaultid = indata[indata.target == 1].id.unique()

print(indata[indata.id.isin(defaultid)].id.unique().shape)
print(indata[~indata.id.isin(defaultid)].id.unique().shape)
print(indata.id.unique().shape)
</code></pre>
<pre><code>(106,)
(1062,)
(1168,)
</code></pre>
<p>For furture use, I will pick out the defaulted borrowers to a standalone data set.</p>
<pre><code class="language-python">defaults = indata[indata.id.isin(defaultid)]
print(defaults.shape)
</code></pre>
<pre><code>(10713, 12)
</code></pre>
<h2>2. Data Engineering</h2>
<h3>2.1. Unique values for each variable</h3>
<p>The data does not have a dictionary to indicate clearly what does the x mean. I will determine them based on their values information. </p>
<p>x1 and x6 have lots of unique values, they are more like continuous variables. The other variables x are more like categorical variables.</p>
<pre><code class="language-python"># pd.concat([indata[:5], indata[-5:]], axis = 0).reset_index(drop=True).to_html()
for i in indata.columns:
    print(&quot;For variable &quot; + str(i) + &quot;, there are &quot; + str(len(indata[i].unique())) + &quot; unique values&quot;)
</code></pre>
<pre><code>For variable date, there are 304 unique values
For variable id, there are 1168 unique values
For variable target, there are 2 unique values
For variable x1, there are 123878 unique values
For variable x2, there are 558 unique values
For variable x3, there are 47 unique values
For variable x4, there are 115 unique values
For variable x5, there are 60 unique values
For variable x6, there are 44838 unique values
For variable x7, there are 28 unique values
For variable x8, there are 28 unique values
For variable x9, there are 65 unique values
</code></pre>
<h3>2.2. process variable date and id</h3>
<p>First I will grab the month informtion to have a look if the defaults have any relation with month. Also I will do the same thing for the id.</p>
<p>There are only 3 different values for the first 2 strings in id. I guess it is like geo info. I will check if will help to explain y.</p>
<p>From the plot it shows month 5 and month 7 are higher than the ohter months. There is also period default trend by month. </p>
<p>By id it shows w1 has higher default rate then z1. And z1 is higher than s1. Since extracted month and id0 are characters. I will replace them by the log(odds)</p>
<pre><code class="language-python">indata['month'] = indata.date.map(lambda x: x.split('-')[1])
indata['id0'] = indata.id.map(lambda x: x[:2])
defaults['month'] = defaults.date.map(lambda x: x.split('-')[1])
defaults['id0'] = defaults.id.map(lambda x: x[:2])

def attr_f1(df, x, name):
    print(&quot;-&quot;*20 + &quot; this is for x &quot; + x + &quot;-&quot;*20)
    a = df.groupby(x).target.mean().reset_index() 
    a.columns = [x, name]
    df = pd.merge(df, a, how = &quot;left&quot;, on = x)
    #df.loc[:, name] = np.log((df.loc[:, name] + 1e-8)/(1 - df.loc[:, name] + 1e-8))
    return df

indata = attr_f1(indata, &quot;x2&quot;, &quot;a2&quot;)
indata = attr_f1(indata, &quot;x3&quot;, &quot;a3&quot;)
indata = attr_f1(indata, &quot;x4&quot;, &quot;a4&quot;)
indata = attr_f1(indata, &quot;x5&quot;, &quot;a5&quot;)
indata = attr_f1(indata, &quot;x7&quot;, &quot;a7&quot;)
indata = attr_f1(indata, &quot;x8&quot;, &quot;a8&quot;)
indata = attr_f1(indata, &quot;x9&quot;, &quot;a9&quot;)
indata = attr_f1(indata, &quot;id0&quot;, &quot;id1&quot;)
indata = attr_f1(indata, &quot;month&quot;, &quot;month0&quot;)


defaults = attr_f1(defaults, &quot;x2&quot;, &quot;a2&quot;)
defaults = attr_f1(defaults, &quot;x3&quot;, &quot;a3&quot;)
defaults = attr_f1(defaults, &quot;x4&quot;, &quot;a4&quot;)
defaults = attr_f1(defaults, &quot;x5&quot;, &quot;a5&quot;)
defaults = attr_f1(defaults, &quot;x7&quot;, &quot;a7&quot;)
defaults = attr_f1(defaults, &quot;x8&quot;, &quot;a8&quot;)
defaults = attr_f1(defaults, &quot;x9&quot;, &quot;a9&quot;)
defaults = attr_f1(defaults, &quot;id0&quot;, &quot;id1&quot;)
defaults = attr_f1(defaults, &quot;month&quot;, &quot;month0&quot;)
</code></pre>
<pre><code class="language-python">indata.groupby('month').target.mean().plot()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_01.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">indata.groupby('id0').target.mean().plot()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_02.png" title="Logo Title Text 1"></p>
<p>However, when I only look at the defaults data, I will find that month 5 and month 7 are not the highest default month. The trend is not exactly the same as the trend from the full data set.</p>
<pre><code class="language-python">defaults.groupby('month').target.mean().plot()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_03.png" title="Logo Title Text 1"></p>
<p>The transformed variable from id still works:</p>
<pre><code class="language-python">defaults.groupby('id0').target.mean().plot()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_04.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">sns.factorplot(x = &quot;x3&quot;, hue = &quot;target&quot;, col = &quot;id0&quot;, 
               data = defaults[defaults.x3 != 0], kind = &quot;count&quot;, size = 8, aspect=1)
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_05.png" title="Logo Title Text 1"></p>
<h3>2.3. x1 and x6</h3>
<p>Let us look at the tow continuous variable x1 and x6. I will scatter plot the default rate based on the grids from x1 and x6.</p>
<p>target=0 will be colored as blue and target=1 will be colored as red. It shows most of the x1 are between 200000 to 500000. </p>
<pre><code class="language-python">fig, ax = plt.subplots()
colors = {0:'#9bc3f6', 1:'#ff0000'}
ax.scatter(indata.x1, indata.x6, c = indata.target.apply(lambda x: colors[x]))
plt.show() 
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_06.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">fig, ax = plt.subplots()
colors = {0:'#9bc3f6', 1:'#ff0000'}
ax.scatter(defaults.x1, defaults.x6, c = defaults.target.apply(lambda x: colors[x]))
plt.show()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_07.png" title="Logo Title Text 1"></p>
<h3>2.4. Correlation</h3>
<p>It will be helpful to check the correlation between x and y. Also it is helpful to check the correlation between all the x so that we can know if there is any multicollinearity or not.</p>
<p>We can see there is high correlation between x7 and x8. Alao x9 and 3 has a correlation around 0.53.</p>
<pre><code class="language-python">cols = [&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;id1&quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(indata[[&quot;target&quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_08.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">cols = [&quot;x1&quot;, &quot;x2&quot;, &quot;x3&quot;, &quot;x4&quot;, &quot;x5&quot;, &quot;x6&quot;, &quot;x7&quot;, &quot;x8&quot;, &quot;x9&quot;, &quot;id1&quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(defaults[[&quot;target&quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_09.png" title="Logo Title Text 1"></p>
<p>Also, we can check the correlation between y and the transformed values a.</p>
<pre><code class="language-python">cols = [&quot;x1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;, &quot;a5&quot;, &quot;x6&quot;, &quot;a7&quot;, &quot;a8&quot;, &quot;a9&quot;, &quot;id1&quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(indata[[&quot;target&quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_10.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">cols = [&quot;x1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;, &quot;a5&quot;, &quot;x6&quot;, &quot;a7&quot;, &quot;a8&quot;, &quot;a9&quot;, &quot;id1&quot;, 'month0']
colormap = plt.cm.viridis
plt.figure(figsize = (16, 12))
plt.title('Pearson Correlation of Features', y = 1.05, size  = 15)
sns.heatmap(defaults[[&quot;target&quot;] + cols].astype(float).corr(), 
            linewidth = 0.1, vmax = 1.0, square = True, cmap = colormap, linecolor = 'white', annot = True)
plt.show()
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_11.png" title="Logo Title Text 1"></p>
<h2>3. Model</h2>
<h3>3.1. imbalanced Data</h3>
<p>Since there are only 106 defaults comparing to 1168 borrowers and 124494 observations, the proportion of target=1 is  very low. That is, our data is imbalanced data.</p>
<p>If we used imbalanced data directly to build the model, we might be in trouble because the model will be ignoring the target=1 data if we use accuracy to measure the model performance.</p>
<p>There are some methods to deal with the imbalanced data:</p>
<ol>
<li>
<p>oversampling: we repeat the low proportion data to make the proportion of target=1 and target=0 to be close in the oversampled data.</p>
</li>
<li>
<p>downsampling: unlike oversampling to increase the low proportion data, downsampling will try to sample from high proportion(target=1 here) to make the data balanced</p>
</li>
<li>
<p>adjust the low proportion data weight in the algorithm</p>
</li>
<li>
<p>adjust the decision threshold of the output probability to classify</p>
</li>
<li>
<p>adjust the lost function if we want to give more weights on the low proportion data</p>
</li>
</ol>
<h3>3.2. Oversampling</h3>
<p>Here we will do oversampling: </p>
<ol>
<li>
<p>we pick all the 10713 observations of 106 borrowers who has been defaulted. There are 106 target=1 with all the rest 10607 observations having target=0</p>
</li>
<li>
<p>Second we will repeat the 106 positive sample 10607/106 times. After this, in the oversampled data, the ratio of 1 v.s. 0 is about half half.</p>
</li>
</ol>
<p>After the new oversampled data is created, I will split them to training part and validation part.</p>
<pre><code class="language-python">from sklearn.model_selection import train_test_split, StratifiedShuffleSplit
from sklearn.metrics import log_loss, auc, roc_auc_score

defaults_1 = defaults.query('target == 1')
defaults_0 = defaults.query('target == 0')
defaults_1_rep = pd.concat([defaults_1]*int(np.ceil(len(defaults_0)/len(defaults_1))), ignore_index = True, axis = 0)
defaults_new = pd.concat([defaults_0, defaults_1_rep], ignore_index = True, axis = 0)

cols = [&quot;x1&quot;, &quot;a2&quot;, &quot;a3&quot;, &quot;a4&quot;, &quot;a5&quot;, &quot;x6&quot;, &quot;a7&quot;, &quot;a9&quot;, &quot;id1&quot;, 'month0']
ftrain = defaults_new[cols]
fx_train = defaults_new[cols].values
fy_train = defaults_new[[&quot;target&quot;]].values.ravel()
print(np.sum(fy_train))

xtrain, xtest, ytrain, ytest = train_test_split(fx_train, fy_train, test_size = 0.4, random_state = 999)
print(ytest.sum())
</code></pre>
<pre><code>10600
4168
</code></pre>
<h3>3.3. First Model: RandomForestClassifier with growing number of estimatiors</h3>
<p>RandomForest is the decision tree based algorithm which builds several decision trees and then combine their output to improve the ability of the model. The method of combining trees is known as ensemble method. <font color="red">Ensembing means combination of weak learners to produce a stronger learner.</font> </p>
<h4>3.3.1. Growing RandomForest</h4>
<p>RandomForstClassifier is a combination of many decision trees. In the first example, I will show how the <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">AUC</a> chages based on different number of decision trees. <code>n_estimators</code> is the hyperparameter indicating how many decision trees will be used. We will start from 100 and increase 10 in each loop. Then we will draw the graph of AUC v.s. n_estimators. </p>
<p>Another benifits is we can get the variable importances from the RandomForest output. The variable importances are measured by the total decrease of node impurities from splitting on the variable.</p>
<pre><code class="language-python">auc = []
growing_rf = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth = 10, 
                                    min_samples_leaf = 2, max_features = 'auto', 
                                    verbose = 0, n_jobs = -1, warm_start = True, random_state = 168)

for i in range(50):
    growing_rf.fit(xtrain, ytrain)
    growing_rf.n_estimators += 10
    auc.append(roc_auc_score(ytest, growing_rf.predict(xtest)))

_ = plt.plot(auc, '-r')
plt.title(&quot;Growing RandomForest AUC on validation data&quot;)
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_12.png" title="Logo Title Text 1"></p>
<p>Feature importance plot shows that a2 is the most important feature, and a4 is the next most important.</p>
<pre><code class="language-python">rf_importance = growing_rf.feature_importances_

feature_df = pd.DataFrame({'features': cols, 'RandomForest feature importances': rf_importance})

trace = go.Scatter(y = feature_df['RandomForest feature importances'].values,
                  x = feature_df['features'].values,
                  mode = &quot;markers&quot;,
                  marker = dict(sizemode = &quot;diameter&quot;,
                               sizeref = 1,
                               size = 25,
                               color = feature_df['RandomForest feature importances'].values,
                               colorscale = &quot;Portland&quot;,
                               showscale = True),
                  text = feature_df['features'].values)
data = [trace]

layout = go.Layout(autosize = True,
                  title = &quot;Growing RandomForest Feature Importances&quot;,
                  hovermode = &quot;closest&quot;,
                  yaxis = dict(title = &quot;Feature Importances&quot;,
                              ticklen = 5, 
                              gridwidth = 2),
                  showlegend = False)

fig = go.Figure(data = data, layout = layout)

py.iplot(fig, filename = 'scatter2010')
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_13.png" title="Logo Title Text 1"></p>
<p>Finally we will apply the built RandomForestClassifier on the original data set. We will get 31 defaults predicted correctly. There are 75 defaults are wrongly predicted as non-defaults. There are also 10 non-defaults are predicted as defaults.</p>
<p>This result also show accuracy is not a good measure here as mentioned above because there are 124378 non-defaults are predicted correctly. So the overall accuracy is very high.</p>
<pre><code class="language-python">print(confusion_matrix(indata.target, growing_rf.predict(indata[cols])))
</code></pre>
<pre><code>[[124384      4]
 [    86     20]]
</code></pre>
<h3>3.4. GridSearch Hyperparameters</h3>
<p>RandomForestClassifier have several hyperparameters to choose.  <code>n_estimator</code> will enable you to choose how many decision trees will be used. <code>max_depth</code> will decide how deep the decision tree will be. <code>min_samples_leaf</code> is the minimum number of samples required to be at a leaf node. </p>
<p>We will test the combination of these hyperparameters and let the data decide which hyperparameter is the best. To do that, we first need to define the scoring function which will be the rule to select hyperparameters. From above the main issue here is we are likely to make type 1 and type 2 errors. So I will define a function to minimize the type 1 and type 2 predictions.  </p>
<pre><code class="language-python">from sklearn.grid_search import GridSearchCV
from sklearn.metrics import make_scorer

def myscoring(ground_truth, predictions):
    cmatrix = confusion_matrix(ground_truth, predictions)
    fp = cmatrix[0, 1]
    fn = cmatrix[1, 0]
    return  fn + fp

my_score = make_scorer(myscoring, greater_is_better = False)
</code></pre>
<pre><code class="language-python">estimator = RandomForestClassifier(random_state=0)

rf_tuned_parameters = {&quot;max_depth&quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                       'min_samples_leaf': [2, 4, 10, 20]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = my_score)
cv_grid.fit(xtrain, ytrain)

best_parameters = cv_grid.best_estimator_.get_params()

for param_name in sorted(rf_tuned_parameters.keys()):
    print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))
pred1 = cv_grid.predict(xtest)
print(&quot;confustion matrix on validation data: &quot; + str(confusion_matrix(ytest, pred1)))
</code></pre>
<pre><code>    max_depth: 20
    min_samples_leaf: 2
    n_estimators: 100
confustion matrix on validation data: [[4259   56]
 [   0 4168]]
</code></pre>
<pre><code class="language-python">pred_full = cv_grid.predict(indata[cols])
print(confusion_matrix(indata.target, pred_full))
</code></pre>
<pre><code>[[124384      4]
 [    88     18]]
</code></pre>
<h3>3.5. ExtraTreeClassifier, AdaboostClassifier, GradientBoostingClassifier, LinearSVC, Logistic Regression</h3>
<p>Next I will build some other classification learners. Then these models will be combined together as input for another model. It is like a new ensembling model is created.</p>
<p>Each classifier has its own hyperparameters. It is better to grad search for finetuning hyperparameters and find the best hyperparameters. But that will take longer time to do. To save time I assigned the hyperparameter directly.</p>
<p>The modeling data(oversampled data) will be split into 5 folders. Each time 4 folders data will be used to train a model and the left 1 folder will be used for test data. </p>
<h4>3.5.1. Build Model</h4>
<pre><code class="language-python">ntrain = fx_train.shape[0]
nfolds = 5
seed = 888
kf = KFold(ntrain, n_folds=nfolds, random_state=seed)

class SklearnClassifier(object):
    def __init__(self, clf, seed = 999, params = None):
        params['random_state'] = seed
        self.clf = clf(**params)

    def fit(self, x_train, y_train):
        self.clf.fit(x_train, y_train)

    def predict(self, x):
        return self.clf.predict(x)

    def feature_importances(self, x, y):
        return self.clf.fit(x, y).feature_importances_

def get_oof(clf, x_train, y_train):
    oof_train = np.zeros((ntrain, ))
    for i, (train_index, test_index) in enumerate(kf):
        x_tr = x_train[train_index]
        y_tr = y_train[train_index]
        x_te = x_train[test_index]
        y_te = x_train[test_index]
        clf.fit(x_tr, y_tr)
        oof_train[test_index] = clf.predict(x_te)
    full_predict = clf.predict(indata[cols]).reshape(-1, 1)
    return (oof_train.reshape(-1, 1), full_predict)

# random forest parameters
rf_params = {'n_jobs': -1, 
            'n_estimators': 50,
            'warm_start': True,
            'max_depth': 50,
            'min_samples_leaf':2,
            'max_features': 'sqrt',
            'verbose':0}

# extra tree parameters
et_params = {'n_jobs': -1,
            'n_estimators': 100,
            'max_depth': 50,
            'min_samples_leaf': 2,
            'verbose':0}

# adaboost parameters
ada_params = {'n_estimators': 100,
             'learning_rate': 0.75}

# gradient boosting parameters
gb_params = {'n_estimators': 100,
            'max_features': 0.5,
            'max_depth': 50,
            'min_samples_leaf':2,
            'verbose':0}

# LinearSVC parameters
svc_params = {'C': 0.25}
</code></pre>
<pre><code class="language-python">rf = SklearnClassifier(clf = RandomForestClassifier, seed = seed, params = rf_params)
et = SklearnClassifier(clf = ExtraTreesClassifier, seed = seed, params = et_params)
ada = SklearnClassifier(clf = AdaBoostClassifier, seed = seed, params = ada_params)
gb = SklearnClassifier(clf = GradientBoostingClassifier, seed = seed, params = gb_params)
svc = SklearnClassifier(clf = LinearSVC, seed = seed, params = svc_params)
</code></pre>
<pre><code class="language-python">from datetime import datetime
startt = datetime.now()

(rf_oof_train, rf_oof_full) = get_oof(rf, fx_train, fy_train)  # 0:00:02.194219
(et_oof_train, et_oof_full) = get_oof(et, fx_train, fy_train)  # 0:00:05.207521
(ada_oof_train,  ada_oof_full) = get_oof(ada, fx_train, fy_train) # 0:00:27.385738
(gb_oof_train, gb_oof_full) = get_oof(gb, fx_train, fy_train)  # 0:00:16.449645
# SVM usually takes too long time to run, use LinearSVC
#https://stackoverflow.com/questions/40077432/scikit-learn-svm-svc-is-extremely-slow
(svc_oof_train, svc_oof_full) = get_oof(svc, fx_train, fy_train)

endt = datetime.now()
print(endt - startt)
</code></pre>
<pre><code>0:00:30.987350
</code></pre>
<h4>3.5.2. Feature Importances</h4>
<p>Next we will get the feature importances and plot them. SVM does not have feature importances because it depends on the boundary data points(supporting vectors) on the splitting surface.</p>
<pre><code class="language-python">rf_features = rf.feature_importances(fx_train, fy_train)
et_features = et.feature_importances(fx_train, fy_train)
ada_features = ada.feature_importances(fx_train, fy_train)
gb_features = gb.feature_importances(fx_train, fy_train)

feature_dataframe = pd.DataFrame({'features':cols, 
                                 'Random Forest feature importances': rf_features,
                                 'AdaBoost feature importances': ada_features,
                                 'Gradient Boost feature importances': gb_features,
                                 'Extra trees feature importances': et_features})
</code></pre>
<pre><code class="language-python">def scatter_plot(picked_feature_name = 'Random Forest feature importances'):
    trace = go.Scatter(y = feature_dataframe[picked_feature_name].values,
                      x = feature_dataframe['features'].values,
                      mode = &quot;markers&quot;,
                      marker = dict(sizemode = &quot;diameter&quot;,
                                   sizeref = 1,
                                   size = 25,
                                   color = feature_dataframe[picked_feature_name].values,
                                   colorscale = &quot;Portland&quot;,
                                   showscale = True),
                      text = feature_dataframe['features'].values)
    data = [trace]

    layout = go.Layout(autosize = True,
                      title = picked_feature_name,
                      hovermode = &quot;closest&quot;,
                      yaxis = dict(title = &quot;Feature Importances&quot;,
                                  ticklen = 5, 
                                  gridwidth = 2),
                      showlegend = False)

    fig = go.Figure(data = data, layout = layout)

    py.iplot(fig, filename = 'scatter2010')


</code></pre>
<h5>3.5.2.1. RandomForestClassifier Feature Importances</h5>
<pre><code class="language-python">scatter_plot(picked_feature_name = 'Random Forest feature importances')
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_14.png" title="Logo Title Text 1"></p>
<h5>3.5.2.2. AdaBoost Feature Importances</h5>
<pre><code class="language-python">scatter_plot(picked_feature_name = 'AdaBoost feature importances')
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_15.png" title="Logo Title Text 1"></p>
<h5>3.5.2.3. Gradient Boost Feature Importances</h5>
<pre><code class="language-python">scatter_plot(picked_feature_name = 'Gradient Boost feature importances')
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_16.png" title="Logo Title Text 1"></p>
<h5>3.5.2.4. Extra trees Feature Importances</h5>
<pre><code class="language-python">scatter_plot(picked_feature_name = 'Extra trees feature importances')
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_17.png" title="Logo Title Text 1"></p>
<h3>3.6. XGBoost</h3>
<p>Finally we will build another model with inputs from the output of the previous models. (This is a little like multi-layers neural network which use previous layers output as current layer input).</p>
<p>This time we will XGBoost to build the model. XGBoost is known for boosted tree learners. It optimizes large scale boosted tree. </p>
<pre><code class="language-python">base_pred = pd.DataFrame({'randomforest': rf_oof_train.ravel(),
                                'extratree': et_oof_train.ravel(),
                                'adaboost': ada_oof_train.ravel(),
                                'gradientboost': gb_oof_train.ravel(), 
                                'linearsvc': svc_oof_train.ravel()})
</code></pre>
<p>The correlation of the output from the previous model output is given below. We can see the correlation is not very high so we can safely use these output as input for XGBoost model.</p>
<pre><code class="language-python">data = [
    go.Heatmap(z = base_pred.astype(float).corr().values,
               x = base_pred.columns.values,
               y = base_pred.columns.values,
               colorscale = &quot;Portland&quot;,
               showscale = True,
               reversescale = True
              )
]
py.iplot(data, filename = &quot;labelled-Heatmap&quot;)
</code></pre>
<p><img alt="jpgpng" src="/figures/20170917_aws_device_failure_18.png" title="Logo Title Text 1"></p>
<pre><code class="language-python">xgb_x_train = np.concatenate((et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis = 1)
xgb_y_train = fy_train

import xgboost as xgb

gbm = xgb.XGBClassifier(
    n_estimators = 200,
    max_depth = 50,
    min_child_weight = 2,
    gamma = 0.6,
    subsample = 0.8,
    colsample_bytree = 0.8,
    objective = &quot;binary:logistic&quot;,
    nthread = -1,
    scale_pos_weight = 1
).fit(xgb_x_train, xgb_y_train)
</code></pre>
<pre><code class="language-python">confusion_matrix(xgb_y_train, gbm.predict(xgb_x_train))
</code></pre>
<pre><code>array([[10267,   340],
       [    0, 10600]])
</code></pre>
<h3>GridSearch for All</h3>
<pre><code class="language-python">def gscv(estimator, tuned_parameters):
    cv_grid = GridSearchCV(estimator, param_grid = tuned_parameters, scoring = my_score)
    cv_grid.fit(xtrain, ytrain)

    best_parameters = cv_grid.best_estimator_.get_params()

    for param_name in sorted(tuned_parameters.keys()):
        print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))
    pred1 = cv_grid.predict(xtest)
    print(&quot;confustion matrix on validation data: &quot; + str(confusion_matrix(ytest, pred1)))
    pred_full = cv_grid.predict(indata[cols])
    print(&quot;confustion matrix on validation data: &quot; + str(confusion_matrix(indata.target, pred_full)))
    return pred_full

</code></pre>
<pre><code class="language-python">rf_tuned_parameters = {&quot;max_depth&quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
rf_estimator = RandomForestClassifier(random_state=0)
rf_out = gscv(rf_estimator, rf_tuned_parameters)
</code></pre>
<pre><code>max_depth: 20
min_samples_leaf: 2
n_estimators: 100

confustion matrix on validation data: [[4259   56] [   0 4168]]
confustion matrix on validation data: [[124384      4] [    88     18]]
</code></pre>
<pre><code class="language-python">et_tuned_parameters = {&quot;max_depth&quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
et_estimator = ExtraTreesClassifier(random_state=0)
et_out = gscv(et_estimator, et_tuned_parameters)
</code></pre>
<pre><code>max_depth: 50
min_samples_leaf: 2
n_estimators: 200

confustion matrix on validation data: [[4241   74] [   0 4168]]
confustion matrix on validation data: [[124382      6] [    82     24]]
</code></pre>
<pre><code class="language-python">ada_tuned_parameters = {'n_estimators': [20, 50, 100, 200], 'learning_rate': [0.2, 0.5, 1, 2, 5]}
ada_estimator = AdaBoostClassifier(random_state=0)
ada_out = gscv(ada_estimator, ada_tuned_parameters)
</code></pre>
<pre><code>learning_rate: 1
n_estimators: 200

confustion matrix on validation data: [[4026  289] [ 151 4017]]
confustion matrix on validation data: [[124388      0] [   105      1]]
</code></pre>
<pre><code class="language-python">gb_tuned_parameters = {&quot;max_depth&quot;: [2, 5, 10, 20, 50, 100], 'n_estimators': [20, 50, 100, 200], 
                           'min_samples_leaf': [2, 4, 10, 20]}
gb_estimator = GradientBoostingClassifier(random_state=0)
gb_out = gscv(gb_estimator, gb_tuned_parameters)
</code></pre>
<pre><code>max_depth: 10
min_samples_leaf: 20
n_estimators: 200

confustion matrix on validation data: [[4287   28] [   0 4168]]
confustion matrix on validation data: [[124386      2] [    77     29]]
</code></pre>
<pre><code class="language-python">svc_tuned_parameters = {&quot;C&quot;: [0.2, 0.5, 1, 2]}
svc_estimator = LinearSVC(random_state=0)
svc_out = gscv(svc_estimator, svc_tuned_parameters)
</code></pre>
<pre><code>C: 0.2

confustion matrix on validation data: [[4315    0] [4168    0]]
confustion matrix on validation data: [[124388      0] [   106      0]]
</code></pre>
<pre><code class="language-python">xgb_x_train_ = np.concatenate((rf_out.reshape(-1, 1), et_out.reshape(-1, 1), 
                               ada_out.reshape(-1, 1), gb_out.reshape(-1, 1), 
                               svc_out.reshape(-1, 1)), axis = 1)
xgb_y_train_ = indata.target

gbm = xgb.XGBClassifier(
    n_estimators = 200,
    max_depth = 50,
    min_child_weight = 2,
    gamma = 0.6,
    subsample = 0.8,
    colsample_bytree = 0.8,
    objective = &quot;binary:logistic&quot;,
    nthread = -1,
    scale_pos_weight = 1
).fit(xgb_x_train_, xgb_y_train_)
</code></pre>
<pre><code class="language-python">print(confusion_matrix(indata.target, gbm.predict(xgb_x_train_)))
</code></pre>
<pre><code>[[124386     2]
 [    60     46]]
</code></pre>
<h3>References</h3>
<ol>
<li><a href="http://scikit-learn.org/stable/modules/model_evaluation.html">sklearn score function</a></li>
<li><a href="https://www.hackerearth.com/zh/practice/machine-learning/machine-learning-algorithms/tutorial-random-forest-parameter-tuning-r/tutorial/">Practical Tutorial on Random Forest and Parameter Tuning in R</a></li>
</ol></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-09-23T18:08:00-05:00" pubdate>Sat 23 September 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/data-mining.html">data mining</a>,    <a class="category" href="/tag/sklearn.html">sklearn</a>,    <a class="category" href="/tag/data-visualization.html">data visualization</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/02/23/deepseek-v3-learning-notes/">DeepSeek V3 learning notes</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2024/04/21/prediction-in-decoder-and-kv-cache/">Prediction in decoder and KV-Cache</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/agi.html">AGI</a>,    <a href="/tag/gpt.html">GPT</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/09/23/data-engineering-and-modeling-01-predict-defaults-with-imbalanced-data/';
    var disqus_url = '/pages/2017/09/23/data-engineering-and-modeling-01-predict-defaults-with-imbalanced-data/';
    var disqus_title = 'Data Engineering and Modeling 01: predict defaults with imbalanced data';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>