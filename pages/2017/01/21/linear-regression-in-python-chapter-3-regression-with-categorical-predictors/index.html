<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>linear regression in python, Chapter 3 - Regression with Categorical Predictors &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">linear regression in python, Chapter 3 - Regression with Categorical Predictors</h1>
    <p class="meta">
<time datetime="2017-01-21T00:00:00-06:00" pubdate>Sat 21 January 2017</time>    </p>
</header>

  <div class="entry-content"><h2>3 - Regression with Categorical Predictors</h2>
<h3>Chapter Outline</h3>
<div class="highlight"><pre>3.0 Regression with categorical predictors
3.1 Regression with a 0/1 variable
3.2 Regression with a 1/2 variable
3.3 Regression with a 1/2/3 variable
3.4 Regression with multiple categorical predictors
3.5 Categorical predictor with interactions
3.6 Continuous and categorical variables
3.7 Interactions of continuous by 0/1 categorical variables
3.8 Continuous and categorical variables, interaction with 1/2/3 variable
3.9 Summary
3.10 For more information
</pre></div>


<h2>3.0 Introduction</h2>
<p>In the previous two chapters, we have focused on regression analyses using continuous variables. However, it is possible to include categorical predictors in a regression analysis, but it requires some extra work in performing the analysis and extra work in properly interpreting the results.  This chapter will illustrate how you can use Python for including categorical predictors in your analysis and describe how to interpret the results of such analyses.</p>
<p>This chapter will use the elemapi2 data that you have seen in the prior chapters. We will focus on four variables api00, some_col, yr_rnd and mealcat, which takes meals and breaks it up into three categories. Let's have a quick look at these variables.  </p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span><span class="p">,</span> <span class="n">combinations</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">scipystats</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.stats.stattools</span> <span class="kn">as</span> <span class="nn">stools</span>
<span class="kn">import</span> <span class="nn">statsmodels.stats</span> <span class="kn">as</span> <span class="nn">stats</span>
<span class="kn">from</span> <span class="nn">statsmodels.graphics.regressionplots</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>

<span class="n">mypath</span> <span class="o">=</span>  <span class="s1">r&#39;F:\Dropbox\ipynb_notes\ipynb_blog</span><span class="se">\\</span><span class="s1">&#39;</span>

<span class="n">elemapi2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">mypath</span> <span class="o">+</span> <span class="s1">r&#39;elemapi2.csv&#39;</span><span class="p">)</span>   
</pre></div>


<div class="highlight"><pre><span class="n">elemapi2_sel</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;api00&quot;</span><span class="p">,</span> <span class="s2">&quot;some_col&quot;</span><span class="p">,</span> <span class="s2">&quot;yr_rnd&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat&quot;</span><span class="p">]]</span>
</pre></div>


<div class="highlight"><pre><span class="k">print</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">cv_desc</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">var</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>

<span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>  
<span class="k">print</span> <span class="n">cv_desc</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="p">,</span> <span class="s1">&#39;mealcat&#39;</span><span class="p">)</span>
<span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>  
<span class="k">print</span> <span class="n">cv_desc</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="p">,</span> <span class="s1">&#39;yr_rnd&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>            api00    some_col     yr_rnd     mealcat
count  400.000000  400.000000  400.00000  400.000000
mean   647.622500   19.712500    0.23000    2.015000
std    142.248961   11.336938    0.42136    0.819423
min    369.000000    0.000000    0.00000    1.000000
25%    523.750000   12.000000    0.00000    1.000000
50%    643.000000   19.000000    0.00000    2.000000
75%    762.250000   28.000000    0.00000    3.000000
max    940.000000   67.000000    1.00000    3.000000


3    137
2    132
1    131
Name: mealcat, dtype: int64


0    308
1     92
Name: yr_rnd, dtype: int64
</pre></div>


<p>The variable api00 is a measure of the performance of the students. The variable some_col is a continuous variable that measures the percentage of the parents in the school who have attended college. The variable yr_rnd is a categorical variable that is coded 0 if the school is not year round, and 1 if year round. The variable meals is the percentage of students who are receiving state sponsored free meals and can be used as an indicator of poverty. This was broken into 3 categories (to make equally sized groups) creating the variable mealcat. The following macro function created for this dataset gives us codebook type information on a variable that we specify. It gives the information of the number of unique values that a variable take.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">codebook</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">var</span><span class="p">):</span>
    <span class="n">title</span> <span class="o">=</span> <span class="s2">&quot;Codebook for &quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">var</span><span class="p">)</span>
    <span class="n">unique_values</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
    <span class="n">max_v</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">min_v</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">n_miss</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">isnull</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]))</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">stdev</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">var</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="k">print</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;title&#39;</span><span class="p">:</span> <span class="n">title</span><span class="p">,</span> <span class="s1">&#39;unique values&#39;</span><span class="p">:</span> <span class="n">unique_values</span><span class="p">,</span> <span class="s1">&#39;max value&#39;</span> <span class="p">:</span> <span class="n">max_v</span><span class="p">,</span> <span class="s1">&#39;min value&#39;</span><span class="p">:</span> <span class="n">min_v</span><span class="p">,</span> <span class="s1">&#39;num of missing&#39;</span> <span class="p">:</span> <span class="n">n_miss</span><span class="p">,</span> <span class="s1">&#39;mean&#39;</span> <span class="p">:</span> <span class="n">mean</span><span class="p">,</span> <span class="s1">&#39;stdev&#39;</span> <span class="p">:</span> <span class="n">stdev</span><span class="p">},</span> <span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">return</span>

<span class="n">codebook</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="p">,</span> <span class="s1">&#39;api00&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>   max value      mean  min value  num of missing       stdev  \
0        940  647.6225        369               0  142.248961

                title  unique values  
0  Codebook for api00            271
</pre></div>


<h2>3.1 Regression with a 0/1 variable</h2>
<p>The simplest example of a categorical predictor in a regression analysis is a 0/1 variable, also called a dummy variable or sometimes an indicator variable. Let's use the variable yr_rnd as an example of a dummy variable. We can include a dummy variable as a predictor in a regression analysis as shown below.</p>
<div class="highlight"><pre><span class="n">reg</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;api00 ~ yr_rnd&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.226</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.224</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   116.2</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>5.96e-24</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:04:29</td>     <th>  Log-Likelihood:    </th> <td> -2498.9</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   5002.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   398</td>      <th>  BIC:               </th> <td>   5010.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  684.5390</td> <td>    7.140</td> <td>   95.878</td> <td> 0.000</td> <td>  670.503   698.575</td>
</tr>
<tr>
  <th>yr_rnd</th>    <td> -160.5064</td> <td>   14.887</td> <td>  -10.782</td> <td> 0.000</td> <td> -189.774  -131.239</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>45.748</td> <th>  Durbin-Watson:     </th> <td>   1.499</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  13.162</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.006</td> <th>  Prob(JB):          </th> <td> 0.00139</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.111</td> <th>  Cond. No.          </th> <td>    2.53</td>
</tr>
</table>

<p>This may seem odd at first, but this is a legitimate analysis. But what does this mean? Let's go back to basics and write out the regression equation that this model implies.</p>
<div class="highlight"><pre>api00 = Intercept + Byr_rnd * yr_rnd
</pre></div>


<p>where Intercept is the intercept (or constant) and we use Byr_rnd to represent the coefficient for variable yr_rnd.  Filling in the values from the regression equation, we get</p>
<div class="highlight"><pre>api00 = 684.539 + -160.5064 * yr_rnd
</pre></div>


<p>If a school is not a year-round school (i.e., yr_rnd is 0) the regression equation would simplify to</p>
<div class="highlight"><pre>api00 = constant    + 0 * Byr_rnd
api00 = 684.539     + 0 * -160.5064  
api00 = 684.539
</pre></div>


<p>If a school is a year-round school, the regression equation would simplify to</p>
<div class="highlight"><pre>api00 = constant + 1 * Byr_rnd
api00 = 684.539  + 1 * -160.5064
api00 = 524.0326
</pre></div>


<p>We can graph the observed values and the predicted values as shown below. Although yr_rnd only has two values, we can still draw a regression line showing the relationship between yr_rnd and api00.  Based on the results above, we see that the predicted value for non-year round schools is 684.539 and the predicted value for the year round schools is 524.032, and the slope of the line is negative, which makes sense since the coefficient for yr_rnd was negative (-160.5064).    </p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">yr_rnd</span><span class="p">,</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">)],</span> <span class="s1">&#39;r--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;api 2000&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;year round school&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_01.png" /></p>
<p>Let's compare these predicted values to the mean api00 scores for the year-round and non-year-round students. Let's create a format for variable yr_rnd and mealcat so we can label these categorical variables. Notice that we use the format statement in groupby aggregate mean below to show value labels for variable yr_rnd.</p>
<div class="highlight"><pre><span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;yr_rnd_c&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="mi">0</span><span class="p">:</span> <span class="s2">&quot;No&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;Yes&quot;</span><span class="p">})</span>
<span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;mealcat_c&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="mi">1</span><span class="p">:</span> <span class="s2">&quot;0-46</span><span class="si">% f</span><span class="s2">ree meals&quot;</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span> <span class="s2">&quot;47-80</span><span class="si">% f</span><span class="s2">ree meals&quot;</span><span class="p">,</span> <span class="mi">3</span><span class="p">:</span> <span class="s2">&quot;1-100</span><span class="si">% f</span><span class="s2">ree meals&quot;</span><span class="p">})</span>

<span class="n">elemapi2_sel_group</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;yr_rnd_c&quot;</span><span class="p">)</span>
<span class="n">elemapi2_sel_group</span><span class="o">.</span><span class="n">api00</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">])</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean</th>
      <th>std</th>
    </tr>
    <tr>
      <th>yr_rnd_c</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>No</th>
      <td>684.538961</td>
      <td>132.112534</td>
    </tr>
    <tr>
      <th>Yes</th>
      <td>524.032609</td>
      <td>98.916043</td>
    </tr>
  </tbody>
</table>
</div>

<p>As you see, the regression equation predicts that for a school, the value of api00 will be the mean value of the group determined by the school type.</p>
<p>Let's relate these predicted values back to the regression equation. For the non-year-round schools, their mean is the same as the intercept (684.539). The coefficient for yr_rnd is the amount we need to add to get the mean for the year-round schools, i.e., we need to add -160.5064 to get 524.0326, the mean for the non year-round schools. In other words, Byr_rnd is the mean api00 score for the year-round schools minus the mean api00 score for the non year-round schools, i.e., mean(year-round) - mean(non year-round).</p>
<p>It may be surprising to note that this regression analysis with a single dummy variable is the same as doing a t-test comparing the mean api00 for the year-round schools with the non year-round schools (see below). <strong>You can see that the t value below is the same as the t value for yr_rnd in the regression above</strong>. This is because Byr_rnd compares the non year-rounds and non year-rounds (since the coefficient is mean(year round)-mean(non year-round)).</p>
<div class="highlight"><pre><span class="c1"># pooled ttest, assume equal population variance</span>
<span class="k">print</span> <span class="n">scipystats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">)</span>

<span class="c1"># does not assume equal variance</span>
<span class="k">print</span> <span class="n">scipystats</span><span class="o">.</span><span class="n">ttest_ind</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">equal_var</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>Ttest_indResult(statistic=10.781500136400451, pvalue=5.9647081127888056e-24)
Ttest_indResult(statistic=12.57105956566846, pvalue=5.2973148066493157e-27)
</pre></div>


<p>Since a <strong>t-test is the same as doing an anova</strong>, we can get the same results using anova as well.</p>
<div class="highlight"><pre><span class="k">print</span> <span class="n">scipystats</span><span class="o">.</span><span class="n">f_oneway</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>F_onewayResult(statistic=116.2407451912029, pvalue=5.9647081127907988e-24)
</pre></div>


<p>If we square the t-value from the t-test, we get the same value as the F-value from anova:
<span style="color:red">10.78^2=116.21</span> (with a little rounding error.)</p>
<h2>3.2 Regression with a 1/2 variable</h2>
<p>A categorical predictor variable does not have to be coded 0/1 to be used in a regression model. It is easier to understand and interpret the results from a model with dummy variables, but the results from a variable coded 1/2 yield essentially the same results.</p>
<p>Lets make a copy of the variable yr_rnd called yr_rnd2 that is coded 1/2, 1=non year-round and 2=year-round.</p>
<div class="highlight"><pre><span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;yr_rnd2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;yr_rnd&quot;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ yr_rnd2&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.226</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.224</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   116.2</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>5.96e-24</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:15:36</td>     <th>  Log-Likelihood:    </th> <td> -2498.9</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   5002.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   398</td>      <th>  BIC:               </th> <td>   5010.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  845.0453</td> <td>   19.353</td> <td>   43.664</td> <td> 0.000</td> <td>  806.998   883.093</td>
</tr>
<tr>
  <th>yr_rnd2</th>   <td> -160.5064</td> <td>   14.887</td> <td>  -10.782</td> <td> 0.000</td> <td> -189.774  -131.239</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>45.748</td> <th>  Durbin-Watson:     </th> <td>   1.499</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  13.162</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.006</td> <th>  Prob(JB):          </th> <td> 0.00139</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.111</td> <th>  Cond. No.          </th> <td>    6.23</td>
</tr>
</table>

<p>Note that the coefficient for yr_rnd is the same as yr_rnd2. So, you can see that if you code yr_rnd as 0/1 or as 1/2, the regression coefficient works out to be the same. However the intercept (Intercept) is a bit less intuitive. When we used yr_rnd, the intercept was the mean for the non year-rounds. When using yr_rnd2, the intercept is the mean for the non year-rounds minus Byr_rnd2, i.e., 684.539 - (-160.506) = 845.045</p>
<p>Note that you can use 0/1 or 1/2 coding and the results for the coefficient come out the same, but the interpretation of constant in the regression equation is different. It is often easier to interpret the estimates for 0/1 coding.</p>
<p>In summary, these results indicate that the api00 scores are significantly different for the schools depending on the type of school, year round school versus non-year round school. Non year-round schools have significantly higher API scores than year-round schools. Based on the regression results, non year-round schools have scores that are 160.5 points higher than year-round schools.</p>
<h2>3.3 Regression with a 1/2/3 variable</h2>
<h3>3.3.1 Manually creating dummy variables</h3>
<p>Say, that we would like to examine the relationship between the amount of poverty and api scores. We don't have a measure of poverty, but we can use mealcat as a proxy for a measure of poverty. From the previous section, we have seen that variable mealcat has three unique values. These are the levels of percent of students on free meals. We can associate a value label to variable mealcat to make it more meaningful for us when we run python regression with mealcat.</p>
<div class="highlight"><pre><span class="n">elemapi2_sel_group</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;mealcat_c&quot;</span><span class="p">)</span>
<span class="n">elemapi2_sel_group</span><span class="o">.</span><span class="n">api00</span><span class="o">.</span><span class="n">agg</span><span class="p">([</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">])</span>
</pre></div>


<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>&lt;lambda&gt;</th>
      <th>mean</th>
      <th>std</th>
    </tr>
    <tr>
      <th>mealcat_c</th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0-46% free meals</th>
      <td>131</td>
      <td>805.717557</td>
      <td>65.668664</td>
    </tr>
    <tr>
      <th>1-100% free meals</th>
      <td>137</td>
      <td>504.379562</td>
      <td>62.727015</td>
    </tr>
    <tr>
      <th>47-80% free meals</th>
      <td>132</td>
      <td>639.393939</td>
      <td>82.135130</td>
    </tr>
  </tbody>
</table>
</div>

<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ mealcat&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.752</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.752</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1208.</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>1.29e-122</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:17:37</td>     <th>  Log-Likelihood:    </th> <td> -2271.1</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4546.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   398</td>      <th>  BIC:               </th> <td>   4554.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  950.9874</td> <td>    9.422</td> <td>  100.935</td> <td> 0.000</td> <td>  932.465   969.510</td>
</tr>
<tr>
  <th>mealcat</th>   <td> -150.5533</td> <td>    4.332</td> <td>  -34.753</td> <td> 0.000</td> <td> -159.070  -142.037</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 3.106</td> <th>  Durbin-Watson:     </th> <td>   1.516</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.212</td> <th>  Jarque-Bera (JB):  </th> <td>   3.112</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.214</td> <th>  Prob(JB):          </th> <td>   0.211</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.943</td> <th>  Cond. No.          </th> <td>    6.86</td>
</tr>
</table>

<p>This is looking at the linear effect of mealcat with api00, but mealcat is not an interval variable. Instead, you will want to code the variable so that all the information concerning the three levels is accounted for. In general, we need to go through a data step to create dummy variables. For example, in order to create dummy variables for mealcat, we can do the following using <strong>sklearn</strong> to create dummy variables.  </p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="n">le_mealcat</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">elemapi2_sel</span><span class="p">[</span><span class="s1">&#39;mealcat_dummy&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_mealcat</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span><span class="p">)</span>

<span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;mealcat_dummy&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>


<span class="n">ohe</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mealcat1&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat2&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat3&quot;</span><span class="p">])</span>
<span class="n">elemapi2_sel</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">elemapi2_sel</span><span class="p">,</span> <span class="n">dummy</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ mealcat2 + mealcat3&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.755</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.754</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   611.1</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>6.48e-122</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:20:30</td>     <th>  Log-Likelihood:    </th> <td> -2269.0</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4544.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   4556.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>   <td>  805.7176</td> <td>    6.169</td> <td>  130.599</td> <td> 0.000</td> <td>  793.589   817.846</td>
</tr>
<tr>
  <th>mealcat2[0]</th> <td>  -83.1618</td> <td>    4.354</td> <td>  -19.099</td> <td> 0.000</td> <td>  -91.722   -74.602</td>
</tr>
<tr>
  <th>mealcat2[1]</th> <td>  -83.1618</td> <td>    4.354</td> <td>  -19.099</td> <td> 0.000</td> <td>  -91.722   -74.602</td>
</tr>
<tr>
  <th>mealcat3[0]</th> <td> -150.6690</td> <td>    4.314</td> <td>  -34.922</td> <td> 0.000</td> <td> -159.151  -142.187</td>
</tr>
<tr>
  <th>mealcat3[1]</th> <td> -150.6690</td> <td>    4.314</td> <td>  -34.922</td> <td> 0.000</td> <td> -159.151  -142.187</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.593</td> <th>  Durbin-Watson:     </th> <td>   1.541</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.451</td> <th>  Jarque-Bera (JB):  </th> <td>   1.684</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.139</td> <th>  Prob(JB):          </th> <td>   0.431</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.847</td> <th>  Cond. No.          </th> <td>2.14e+16</td>
</tr>
</table>

<p>The interpretation of the coefficients is much like that for the binary variables. Group 1 is the omitted group, so Intercept is the mean for group 1. The coefficient for mealcat2 is the mean for group 2 minus the mean of the omitted group (group 1). And the coefficient for mealcat3 is the mean of group 3 minus the mean of group 1. You can verify this by comparing the coefficients with the means of the groups.</p>
<div class="highlight"><pre><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;mealcat&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>mealcat
1    805.717557
2    639.393939
3    504.379562
Name: api00, dtype: float64
</pre></div>


<p>Based on these results, we can say that the three groups differ in their api00 scores, and that in particular group 2 is significantly different from group1 (because mealcat2 was significant) and group 3 is significantly different from group 1 (because mealcat3 was significant).</p>
<h3>3.3.2 More on dummy coding</h3>
<p>Of course, there is the way of coding by hand. I will ignore it here.</p>
<h3>3.3.3 run regression with categorical variable directly</h3>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ C(mealcat)&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.755</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.754</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   611.1</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>6.48e-122</td>
</tr>
<tr>
  <th>Time:</th>                 <td>14:21:59</td>     <th>  Log-Likelihood:    </th> <td> -2269.0</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4544.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   4556.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>       <td>  805.7176</td> <td>    6.169</td> <td>  130.599</td> <td> 0.000</td> <td>  793.589   817.846</td>
</tr>
<tr>
  <th>C(mealcat)[T.2]</th> <td> -166.3236</td> <td>    8.708</td> <td>  -19.099</td> <td> 0.000</td> <td> -183.444  -149.203</td>
</tr>
<tr>
  <th>C(mealcat)[T.3]</th> <td> -301.3380</td> <td>    8.629</td> <td>  -34.922</td> <td> 0.000</td> <td> -318.302  -284.374</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.593</td> <th>  Durbin-Watson:     </th> <td>   1.541</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.451</td> <th>  Jarque-Bera (JB):  </th> <td>   1.684</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.139</td> <th>  Prob(JB):          </th> <td>   0.431</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.847</td> <th>  Cond. No.          </th> <td>    3.76</td>
</tr>
</table>

<h3>3.3.4 Other coding schemes</h3>
<p>It is generally very convenient to use dummy coding but it is not the only kind of coding that can be used. As you have seen, when you use dummy coding one of the groups becomes the reference group and all of the other groups are compared to that group. This may not be the most interesting set of comparisons.</p>
<p>Say you want to compare group 1 with 2, and group 2 with group 3. You need to generate a coding scheme that forms these 2 comparisons. In python, we can first generate the corresponding coding scheme in a data step shown below and use them in the regression.</p>
<p>We create two dummy variables, one for group 1 and the other for group 3.  </p>
<div class="highlight"><pre><span class="n">elemapi2_sel</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;api00&quot;</span><span class="p">,</span> <span class="s2">&quot;some_col&quot;</span><span class="p">,</span> <span class="s2">&quot;yr_rnd&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat&quot;</span><span class="p">]]</span>

<span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;mealcat1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;mealcat2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">2</span><span class="p">,</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="n">elemapi2_sel</span><span class="p">[</span><span class="s2">&quot;mealcat3&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="mf">2.0</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>

<span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;mealcat&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat1&quot;</span><span class="p">,</span>  <span class="s2">&quot;mealcat3&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>mealcat  mealcat1   mealcat3
1         0.666667  -0.333333    131
2        -0.333333  -0.333333    132
3        -0.333333   0.666667    137
dtype: int64
</pre></div>


<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ mealcat1 + mealcat3&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.755</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.754</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   611.1</td>
</tr>
<tr>
  <th>Date:</th>             <td>Mon, 09 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>6.48e-122</td>
</tr>
<tr>
  <th>Time:</th>                 <td>00:19:01</td>     <th>  Log-Likelihood:    </th> <td> -2269.0</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4544.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   397</td>      <th>  BIC:               </th> <td>   4556.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  649.8304</td> <td>    3.531</td> <td>  184.021</td> <td> 0.000</td> <td>  642.888   656.773</td>
</tr>
<tr>
  <th>mealcat1</th>  <td>  166.3236</td> <td>    8.708</td> <td>   19.099</td> <td> 0.000</td> <td>  149.203   183.444</td>
</tr>
<tr>
  <th>mealcat3</th>  <td> -135.0144</td> <td>    8.612</td> <td>  -15.677</td> <td> 0.000</td> <td> -151.945  -118.083</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.593</td> <th>  Durbin-Watson:     </th> <td>   1.541</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.451</td> <th>  Jarque-Bera (JB):  </th> <td>   1.684</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.139</td> <th>  Prob(JB):          </th> <td>   0.431</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.847</td> <th>  Cond. No.          </th> <td>    3.01</td>
</tr>
</table>

<p>If you compare the parameter estimates with the group means of mealcat you can verify that B1 (i.e. 0-46% free meals) is the mean of group 1 minus group 2, and B2 (i.e., 47-80% free meals) is the mean of group 2 minus group 3.  Both of these comparisons are significant, indicating that group 1 significantly differs from group 2, and group 2 significantly differs from group 3.</p>
<p>And the value of the intercept term Intercept is the unweighted average of the means of the three groups, (805.71756 +639.39394 +504.37956)/3 = 649.83035.</p>
<h2>3.4 Regression with two categorical predictors</h2>
<h3>3.4.1 Manually creating dummy variables</h3>
<p>Previously we looked at using yr_rnd to predict api00 and we have also looked at using mealcat to predict api00. Let's include the parameter estimates for each model below.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ yr_rnd&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ mealcat1 + mealcat2&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.226
Model:                            OLS   Adj. R-squared:                  0.224
Method:                 Least Squares   F-statistic:                     116.2
Date:                Sun, 22 Jan 2017   Prob (F-statistic):           5.96e-24
Time:                        14:23:42   Log-Likelihood:                -2498.9
No. Observations:                 400   AIC:                             5002.
Df Residuals:                     398   BIC:                             5010.
Df Model:                           1                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    684.5390      7.140     95.878      0.000       670.503   698.575
<span class="gh">yr_rnd      -160.5064     14.887    -10.782      0.000      -189.774  -131.239</span>
<span class="gh">==============================================================================</span>
Omnibus:                       45.748   Durbin-Watson:                   1.499
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               13.162
Skew:                           0.006   Prob(JB):                      0.00139
<span class="gh">Kurtosis:                       2.111   Cond. No.                         2.53</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.755
Model:                            OLS   Adj. R-squared:                  0.754
Method:                 Least Squares   F-statistic:                     611.1
Date:                Sun, 22 Jan 2017   Prob (F-statistic):          6.48e-122
Time:                        14:23:42   Log-Likelihood:                -2269.0
No. Observations:                 400   AIC:                             4544.
Df Residuals:                     397   BIC:                             4556.
Df Model:                           2                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    649.8304      3.531    184.021      0.000       642.888   656.773
mealcat1     301.3380      8.629     34.922      0.000       284.374   318.302
<span class="gh">mealcat2     135.0144      8.612     15.677      0.000       118.083   151.945</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.593   Durbin-Watson:                   1.541
Prob(Omnibus):                  0.451   Jarque-Bera (JB):                1.684
Skew:                          -0.139   Prob(JB):                        0.431
<span class="gh">Kurtosis:                       2.847   Cond. No.                         2.98</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>In the first model with only yr_rnd as the only predictor, the intercept term is the mean api score for the non-year-round schools. The coefficient for yr_rnd is the difference between the year round and non-year round group. In the second model, the coefficient for mealcat1 is the difference between mealcat=1 and mealcat=3, and the coefficient for mealcat2 is the difference between mealcat=2 and mealcat=3. The intercept is the mean for the mealcat=3.</p>
<p>Of course, we can include both yr_rnd and mealcat together in the same model. Now the question is how to interpret the coefficients.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ yr_rnd + mealcat1 + mealcat2&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.767
Model:                            OLS   Adj. R-squared:                  0.765
Method:                 Least Squares   F-statistic:                     435.0
Date:                Mon, 09 Jan 2017   Prob (F-statistic):          6.40e-125
Time:                        21:18:55   Log-Likelihood:                -2258.6
No. Observations:                 400   AIC:                             4525.
Df Residuals:                     396   BIC:                             4541.
Df Model:                           3                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    659.5396      4.043    163.126      0.000       651.591   667.488
yr_rnd       -42.9601      9.362     -4.589      0.000       -61.365   -24.555
mealcat1     281.6832      9.446     29.821      0.000       263.113   300.253
<span class="gh">mealcat2     117.9458      9.189     12.836      0.000        99.881   136.011</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.210   Durbin-Watson:                   1.584
Prob(Omnibus):                  0.546   Jarque-Bera (JB):                1.279
Skew:                          -0.086   Prob(JB):                        0.528
<span class="gh">Kurtosis:                       2.783   Cond. No.                         3.95</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>Let's dig below the surface and see how the coefficients relate to the predicted values. Let's view the cells formed by crossing yr_rnd and mealcat and number the cells from cell1 to cell6.</p>
<table>
<thead>
<tr>
<th></th>
<th>mealcat=1</th>
<th>mealcat=2</th>
<th>mealcat=3</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>yr_rnd=0</td>
<td>cell1</td>
<td>cell2</td>
<td>cell3</td>
<td></td>
</tr>
<tr>
<td>yr_rnd=1</td>
<td>cell4</td>
<td>cell5</td>
<td>cell6</td>
<td></td>
</tr>
</tbody>
</table>
<p>With respect to mealcat, the group mealcat=3 is the reference category, and with respect to yr_rnd the group yr_rnd=0 is the reference category. As a result, cell3 is the reference cell. The intercept is the predicted value for this cell.</p>
<p>The coefficient for yr_rnd is the difference between cell3 and cell6. Since this model has only main effects, it is also the difference between cell2 and cell5, or from cell1 and cell4. In other words, Byr_rnd is the amount you add to the predicted value when you go from non-year round to year round schools.</p>
<p>The coefficient for mealcat1 is the predicted difference between cell1 and cell3. Since this model only has main effects, it is also the predicted difference between cell4 and cell6. Likewise, Bmealcat2 is the predicted difference between cell2 and cell3, and also the predicted difference between cell5 and cell6.</p>
<p>So, the predicted values, in terms of the coefficients, would be</p>
<table>
<thead>
<tr>
<th></th>
<th>mealcat=1</th>
<th>mealcat=2</th>
<th>mealcat=3</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>yr_rnd=0</td>
<td>Intercept + Bmealcat1</td>
<td>Intercept + Bmealcat2</td>
<td>Intercept</td>
<td></td>
</tr>
<tr>
<td>yr_rnd=1</td>
<td>Intercept + Byr_rnd + Bmealcat1</td>
<td>Intercept + Byr_rnd + Bmealcat2</td>
<td>Intercept + Byr_rnd</td>
<td></td>
</tr>
</tbody>
</table>
<p>We should note that if you computed the predicted values for each cell, they would not exactly match the means in the six cells.  The predicted means would be close to the observed means in the cells, but not exactly the same.  This is because our model only has main effects and assumes that the difference between cell1 and cell4 is exactly the same as the difference between cells 2 and 5 which is the same as the difference between cells 3 and 5.  Since the observed values don't follow this pattern, there is some discrepancy between the predicted means and observed means.</p>
<h3>3.4.2 Using the <code>C</code> formula directly</h3>
<p>We can run the same analysis using the categorical formula without manually coding the dummy variables.  </p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ yr_rnd + C(mealcat)&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.767
Model:                            OLS   Adj. R-squared:                  0.765
Method:                 Least Squares   F-statistic:                     435.0
Date:                Sun, 22 Jan 2017   Prob (F-statistic):          6.40e-125
Time:                        14:24:47   Log-Likelihood:                -2258.6
No. Observations:                 400   AIC:                             4525.
Df Residuals:                     396   BIC:                             4541.
Df Model:                           3                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">===================================================================================</span>
                      coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
-----------------------------------------------------------------------------------
Intercept         808.0131      6.040    133.777      0.000       796.139   819.888
C(mealcat)[T.2]  -163.7374      8.515    -19.229      0.000      -180.478  -146.997
C(mealcat)[T.3]  -281.6832      9.446    -29.821      0.000      -300.253  -263.113
<span class="gh">yr_rnd            -42.9601      9.362     -4.589      0.000       -61.365   -24.555</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.210   Durbin-Watson:                   1.584
Prob(Omnibus):                  0.546   Jarque-Bera (JB):                1.279
Skew:                          -0.086   Prob(JB):                        0.528
<span class="gh">Kurtosis:                       2.783   Cond. No.                         4.17</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>In summary, these results indicate the differences between year round and non-year round schools is significant, and the differences among the three mealcat groups are significant.</p>
<h2>3.5 Categorical predictor with interactions</h2>
<h3>3.5.1 Manually creating dummy variables</h3>
<p>Let's perform the same analysis that we performed above, this time let's include the interaction of mealcat by yr_rnd. In this section we show how to do it by manually creating all the dummy variables. We use the array structure again. This time we have to declare two set of arrays, one for the dummy variables of mealcat and one for the interaction of yr_rnd and mealcat.</p>
<div class="highlight"><pre><span class="n">elemapi2_sel</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">ix</span><span class="p">[:,</span> <span class="p">[</span><span class="s2">&quot;api00&quot;</span><span class="p">,</span> <span class="s2">&quot;some_col&quot;</span><span class="p">,</span> <span class="s2">&quot;yr_rnd&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat&quot;</span><span class="p">]]</span>

<span class="n">ohe</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">OneHotEncoder</span><span class="p">()</span>
<span class="n">dummy</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">ohe</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">toarray</span><span class="p">(),</span> <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mealcat1&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat2&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat3&quot;</span><span class="p">])</span>
<span class="n">mealxynd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">dummy</span><span class="p">,</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">mealxynd</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;mealxynd1&quot;</span><span class="p">,</span> <span class="s2">&quot;mealxynd2&quot;</span><span class="p">,</span> <span class="s2">&quot;mealxynd3&quot;</span><span class="p">]</span>
<span class="n">elemapi2_sel</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">elemapi2_sel</span><span class="p">,</span> <span class="n">dummy</span><span class="p">,</span> <span class="n">mealxynd</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">elemapi2_sel</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;yr_rnd&quot;</span><span class="p">,</span> <span class="s2">&quot;mealcat&quot;</span><span class="p">,</span> <span class="s2">&quot;mealxynd1&quot;</span><span class="p">,</span> <span class="s2">&quot;mealxynd2&quot;</span><span class="p">,</span> <span class="s2">&quot;mealxynd3&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>yr_rnd  mealcat  mealxynd1  mealxynd2  mealxynd3
0       1        0.0        0.0        0.0          124
        2        0.0        0.0        0.0          117
        3        0.0        0.0        0.0           67
1       1        1.0        0.0        0.0            7
        2        0.0        1.0        0.0           15
        3        0.0        0.0        1.0           70
dtype: int64
</pre></div>


<p>Now let's add these dummy variables for interaction between yr_rnd and mealcat to our model. We can all add a test statement to test the overall interaction. The output shows that the interaction effect is not significant.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ yr_rnd + mealcat1 + mealcat2 + mealxynd1 + mealxynd2&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.769
Model:                            OLS   Adj. R-squared:                  0.766
Method:                 Least Squares   F-statistic:                     261.6
Date:                Sun, 22 Jan 2017   Prob (F-statistic):          9.19e-123
Time:                        14:26:06   Log-Likelihood:                -2257.5
No. Observations:                 400   AIC:                             4527.
Df Residuals:                     394   BIC:                             4551.
Df Model:                           5                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    521.4925      8.414     61.978      0.000       504.950   538.035
yr_rnd       -33.4925     11.771     -2.845      0.005       -56.635   -10.350
mealcat1     288.1929     10.443     27.597      0.000       267.662   308.724
mealcat2     123.7810     10.552     11.731      0.000       103.036   144.526
mealxynd1    -40.7644     29.231     -1.395      0.164       -98.233    16.704
<span class="gh">mealxynd2    -18.2476     22.256     -0.820      0.413       -62.003    25.508</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.439   Durbin-Watson:                   1.583
Prob(Omnibus):                  0.487   Jarque-Bera (JB):                1.484
Skew:                          -0.096   Prob(JB):                        0.476
<span class="gh">Kurtosis:                       2.771   Cond. No.                         10.4</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>It is important to note how the meaning of the coefficients change in the presence of these interaction terms. For example, in the prior model, with only main effects, we could interpret Byr_rnd as the difference between the year round and non year round schools. However, now that we have added the interaction term, the term Byr_rnd represents the difference between cell3 and cell6, or the difference between the year round and non-year round schools when mealcat=3 (because mealcat=3 was the omitted group). The presence of an interaction would imply that the difference between year round and non-year round schools depends on the level of mealcat. The interaction terms Bmealxynd1 and Bmealxynd2 represent the extent to which the difference between the year round/non year round schools changes when mealcat=1 and when mealcat=2 (as compared to the reference group, mealcat=3). For example the term Bmealxynd1 represents the difference between year round and non-year round for mealcat=1 versus the difference for mealcat=3. In other words, Bmealxynd1 in this design is (cell1-cell4) - (cell3-cell6), or it represents how much the effect of yr_rnd differs between mealcat=1 and mealcat=3.</p>
<p>Below we have shown the predicted values for the six cells in terms of the coefficients in the model.  If you compare this to the main effects model, you will see that the predicted values are the same except for the addition of mealxynd1 (in cell 4) and mealxynd2 (in cell 5).  </p>
<table>
<thead>
<tr>
<th></th>
<th>mealcat=1</th>
<th>mealcat=2</th>
<th>mealcat=3</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>yr_rnd=0</td>
<td>Intercept + Bmealcat1</td>
<td>Intercept + Bmealcat2</td>
<td>Intercept</td>
<td></td>
</tr>
<tr>
<td>yr_rnd=1</td>
<td>Intercept + Byr_rnd + Bmealcat1 + Bmealxynd1</td>
<td>Intercept + Byr_rnd + Bmealcat2  + Bmealxynd2</td>
<td>Intercept + Byr_rnd</td>
<td></td>
</tr>
</tbody>
</table>
<p>It can be very tricky to interpret these interaction terms if you wish to form specific comparisons. For example, if you wanted to perform a test of the simple main effect of yr_rnd when mealcat=1, i.e., comparing compare cell1 with cell4, you would want to compare Intercept+ mealcat1 versus Intercept + mealcat1 + yr_rnd + mealxynd1 and since Intercept and Imealcat1 would drop out.</p>
<h3>3.5.2 Using anova</h3>
<p>Constructing these interactions can be easier. We can also avoid manually coding our dummy variables. As you see below, the <code>C</code> formula gives us the test of the overall main effects and interactions without the need to perform subsequent test commands.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ C(yr_rnd) * C(mealcat)&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_sel</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.769
Model:                            OLS   Adj. R-squared:                  0.766
Method:                 Least Squares   F-statistic:                     261.6
Date:                Mon, 09 Jan 2017   Prob (F-statistic):          9.19e-123
Time:                        21:54:22   Log-Likelihood:                -2257.5
No. Observations:                 400   AIC:                             4527.
Df Residuals:                     394   BIC:                             4551.
Df Model:                           5                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==================================================================================================</span>
                                     coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
--------------------------------------------------------------------------------------------------
Intercept                        809.6855      6.185    130.911      0.000       797.526   821.845
C(yr_rnd)[T.1]                   -74.2569     26.756     -2.775      0.006      -126.860   -21.654
C(mealcat)[T.2]                 -164.4120      8.877    -18.522      0.000      -181.864  -146.960
C(mealcat)[T.3]                 -288.1929     10.443    -27.597      0.000      -308.724  -267.662
C(yr_rnd)[T.1]:C(mealcat)[T.2]    22.5167     32.752      0.687      0.492       -41.873    86.907
<span class="gh">C(yr_rnd)[T.1]:C(mealcat)[T.3]    40.7644     29.231      1.395      0.164       -16.704    98.233</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.439   Durbin-Watson:                   1.583
Prob(Omnibus):                  0.487   Jarque-Bera (JB):                1.484
Skew:                          -0.096   Prob(JB):                        0.476
<span class="gh">Kurtosis:                       2.771   Cond. No.                         16.4</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>The results from above show us the effect of yr_rnd at each of the three levels of mealcat. We can see that the comparison for mealcat = 1 matches those we computed above using the test statement, however, it was much easier and less error prone using the lsmeans statement.</p>
<p>Although this section has focused on how to handle analyses involving interactions, these particular results show no indication of interaction. We could decide to omit interaction terms from future analyses having found the interactions to be non-significant. This would simplify future analyses, however including the interaction term can be useful to assure readers that the interaction term is non-significant.</p>
<h2>3.6 Continuous and categorical variables</h2>
<h3>3.6.1 Using regression</h3>
<p>Say that we wish to analyze both continuous and categorical variables in one analysis. For example, let's include yr_rnd and some_col in the same analysis. We can also plot the predicted values against some_col using plot statement.</p>
<div class="highlight"><pre><span class="n">elemapi2_</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ yr_rnd + some_col&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">elemapi2_</span><span class="p">[</span><span class="s1">&#39;pred&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2_</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">elemapi2_</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2_</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">elemapi2_</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">pred</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.257
Model:                            OLS   Adj. R-squared:                  0.253
Method:                 Least Squares   F-statistic:                     68.54
Date:                Sun, 22 Jan 2017   Prob (F-statistic):           2.69e-26
Time:                        14:27:31   Log-Likelihood:                -2490.8
No. Observations:                 400   AIC:                             4988.
Df Residuals:                     397   BIC:                             5000.
Df Model:                           2                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    637.8581     13.503     47.237      0.000       611.311   664.405
yr_rnd      -149.1591     14.875    -10.027      0.000      -178.403  -119.915
<span class="gh">some_col       2.2357      0.553      4.044      0.000         1.149     3.323</span>
<span class="gh">==============================================================================</span>
Omnibus:                       23.070   Durbin-Watson:                   1.565
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                9.935
Skew:                           0.125   Prob(JB):                      0.00696
<span class="gh">Kurtosis:                       2.269   Cond. No.                         62.5</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_02.png" /></p>
<p>The coefficient for some_col indicates that for every unit increase in some_col the api00 score is predicted to increase by 2.23 units. This is the slope of the lines shown in the above graph. The graph has two lines, one for the year round schools and one for the non-year round schools. The coefficient for yr_rnd is -149.16, indicating that as yr_rnd increases by 1 unit, the api00 score is expected to decrease by about 149 units. As you can see in the graph, the top line is about 150 units higher than the lower line. You can see that the intercept is 637 and that is where the upper line crosses the Y axis when X is 0. The lower line crosses the line about 150 units lower at about 487.</p>
<h3>3.6.2 Using categorical variable directly</h3>
<p>We can run this analysis using the categorical variable directly. We need to use the specify which variables should be considered as categorical variables.  </p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s1">&#39;api00 ~ C(yr_rnd) + some_col&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2_</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.257
Model:                            OLS   Adj. R-squared:                  0.253
Method:                 Least Squares   F-statistic:                     68.54
Date:                Sun, 22 Jan 2017   Prob (F-statistic):           2.69e-26
Time:                        14:29:38   Log-Likelihood:                -2490.8
No. Observations:                 400   AIC:                             4988.
Df Residuals:                     397   BIC:                             5000.
Df Model:                           2                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==================================================================================</span>
                     coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
----------------------------------------------------------------------------------
Intercept        637.8581     13.503     47.237      0.000       611.311   664.405
C(yr_rnd)[T.1]  -149.1591     14.875    -10.027      0.000      -178.403  -119.915
<span class="gh">some_col           2.2357      0.553      4.044      0.000         1.149     3.323</span>
<span class="gh">==============================================================================</span>
Omnibus:                       23.070   Durbin-Watson:                   1.565
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                9.935
Skew:                           0.125   Prob(JB):                      0.00696
<span class="gh">Kurtosis:                       2.269   Cond. No.                         62.5</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<h2>3.7 Interactions of Continuous by 0/1 Categorical variables</h2>
<p>Above we showed an analysis that looked at the relationship between some_col and api00 and also included yr_rnd.  We saw that this produced a graph where we saw the relationship between some_col and api00 but there were two regression lines, one higher than the other but with equal slope.  Such a model assumed that the slope was the same for the two groups.  Perhaps the slope might be different for these groups.  Let's run the regressions separately for these two groups beginning with the non-year round schools.</p>
<div class="highlight"><pre><span class="n">lm_0</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;api00 ~ some_col&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm_0</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.016</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.013</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   4.915</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th>  <td>0.0274</td>
</tr>
<tr>
  <th>Time:</th>                 <td>09:43:46</td>     <th>  Log-Likelihood:    </th> <td> -1938.2</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   308</td>      <th>  AIC:               </th> <td>   3880.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   306</td>      <th>  BIC:               </th> <td>   3888.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  655.1103</td> <td>   15.237</td> <td>   42.995</td> <td> 0.000</td> <td>  625.128   685.093</td>
</tr>
<tr>
  <th>some_col</th>  <td>    1.4094</td> <td>    0.636</td> <td>    2.217</td> <td> 0.027</td> <td>    0.158     2.660</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>63.461</td> <th>  Durbin-Watson:     </th> <td>   1.531</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  13.387</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.003</td> <th>  Prob(JB):          </th> <td> 0.00124</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 1.979</td> <th>  Cond. No.          </th> <td>    48.9</td>
</tr>
</table>

<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">lm_0</span><span class="o">.</span><span class="n">predict</span><span class="p">(),</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;predict&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_03.png" /></p>
<p>Likewise, let's look at the year round schools and we will use the same symbol statements as above.</p>
<div class="highlight"><pre><span class="n">lm_1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;api00 ~ some_col&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;actual&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi2</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">lm_1</span><span class="o">.</span><span class="n">predict</span><span class="p">(),</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s2">&quot;predict&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_04.png" /></p>
<p>Note that the slope of the regression line looks much steeper for the year round schools than for the non-year round schools. This is confirmed by the regression equations that show the slope for the year round schools to be higher (7.4) than non-year round schools (1.3). We can compare these to see if these are significantly different from each other by including the interaction of some_col by yr_rnd, an interaction of a continuous variable by a categorical variable.</p>
<h2>3.7.1 Computing interactions manually</h2>
<p>We will start by manually computing the interaction of some_col by yr_rnd.</p>
<p>Next, let's make a variable that is the interaction of some college (some_col) and year round schools (yr_rnd) called yrxsome.</p>
<div class="highlight"><pre><span class="n">yrxsome_elemapi</span> <span class="o">=</span> <span class="n">elemapi2</span>
<span class="n">yrxsome_elemapi</span><span class="p">[</span><span class="s2">&quot;yrxsome&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span> <span class="o">*</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">some_col</span>
</pre></div>


<p>We can now run the regression that tests whether the coefficient for some_col is significantly different for year round schools and non-year round schools. Indeed, the yrxsome interaction effect is significant.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + yr_rnd + yrxsome&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.283</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.277</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   52.05</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>2.21e-28</td>
</tr>
<tr>
  <th>Time:</th>                 <td>09:48:54</td>     <th>  Log-Likelihood:    </th> <td> -2483.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4975.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   396</td>      <th>  BIC:               </th> <td>   4991.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  655.1103</td> <td>   14.035</td> <td>   46.677</td> <td> 0.000</td> <td>  627.518   682.703</td>
</tr>
<tr>
  <th>some_col</th>  <td>    1.4094</td> <td>    0.586</td> <td>    2.407</td> <td> 0.017</td> <td>    0.258     2.561</td>
</tr>
<tr>
  <th>yr_rnd</th>    <td> -248.0712</td> <td>   29.859</td> <td>   -8.308</td> <td> 0.000</td> <td> -306.773  -189.369</td>
</tr>
<tr>
  <th>yrxsome</th>   <td>    5.9932</td> <td>    1.577</td> <td>    3.800</td> <td> 0.000</td> <td>    2.893     9.094</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>23.863</td> <th>  Durbin-Watson:     </th> <td>   1.593</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>   9.350</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.023</td> <th>  Prob(JB):          </th> <td> 0.00932</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.252</td> <th>  Cond. No.          </th> <td>    117.</td>
</tr>
</table>

<p>We can then save the predicted values to a data set and graph the predicted values for the two types of schools by some_col. You can see how the two lines have quite different slopes, consistent with the fact that the yrxsome interaction was significant.   </p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.collections.PathCollection at 0x1107b780&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_05.png" /></p>
<p>We can also create a plot including the data points. There are multiple ways of doing this and we'll show both ways and their graphs here.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;*&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.collections.PathCollection at 0x1282cf98&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_06.png" /></p>
<p>We can further enhance it so the data points are marked with different symbols. The graph above used the same kind of symbols for the data points for both types of schools. Let's make separate variables for the api00 scores for the two types of schools called api0 for the non-year round schools and api1 for the year round schools.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;*&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;d&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.collections.PathCollection at 0x129dccf8&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_07.png" /></p>
<p>Let's quickly run the regressions again where we performed separate regressions for the two groups. We first split data to yr_rnd = 0 group and yr_rnd = 1 group. Then run regression of api00 to some_col in each group seperately.</p>
<div class="highlight"><pre><span class="n">yrxsome_elemapi_0</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span>
<span class="n">yrxsome_elemapi_1</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span>


<span class="n">lm_0</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yrxsome_elemapi_0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm_0</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="k">print</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>

<span class="n">lm_1</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yrxsome_elemapi_1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm_1</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.016
Model:                            OLS   Adj. R-squared:                  0.013
Method:                 Least Squares   F-statistic:                     4.915
Date:                Sun, 22 Jan 2017   Prob (F-statistic):             0.0274
Time:                        13:43:39   Log-Likelihood:                -1938.2
No. Observations:                 308   AIC:                             3880.
Df Residuals:                     306   BIC:                             3888.
Df Model:                           1                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    655.1103     15.237     42.995      0.000       625.128   685.093
<span class="gh">some_col       1.4094      0.636      2.217      0.027         0.158     2.660</span>
<span class="gh">==============================================================================</span>
Omnibus:                       63.461   Durbin-Watson:                   1.531
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               13.387
Skew:                          -0.003   Prob(JB):                      0.00124
<span class="gh">Kurtosis:                       1.979   Cond. No.                         48.9</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.


                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.420
Model:                            OLS   Adj. R-squared:                  0.413
Method:                 Least Squares   F-statistic:                     65.08
Date:                Sun, 22 Jan 2017   Prob (F-statistic):           2.97e-12
Time:                        13:43:39   Log-Likelihood:                -527.68
No. Observations:                  92   AIC:                             1059.
Df Residuals:                      90   BIC:                             1064.
Df Model:                           1                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    407.0391     16.515     24.647      0.000       374.230   439.848
<span class="gh">some_col       7.4026      0.918      8.067      0.000         5.580     9.226</span>
<span class="gh">==============================================================================</span>
Omnibus:                        5.042   Durbin-Watson:                   1.457
Prob(Omnibus):                  0.080   Jarque-Bera (JB):                4.324
Skew:                           0.471   Prob(JB):                        0.115
<span class="gh">Kurtosis:                       3.492   Cond. No.                         37.7</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>Now, let's show the regression for both types of schools with the interaction term.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + yr_rnd + yrxsome&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.283</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.277</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   52.05</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>2.21e-28</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:43:29</td>     <th>  Log-Likelihood:    </th> <td> -2483.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4975.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   396</td>      <th>  BIC:               </th> <td>   4991.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th> <td>  655.1103</td> <td>   14.035</td> <td>   46.677</td> <td> 0.000</td> <td>  627.518   682.703</td>
</tr>
<tr>
  <th>some_col</th>  <td>    1.4094</td> <td>    0.586</td> <td>    2.407</td> <td> 0.017</td> <td>    0.258     2.561</td>
</tr>
<tr>
  <th>yr_rnd</th>    <td> -248.0712</td> <td>   29.859</td> <td>   -8.308</td> <td> 0.000</td> <td> -306.773  -189.369</td>
</tr>
<tr>
  <th>yrxsome</th>   <td>    5.9932</td> <td>    1.577</td> <td>    3.800</td> <td> 0.000</td> <td>    2.893     9.094</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>23.863</td> <th>  Durbin-Watson:     </th> <td>   1.593</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>   9.350</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.023</td> <th>  Prob(JB):          </th> <td> 0.00932</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.252</td> <th>  Cond. No.          </th> <td>    117.</td>
</tr>
</table>

<p>Note that <strong>the coefficient for some_col in the combined analysis is the same as the coefficient for some_col for the non-year round schools? This is because non-year round schools are the reference group</strong>. Then, <strong>the coefficient for the yrxsome interaction in the combined analysis is the Bsome_col for the year round schools (7.4) minus Bsome_col for the non year round schools (1.41) yielding 5.99</strong>. This interaction is the difference in the slopes of some_col for the two types of schools, and this is why this is useful for testing whether the regression lines for the two types of schools are equal. If the two types of schools had the same regression coefficient for some_col, then the coefficient for the yrxsome interaction would be 0. In this case, the difference is significant, indicating that the regression lines are significantly different.</p>
<p>So, if we look at the graph of the two regression lines we can see the difference in the slopes of the regression lines (see graph below).  Indeed, we can see that the non-year round schools (the solid line) have a smaller slope (1.4) than the slope for the year round schools (7.4).  <strong>The difference between these slopes is 5.99, which is the coefficient for yrxsome</strong>.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 0&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;yr_rnd == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">yrxsome_elemapi</span><span class="o">.</span><span class="n">yr_rnd</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.collections.PathCollection at 0x12bba390&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_08.png" /></p>
<h3>3.7.2 Computing interactions with combinations</h3>
<p>We can also run a model just like the model we showed above. We can include the terms yr_rnd some_col and the interaction yr_rnr*some_col.   </p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + yr_rnd + yr_rnd * some_col &quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">yrxsome_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.283</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.277</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   52.05</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>2.21e-28</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:43:22</td>     <th>  Log-Likelihood:    </th> <td> -2483.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4975.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   396</td>      <th>  BIC:               </th> <td>   4991.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   
</tr>
</table>

<table class="simpletable">
<tr>
         <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>       <td>  655.1103</td> <td>   14.035</td> <td>   46.677</td> <td> 0.000</td> <td>  627.518   682.703</td>
</tr>
<tr>
  <th>some_col</th>        <td>    1.4094</td> <td>    0.586</td> <td>    2.407</td> <td> 0.017</td> <td>    0.258     2.561</td>
</tr>
<tr>
  <th>yr_rnd</th>          <td> -248.0712</td> <td>   29.859</td> <td>   -8.308</td> <td> 0.000</td> <td> -306.773  -189.369</td>
</tr>
<tr>
  <th>yr_rnd:some_col</th> <td>    5.9932</td> <td>    1.577</td> <td>    3.800</td> <td> 0.000</td> <td>    2.893     9.094</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>23.863</td> <th>  Durbin-Watson:     </th> <td>   1.593</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>   9.350</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.023</td> <th>  Prob(JB):          </th> <td> 0.00932</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.252</td> <th>  Cond. No.          </th> <td>    117.</td>
</tr>
</table>

<p>In this section we found that the relationship between some_col and api00 depended on whether the school was from year round schools or from non-year round schools.  For the schools from year round schools, the relationship between some_col and api00 was significantly stronger than for those from non-year round schools.  In general, this type of analysis allows you to test whether the strength of the relationship between two continuous variables varies based on the categorical variable.</p>
<h2>3.8 Continuous and categorical variables, interaction with 1/2/3 variable</h2>
<p>The prior examples showed how to do regressions with a continuous variable and a categorical variable that has two levels.  These examples will extend this further by using a categorical variable with three levels, mealcat.   </p>
<h3>3.8.1 Manually creating dummy variables</h3>
<p>We can use a data step to create all the dummy variables needed for the interaction of mealcat and some_col just as we did before for mealcat. With the dummy variables, we can use regression for the regression analysis. We'll use mealcat1 as the reference group.</p>
<div class="highlight"><pre><span class="n">mxcol_elemapi</span> <span class="o">=</span> <span class="n">elemapi2</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mealcat1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">1</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mealcat2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">2</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mealcat3&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span> <span class="o">==</span> <span class="mi">3</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mxcol1&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat1</span> <span class="o">*</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">some_col</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mxcol2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat2</span> <span class="o">*</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">some_col</span>
<span class="n">mxcol_elemapi</span><span class="p">[</span><span class="s2">&quot;mxcol3&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat3</span> <span class="o">*</span> <span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">some_col</span>

<span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + mealcat2 + mealcat3 +  mxcol2 + mxcol3&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.769</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.767</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   263.0</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>4.13e-123</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:44:39</td>     <th>  Log-Likelihood:    </th> <td> -2256.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4525.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   394</td>      <th>  BIC:               </th> <td>   4549.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>        <td>  825.8937</td> <td>   11.992</td> <td>   68.871</td> <td> 0.000</td> <td>  802.318   849.470</td>
</tr>
<tr>
  <th>mealcat2[T.True]</th> <td> -239.0300</td> <td>   18.665</td> <td>  -12.806</td> <td> 0.000</td> <td> -275.725  -202.334</td>
</tr>
<tr>
  <th>mealcat3[T.True]</th> <td> -344.9476</td> <td>   17.057</td> <td>  -20.223</td> <td> 0.000</td> <td> -378.483  -311.413</td>
</tr>
<tr>
  <th>some_col</th>         <td>   -0.9473</td> <td>    0.487</td> <td>   -1.944</td> <td> 0.053</td> <td>   -1.906     0.011</td>
</tr>
<tr>
  <th>mxcol2</th>           <td>    3.1409</td> <td>    0.729</td> <td>    4.307</td> <td> 0.000</td> <td>    1.707     4.575</td>
</tr>
<tr>
  <th>mxcol3</th>           <td>    2.6073</td> <td>    0.896</td> <td>    2.910</td> <td> 0.004</td> <td>    0.846     4.369</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.272</td> <th>  Durbin-Watson:     </th> <td>   1.565</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.530</td> <th>  Jarque-Bera (JB):  </th> <td>   1.369</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.114</td> <th>  Prob(JB):          </th> <td>   0.504</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.826</td> <th>  Cond. No.          </th> <td>    177.</td>
</tr>
</table>

<p>The interaction now has two terms (mxcol2 and mxcol3). These results indicate that the overall interaction is indeed significant.  This means that the regression lines from the three groups differ significantly. As we have done before, let's compute the predicted values and make a graph of the predicted values so we can see how the regression lines differ.</p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;mealcat == 1&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;mealcat == 2&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">2</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s1">&#39;mealcat == 3&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">some_col</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">predict</span><span class="p">()[</span><span class="n">mxcol_elemapi</span><span class="o">.</span><span class="n">mealcat</span><span class="o">.</span><span class="n">values</span> <span class="o">==</span> <span class="mi">3</span><span class="p">],</span> <span class="n">c</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="s2">&quot;+&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.collections.PathCollection at 0x144a4240&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_3_1_09.png" /></p>
<p><strong>Group 1 was the omitted group, therefore the slope of the line for group 1 is the coefficient for some_col which is -.94</strong>.  Indeed, this line has a downward slope.  <strong>If we add the coefficient for some_col to the coefficient for mxcol2 we get the coefficient for group 2, i.e., 3.14 + (-.94) yields 2.2, the slope for group 2</strong>. Indeed, group 2 shows an upward slope. Likewise,  <strong>if we add the coefficient for some_col to the coefficient for mxcol3 we get the coefficient for group 3, i.e., 2.6 + (-.94) yields 1.66, the slope for group 3</strong>,.  So, the slopes for the 3 groups are</p>
<div class="highlight"><pre>group 1: -0.94
group 2:  2.2
group 3:  1.66
</pre></div>


<p>The test of the coefficient in the parameter estimates for mxcol2 tested whether the coefficient for group 2 differed from group 1, and indeed this was significant.  Likewise, the test of the coefficient for mxcol3 tested whether the coefficient for group 3 differed from group 1, and indeed this was significant.  What did the test of the coefficient some_col test?  This coefficient represents the coefficient for group 1, so this tested whether the coefficient for group 1 (-0.94) was significantly different from 0.  This is probably a non-interesting test.</p>
<p>The comparisons in the above analyses don't seem to be as interesting as comparing group 1 versus 2 and then comparing group 2 versus 3.  These successive comparisons seem much more interesting. We can do this by making group 2 the omitted group, and then each group would be compared to group 2.  </p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + mealcat1 + mealcat3 +  mxcol1 + mxcol3&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.769</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.767</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   263.0</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>4.13e-123</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:52:16</td>     <th>  Log-Likelihood:    </th> <td> -2256.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4525.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   394</td>      <th>  BIC:               </th> <td>   4549.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
          <td></td>            <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>        <td>  586.8637</td> <td>   14.303</td> <td>   41.030</td> <td> 0.000</td> <td>  558.744   614.984</td>
</tr>
<tr>
  <th>mealcat1[T.True]</th> <td>  239.0300</td> <td>   18.665</td> <td>   12.806</td> <td> 0.000</td> <td>  202.334   275.725</td>
</tr>
<tr>
  <th>mealcat3[T.True]</th> <td> -105.9176</td> <td>   18.754</td> <td>   -5.648</td> <td> 0.000</td> <td> -142.789   -69.046</td>
</tr>
<tr>
  <th>some_col</th>         <td>    2.1936</td> <td>    0.543</td> <td>    4.043</td> <td> 0.000</td> <td>    1.127     3.260</td>
</tr>
<tr>
  <th>mxcol1</th>           <td>   -3.1409</td> <td>    0.729</td> <td>   -4.307</td> <td> 0.000</td> <td>   -4.575    -1.707</td>
</tr>
<tr>
  <th>mxcol3</th>           <td>   -0.5336</td> <td>    0.927</td> <td>   -0.576</td> <td> 0.565</td> <td>   -2.357     1.289</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.272</td> <th>  Durbin-Watson:     </th> <td>   1.565</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.530</td> <th>  Jarque-Bera (JB):  </th> <td>   1.369</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.114</td> <th>  Prob(JB):          </th> <td>   0.504</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.826</td> <th>  Cond. No.          </th> <td>    195.</td>
</tr>
</table>

<p>Now, the test of mxcol1 tests whether the coefficient for group 1 differs from group 2, and it does.  Then, the test of mxcol3 tests whether the coefficient for group 3 significantly differs from group 2, and it does not. This makes sense given the graph and given the estimates of the coefficients that we have, that -.94 is significantly different from 2.2 but 2.2 is not significantly different from 1.66.</p>
<h2>3.8.2 Using the combinations</h2>
<p>We can perform the same analysis using the <code>C</code> and combinations directly as shown below.  This allows us to avoid dummy coding for either the categorical variable mealcat and for the interaction term of mealcat and some_col. The tricky part  is to control the reference group.</p>
<div class="highlight"><pre><span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="s2">&quot;api00 ~ some_col + C(mealcat) + some_col * C(mealcat)&quot;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">mxcol_elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>api00</td>      <th>  R-squared:         </th> <td>   0.769</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.767</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   263.0</td>
</tr>
<tr>
  <th>Date:</th>             <td>Sun, 22 Jan 2017</td> <th>  Prob (F-statistic):</th> <td>4.13e-123</td>
</tr>
<tr>
  <th>Time:</th>                 <td>13:53:55</td>     <th>  Log-Likelihood:    </th> <td> -2256.6</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   400</td>      <th>  AIC:               </th> <td>   4525.</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>   394</td>      <th>  BIC:               </th> <td>   4549.</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    
</tr>
</table>

<table class="simpletable">
<tr>
              <td></td>                <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>Intercept</th>                <td>  825.8937</td> <td>   11.992</td> <td>   68.871</td> <td> 0.000</td> <td>  802.318   849.470</td>
</tr>
<tr>
  <th>C(mealcat)[T.2]</th>          <td> -239.0300</td> <td>   18.665</td> <td>  -12.806</td> <td> 0.000</td> <td> -275.725  -202.334</td>
</tr>
<tr>
  <th>C(mealcat)[T.3]</th>          <td> -344.9476</td> <td>   17.057</td> <td>  -20.223</td> <td> 0.000</td> <td> -378.483  -311.413</td>
</tr>
<tr>
  <th>some_col</th>                 <td>   -0.9473</td> <td>    0.487</td> <td>   -1.944</td> <td> 0.053</td> <td>   -1.906     0.011</td>
</tr>
<tr>
  <th>some_col:C(mealcat)[T.2]</th> <td>    3.1409</td> <td>    0.729</td> <td>    4.307</td> <td> 0.000</td> <td>    1.707     4.575</td>
</tr>
<tr>
  <th>some_col:C(mealcat)[T.3]</th> <td>    2.6073</td> <td>    0.896</td> <td>    2.910</td> <td> 0.004</td> <td>    0.846     4.369</td>
</tr>
</table>

<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.272</td> <th>  Durbin-Watson:     </th> <td>   1.565</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.530</td> <th>  Jarque-Bera (JB):  </th> <td>   1.369</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.114</td> <th>  Prob(JB):          </th> <td>   0.504</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.826</td> <th>  Cond. No.          </th> <td>    177.</td>
</tr>
</table>

<p>Because the default order for categorical variables is their numeric values, glm omits the third category. On the other hand, the analysis we showed in previous section omitted the second category, the parameter estimates will not be the same. You can compare the results from below with the results above and see that the parameter estimates are not the same.  Because group 3 is dropped, that is the reference category and all comparisons are made with group 3.</p>
<p>These analyses showed that the relationship between some_col and api00 varied, depending on the level of mealcat.  In comparing group 1 with group 2, the coefficient for some_col was significantly different, but there was no difference in the coefficient for some_col in comparing groups 2 and 3.</p>
<h2>3.9 Summary</h2>
<p>This chapter covered some techniques for analyzing data with categorical variables, especially, manually constructing indicator variables and using the categorical regression formula. Each method has its advantages and disadvantages, as described below.</p>
<p>Manually constructing indicator variables can be very tedious and even error prone. For very simple models, it is not very difficult to create your own indicator variables, but if you have categorical variables with many levels and/or interactions of categorical variables, it can be laborious to manually create indicator variables. However, the advantage is that you can have quite a bit of control over how the variables are created and the terms that are entered into the model.</p>
<p>The <code>C</code> formula approach eliminates the need to create indicator variables making it easy to include variables that have lots of categories, and making it easy to create interactions by allowing you to include terms like some_col * mealcat. It can be easier to perform tests of simple main effects. However, this is not very flexible in letting you choose which category is the omitted category.</p>
<p>As you will see in the next chapter, the regression command includes additional options like the robust option and the cluster option that allow you to perform analyses when you don't exactly meet the assumptions of ordinary least squares regression.  </p>
<h2>3.10 For more information</h2>
<ol>
<li><a href="http://songhuiming.github.io/pages/2016/11/27/linear-regression-in-python-chapter-1/">linear regression in python, Chapter 1</a></li>
<li><a href="http://songhuiming.github.io/pages/2016/12/31/linear-regression-in-python-chapter-2/">linear regression in python, Chapter 2</a></li>
</ol>
<h2>Reference</h2>
<ol>
<li><a href="http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter3/sasreg3.htm">Chapter 3 - Regression with Categorical Predictors</a></li>
</ol>
<div class="highlight"><pre>
</pre></div></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2017-01-21T00:00:00-06:00" pubdate>Sat 21 January 2017</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/data-minging.html">data minging</a>,    <a class="category" href="/tag/statsmodels.html">statsmodels</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2017/05/13/gradient-descent-in-solving-linear-regression-and-logistic-regression/">Gradient Descent in solving linear regression and logistic regression</a>
      </li>
      <li class="post">
          <a href="/pages/2017/05/06/tensorflowjian-jie-05-multivariate-regression-with-stochastic-gradient-descent/">Tensorflow--05: Multivariate Regression with Stochastic Gradient Descent</a>
      </li>
      <li class="post">
          <a href="/pages/2017/05/04/test-math-formula-in-github-pages/">test math formula in github pages</a>
      </li>
      <li class="post">
          <a href="/pages/2017/04/21/numpy-introduction-02/">Numpy Introduction 02</a>
      </li>
      <li class="post">
          <a href="/pages/2017/04/16/convolve-correlate-and-image-process-in-numpy/">convolve, correlate and image process in numpy</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2017  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2017/01/21/linear-regression-in-python-chapter-3-regression-with-categorical-predictors/';
    var disqus_url = '/pages/2017/01/21/linear-regression-in-python-chapter-3-regression-with-categorical-predictors/';
    var disqus_title = 'linear regression in python, Chapter 3 - Regression with Categorical Predictors';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>