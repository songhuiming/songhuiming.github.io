<!DOCTYPE html>
<html lang="en">
<head>
          <title>pydata</title>
        <meta charset="utf-8" />
        <link href="/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="pydata Full Atom Feed" />
        <link href="/feeds/python.atom.xml" type="application/atom+xml" rel="alternate" title="pydata Categories Atom Feed" />



    <meta name="tags" content="python" />
    <meta name="tags" content="data minging" />
    <meta name="tags" content="statsmodels" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="/">pydata <strong>Keep Looking, Don't Settle</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="/functions/archives.html">Archives</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="/pages/2016/11/27/linear-regression-in-python-chapter-1/" rel="bookmark"
         title="Permalink to linear regression in python, Chapter 1">linear regression in python, Chapter 1</a></h2>
 
  </header>
  <footer class="post-info">
    <abbr class="published" title="2016-11-27T00:00:00-06:00">
      Sun 27 November 2016
    </abbr>
    <abbr class="modified" title="2016-11-27T00:00:00-06:00">
      Sun 27 November 2016
    </abbr>
    <address class="vcard author">
      By           <a class="url fn" href="/author/huiming-song.html">Huiming Song</a>
    </address>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <h2>1 - Simple and Multiple Regression</h2>
<h3>Outline</h3>
<div class="highlight"><pre>1.0 Introduction
1.1 A First Regression Analysis
1.2 Multiple regression
1.3 Data Analysis / Examining Data 
1.4 Summary
1.5 For more information / Reference
</pre></div>


<h3>1.0 Introduction</h3>
<p>This first Chapter will cover topics in simple and multiple regression, as well as the supporting tasks that are important in preparing to analyze your data, e.g., data checking, getting familiar with your data file, and examining the distribution of your variables.  We will illustrate the basics of simple and multiple regression and demonstrate the importance of inspecting, checking and verifying your data before accepting the results of your analysis. </p>
<p>In this chapter, and in subsequent chapters, I will be using a data file called elemapi from the California Department of Education's API 2000 dataset.  This data file contains a measure of school academic performance as well as other attributes of the elementary schools, such as, class size, enrollment, poverty, etc.</p>
<h3>1.1 A First Regression Analysis</h3>
<p>Let's dive right in and perform a regression analysis using the variables api00, acs_k3, meals and full. These measure the academic performance of the school (api00), the average class size in kindergarten through 3rd grade (acs_k3), the percentage of students receiving free meals (meals) - which is an indicator of poverty, and the percentage of teachers who have full teaching credentials (full). We expect that better academic performance would be associated with lower class size, fewer students receiving free meals, and a higher percentage of teachers having full teaching credentials.  </p>
<p>First I will import some of the necessary modules in python. Typically I will use <code>statsmodels</code> here whose result is the same as <code>lm</code> in R. <code>sklearn</code> is very popular in data mining considering speed but its result will be different from R or SAS. </p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">itertools</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span><span class="p">,</span> <span class="n">combinations</span>
<span class="kn">import</span> <span class="nn">statsmodels.formula.api</span> <span class="kn">as</span> <span class="nn">smf</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">scipystats</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="kn">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">statsmodels.stats.stattools</span> <span class="kn">as</span> <span class="nn">stools</span>
<span class="kn">import</span> <span class="nn">statsmodels.stats</span> <span class="kn">as</span> <span class="nn">stats</span> 
<span class="kn">from</span> <span class="nn">statsmodels.graphics.regressionplots</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span> 
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>

<span class="n">mypath</span> <span class="o">=</span>  <span class="s1">r&#39;D:\Dropbox\ipynb_download\ipynb_blog</span><span class="se">\\</span><span class="s1">&#39;</span>

<span class="n">elemapi</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">mypath</span> <span class="o">+</span> <span class="s1">r&#39;elemapi.pkl&#39;</span><span class="p">)</span>   

<span class="k">print</span> <span class="n">elemapi</span><span class="p">[[</span><span class="s1">&#39;api00&#39;</span><span class="p">,</span> <span class="s1">&#39;acs_k3&#39;</span><span class="p">,</span> <span class="s1">&#39;meals&#39;</span><span class="p">,</span>  <span class="s1">&#39;full&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>   api00  acs_k3  meals  full
0    693    16.0   67.0  76.0
1    570    15.0   92.0  79.0
2    546    17.0   97.0  68.0
3    571    20.0   90.0  87.0
4    478    18.0   89.0  87.0
</pre></div>


<p>There are different way to run linear regression in statsmodels. One is using formula as R did. Another one is input the data directly. I will show them both here. And both of them will give the same result.</p>
<p>Personally I prefer the first way, the reasons are:</p>
<ol>
<li>
<p>If we use the second way(<code>sm.OLS</code>) to run the model, by default, it does not include intercept part in the model. To include intercept, we need to manually add constant to the data by <code>sm.add_constant</code>.</p>
</li>
<li>
<p>the first way will take care of the missing data while the second will not.</p>
</li>
</ol>
<div class="highlight"><pre><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">1. using formula as R did</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">print</span> <span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">40</span> <span class="o">+</span> <span class="s1">&#39; smf.ols in R formula &#39;</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">40</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;api00 ~ ell&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">ell</span><span class="p">,</span> <span class="n">elemapi</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">ell</span><span class="p">,</span> <span class="n">lm</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">lm</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">elemapi</span><span class="o">.</span><span class="n">ell</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;ell&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;api00&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear Regression Plot&quot;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>---------------------------------------- smf.ols in R formula ----------------------------------------

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.604
Model:                            OLS   Adj. R-squared:                  0.603
Method:                 Least Squares   F-statistic:                     582.3
Date:                Tue, 27 Dec 2016   Prob (F-statistic):           8.06e-79
Time:                        15:10:45   Log-Likelihood:                -2272.4
No. Observations:                 384   AIC:                             4549.
Df Residuals:                     382   BIC:                             4557.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    790.1777      7.428    106.375      0.000       775.572   804.783
ell           -4.4213      0.183    -24.132      0.000        -4.782    -4.061
==============================================================================
Omnibus:                        7.163   Durbin-Watson:                   1.505
Prob(Omnibus):                  0.028   Jarque-Bera (JB):                7.192
Skew:                          -0.308   Prob(JB):                       0.0274
Kurtosis:                       2.737   Cond. No.                         65.5
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.





&lt;matplotlib.text.Text at 0x25792668&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_1_01.png" /></p>
<div class="highlight"><pre><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">2. using data input directly</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="k">print</span> <span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">40</span> <span class="o">+</span> <span class="s1">&#39; sm.OLS with direct input data &#39;</span> <span class="o">+</span> <span class="s1">&#39;-&#39;</span><span class="o">*</span><span class="mi">40</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span>
<span class="n">lm2</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">elemapi</span><span class="p">[</span><span class="s1">&#39;api00&#39;</span><span class="p">],</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">elemapi</span><span class="p">[[</span><span class="s1">&#39;ell&#39;</span><span class="p">]]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>---------------------------------------- sm.OLS with direct input data ----------------------------------------

                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.604
Model:                            OLS   Adj. R-squared:                  0.603
Method:                 Least Squares   F-statistic:                     582.3
Date:                Tue, 20 Dec 2016   Prob (F-statistic):           8.06e-79
Time:                        21:13:43   Log-Likelihood:                -2272.4
No. Observations:                 384   AIC:                             4549.
Df Residuals:                     382   BIC:                             4557.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const        790.1777      7.428    106.375      0.000       775.572   804.783
ell           -4.4213      0.183    -24.132      0.000        -4.782    -4.061
==============================================================================
Omnibus:                        7.163   Durbin-Watson:                   1.505
Prob(Omnibus):                  0.028   Jarque-Bera (JB):                7.192
Skew:                          -0.308   Prob(JB):                       0.0274
Kurtosis:                       2.737   Cond. No.                         65.5
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<h3>1.2 Multiple variable regression</h3>
<p>Most of the time one variable in the model will not give us enough power to use. Usually multiple variables will be used in the model. As described about, let's try 3 variables in the model.</p>
<div class="highlight"><pre><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">1. using formula as R did</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">lm</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span> <span class="o">=</span> <span class="s1">&#39;api00 ~ acs_k3 + meals + full&#39;</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">elemapi</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.669
Model:                            OLS   Adj. R-squared:                  0.666
Method:                 Least Squares   F-statistic:                     198.7
Date:                Tue, 20 Dec 2016   Prob (F-statistic):           1.76e-70
Time:                        17:07:12   Log-Likelihood:                -1670.8
No. Observations:                 299   AIC:                             3350.
Df Residuals:                     295   BIC:                             3364.
Df Model:                           3                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
Intercept    908.5016     28.887     31.451      0.000       851.652   965.351
acs_k3        -2.7820      1.420     -1.959      0.051        -5.577     0.013
meals         -3.7020      0.160    -23.090      0.000        -4.018    -3.386
<span class="gh">full           0.1214      0.093      1.299      0.195        -0.063     0.305</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.986   Durbin-Watson:                   1.463
Prob(Omnibus):                  0.370   Jarque-Bera (JB):                1.971
Skew:                           0.141   Prob(JB):                        0.373
<span class="gh">Kurtosis:                       2.719   Cond. No.                         750.</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<p>however, if I rum <code>sm.OLS(elemapi['api00'], sm.add_constant(elemapi[['acs_k3', 'meals',  'full']])).fit()</code> I will get errors like "LinAlgError: SVD did not converge". The reason is there are missing data in 'acs_k3' and 'meals'. Thus results in the singular value decomposition of the hat matrix failed. To fix it, I need to drop any row with missing data first. After that, it will run successfully and the result is exactly the same as above.</p>
<div class="highlight"><pre><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">2. using data input directly</span>
<span class="sd">&#39;&#39;&#39;</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">elemapi</span><span class="p">[[</span><span class="s1">&#39;api00&#39;</span><span class="p">,</span> <span class="s1">&#39;acs_k3&#39;</span><span class="p">,</span> <span class="s1">&#39;meals&#39;</span><span class="p">,</span>  <span class="s1">&#39;full&#39;</span><span class="p">]]</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">how</span> <span class="o">=</span> <span class="s1">&#39;any&#39;</span><span class="p">)</span>

<span class="n">lm2</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">OLS</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s1">&#39;api00&#39;</span><span class="p">],</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">data</span><span class="p">[[</span><span class="s1">&#39;acs_k3&#39;</span><span class="p">,</span> <span class="s1">&#39;meals&#39;</span><span class="p">,</span>  <span class="s1">&#39;full&#39;</span><span class="p">]]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="k">print</span> <span class="n">lm2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  api00   R-squared:                       0.669
Model:                            OLS   Adj. R-squared:                  0.666
Method:                 Least Squares   F-statistic:                     198.7
Date:                Tue, 20 Dec 2016   Prob (F-statistic):           1.76e-70
Time:                        17:10:47   Log-Likelihood:                -1670.8
No. Observations:                 299   AIC:                             3350.
Df Residuals:                     295   BIC:                             3364.
Df Model:                           3                                         
<span class="gh">Covariance Type:            nonrobust                                         </span>
<span class="gh">==============================================================================</span>
                 coef    std err          t      P&gt;|t|      [95.0% Conf. Int.]
------------------------------------------------------------------------------
const        908.5016     28.887     31.451      0.000       851.652   965.351
acs_k3        -2.7820      1.420     -1.959      0.051        -5.577     0.013
meals         -3.7020      0.160    -23.090      0.000        -4.018    -3.386
<span class="gh">full           0.1214      0.093      1.299      0.195        -0.063     0.305</span>
<span class="gh">==============================================================================</span>
Omnibus:                        1.986   Durbin-Watson:                   1.463
Prob(Omnibus):                  0.370   Jarque-Bera (JB):                1.971
Skew:                           0.141   Prob(JB):                        0.373
<span class="gh">Kurtosis:                       2.719   Cond. No.                         750.</span>
<span class="gh">==============================================================================</span>

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div>


<h3>1.3 Data Analysis</h3>
<p>Generally, when we get the data we will do some analysis of the data. </p>
<p>For numeric data, we will usualy do:</p>
<ol>
<li>if there is any missing data</li>
<li>what is the distribution of the data, and its visualization like histogram and box-plot</li>
<li>what is the five numbers: min, 25 percentile, median, 75 percentile, and the max</li>
<li>mean, stdev, length</li>
<li>the correlation between the data</li>
<li>futuremore, is there any outliers in the data?</li>
<li>other plots: pairwise scatter plot, kernal density plot </li>
</ol>
<p>For categorical data, we will usually do:</p>
<ol>
<li>is there any missing data?</li>
<li>how many unique values of the data? what is their frequency?</li>
</ol>
<p>Next we will answer these questions.</p>
<p>we will pick 4 variables from the elemapi data as an example. </p>
<h4>1.3.1. The <code>df.describe()</code> will gives the summary information like how many non-missing values there, mean, stdev, and the five number summary.</h4>
<p>As we can see, api00 and full does not have missing values and their length is 384. acs_k3 has 382 non-missing values, so it has two missing data. meals has 301 non-missing values so it has 83 missings.</p>
<div class="highlight"><pre><span class="n">sample_data</span> <span class="o">=</span> <span class="n">elemapi</span><span class="p">[[</span><span class="s1">&#39;api00&#39;</span><span class="p">,</span> <span class="s1">&#39;acs_k3&#39;</span><span class="p">,</span> <span class="s1">&#39;meals&#39;</span><span class="p">,</span>  <span class="s1">&#39;full&#39;</span><span class="p">]]</span>
<span class="k">print</span> <span class="n">sample_data</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>            api00      acs_k3       meals        full
count  384.000000  382.000000  301.000000  384.000000
mean   649.421875   18.534031   71.890365   65.124271
std    143.020564    5.104436   24.367696   40.823987
min    369.000000  -21.000000    6.000000    0.420000
25%    525.000000         NaN         NaN    0.927500
50%    646.500000         NaN         NaN   87.000000
75%    763.500000         NaN         NaN   97.000000
max    940.000000   25.000000  100.000000  100.000000
</pre></div>


<h4>1.3.2. plot the histogram and the density plot</h4>
<p>From the hist plot, the data api00 is pretty even. There is no special concentration.</p>
<p>From the box plot below, we did not see there is any outliers.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">gaussian_kde</span>

<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">normed</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">facecolor</span> <span class="o">=</span> <span class="s1">&#39;g&#39;</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="c1"># add density plot</span>
<span class="n">density</span> <span class="o">=</span> <span class="n">gaussian_kde</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">api00</span><span class="p">)</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span>
<span class="n">density</span><span class="o">.</span><span class="n">covariance_factor</span> <span class="o">=</span> <span class="k">lambda</span> <span class="p">:</span> <span class="o">.</span><span class="mi">2</span>
<span class="n">density</span><span class="o">.</span><span class="n">_compute_covariance</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">density</span><span class="p">(</span><span class="n">xs</span><span class="p">),</span> <span class="n">color</span> <span class="o">=</span> <span class="s2">&quot;b&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_1_02.png" /></p>
<div class="highlight"><pre><span class="n">plt</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">api00</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;gD&#39;</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>{&#39;boxes&#39;: [&lt;matplotlib.lines.Line2D at 0x18adfb00&gt;],
 &#39;caps&#39;: [&lt;matplotlib.lines.Line2D at 0x18af37b8&gt;,
  &lt;matplotlib.lines.Line2D at 0x18af3d30&gt;],
 &#39;fliers&#39;: [&lt;matplotlib.lines.Line2D at 0x18aff860&gt;],
 &#39;means&#39;: [],
 &#39;medians&#39;: [&lt;matplotlib.lines.Line2D at 0x18aff2e8&gt;],
 &#39;whiskers&#39;: [&lt;matplotlib.lines.Line2D at 0x18adfc18&gt;,
  &lt;matplotlib.lines.Line2D at 0x18af3240&gt;]}
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_1_03.png" /></p>
<h4>1.3.3. The correlation of the data</h4>
<p><code>df.corr()</code> will give the correlation matrix of all the variables. Pick the corresponding row or column will give us the correlation between that variables with all the other variables.</p>
<div class="highlight"><pre><span class="c1"># check correlation between each variable and api00</span>
<span class="k">print</span> <span class="n">elemapi</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span><span class="o">.</span><span class="n">ix</span><span class="p">[</span><span class="s1">&#39;api00&#39;</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>mealcat    -0.865752
meals      -0.815576
ell        -0.777091
not_hsg    -0.681712
emer       -0.596273
yr_rnd     -0.487599
hsg        -0.361842
enroll     -0.329078
mobility   -0.191732
growth     -0.102593
acs_k3     -0.095982
dnum        0.007991
acs_46      0.223256
some_col    0.258125
snum        0.259491
full        0.424186
col_grad    0.529277
grad_sch    0.632367
avg_ed      0.793012
api99       0.985950
api00       1.000000
Name: api00, dtype: float64
</pre></div>


<h4>1.3.4. scatter plot of all the variables in the data</h4>
<p>This will scatter plot all the pairs of the data so that we can easily find their relations. If there is too many variables, this is not a good way because there are too many graphs to display. In that case, the correlation will be easier to check.</p>
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">elemapi</span><span class="p">[[</span><span class="s1">&#39;api00&#39;</span><span class="p">,</span> <span class="s1">&#39;acs_k3&#39;</span><span class="p">,</span> <span class="s1">&#39;meals&#39;</span><span class="p">,</span>  <span class="s1">&#39;full&#39;</span><span class="p">,</span> <span class="s1">&#39;ell&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">how</span> <span class="o">=</span> <span class="s1">&#39;any&#39;</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre>&lt;seaborn.axisgrid.PairGrid at 0x1d83e6d8&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_1_04.png" /></p>
<h4>1.3.5. keneral density plot</h4>
<p>R has a very friendly function <code>plot(density(data, bw=0.5))</code> to let us plot the kernal density function. Here is try to mimic that in python. From the plot we can observe that the ell variable skewed to the right.</p>
<div class="highlight"><pre><span class="n">sns</span><span class="o">.</span><span class="n">kdeplot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">elemapi</span><span class="o">.</span><span class="n">ell</span><span class="p">),</span> <span class="n">bw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>&lt;matplotlib.axes._subplots.AxesSubplot at 0x25336f98&gt;
</pre></div>


<p><img alt="png" src="/figures/ucla_ats_linreg_1_05.png" /></p>
<h4>1.3.6. categorical variable frequency</h4>
<p>For categorical variable, most of the time we care about if there is any missing data, and what is the frequency of the values. We can see for acs_k3, there are about 137 data points equal to 19.</p>
<div class="highlight"><pre><span class="n">elemapi</span><span class="o">.</span><span class="n">acs_k3</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">sort_index</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>-21.0      3
-20.0      2
-19.0      1
 14.0      2
 15.0      1
 16.0     14
 17.0     19
 18.0     59
 19.0    137
 20.0     94
 21.0     39
 22.0      7
 23.0      3
 25.0      1
NaN        2
Name: acs_k3, dtype: int64
</pre></div>


<h3>1.4 Summary</h3>
<p>Most of the time, when I get the data, I will do single variable analysis to check the distribution of my data and missing information. If data is not big, I will check the pairwise relations by pairwise scatter plot or the correlations.</p>
<p>If there is missing, how to do? Drop the data or consider of any way to impute the missingt data?</p>
<p>If the relation is not linear, what shall we do?</p>
<h3>1.5 For more information</h3>
<p>If you have any suggestions, please contact me.</p>
<p>Some references that I think may help:</p>
<ol>
<li>
<p><a href="https://www.amazon.com/Applied-Linear-Statistical-Models-Michael/dp/007310874X">Applied Linear Statistical Models</a>. I have a hard copy of this book which I bought in XJTU library. It only costs about two US Dollars</p>
</li>
<li>
<p><a href="http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter1/sasreg1.htm">UCLA ATS: regression with SAS</a>. Although it uses SAS, it gives very detailed introduction about linear models. I will follow the structure of this web book.</p>
</li>
<li>
<p><a href="http://statsmodels.sourceforge.net/">statsmodels</a> A python package to run statistical models and tests.</p>
</li>
</ol>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>