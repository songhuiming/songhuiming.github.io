<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>DeepSeek V3 learning notes &mdash; pydata: my learning notes</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><header>
  <h1><a href="/">pydata: my learning notes</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</header>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  // 允许 $...$ 和 \( ... \) 作为行内公式
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  // 允许 $$...$$ 和 \[ ... \] 作为块级公式
    processEscapes: true,  // 允许在公式中使用转义符，如 \$ 表示美元符号
    processEnvironments: true  // 允许解析 \begin{equation} ... \end{equation} 等数学环境
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],  // 跳过某些 HTML 标签，防止错误解析
    renderActions: {
      addMenu: []  // 移除右键菜单
    }
  }
};

window.addEventListener('load', () => {
  document.querySelectorAll("mjx-container").forEach(x => {
    x.parentElement.classList.add('has-jax');  // 使用 classList.add() 避免字符串拼接错误
  });
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></header>
  <nav role="navigation">

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">DeepSeek V3 learning notes</h1>
    <p class="meta">
<time datetime="2025-02-23T00:00:00-06:00" pubdate>Sun 23 February 2025</time>    </p>
</header>

  <div class="entry-content"><h2>1. What the problem to solve?</h2>
<p>When I went back for the Spring Festival, DeepSeek released a new model. For a while, all kinds of media discussed it a lot, almost rising to the height of national destiny. The most important points discussed should be two: the first is the benchmark score comparable to other mainstream models; the second is that the training speed is much faster and uses much less GPU time. However, most of the discussions are prosperous discussions and exciting news, but there is no specific reason why DS is good and how it is good. The first weekend after I came back, I spent the weekend reading the DS V3 paper. The reason why I chose V3 is that R1 is RL and SFT to enhance the reasoning ability of the model. The basic model is still V3, and the DS V3 paper is more detailed. The second part of the article describes what improvements DS has made in the model architecture (science), and the third part describes what improvements have been made in the model training (engineering). Then the two parts talk about pretraining, posttraining and evaluation related things.</p>
<h2>2. How to solve?</h2>
<h3>2.1. Basic Architecture</h3>
<p>There are two main points in the basic architecture of the model: 1. The original Attention is changed to Multi-head Latent Attention; 2. The original FFN is changed to DeepSeekMoE. The purpose of doing this is to reduce the amount of calculation and reduce the cache data, thereby saving GPU memory. The details are as follows:</p>
<p>DeekSeek Model Architecture
<img alt="20250216_01_deepseek_v3_architecture.png" src="/figures/20250216_01_deepseek_v3_architecture.png"></p>
<h4>2.1.1. Multi-head latent attention (MLA)</h4>
<p>Compared with the attention mechanism introduced in the original <a href="https://arxiv.org/abs/1706.03762">Attention</a> model, DS uses the Multi-head latent attention (MLA) architecture. The difference from the original Multi-head attention is that when calculating <span class="math">\(k, q, v\)</span> from <span class="math">\(h_t\)</span>, it first reduces the dimension and projects <span class="math">\(h_t \in \mathbb{R}^d\)</span> to a low-dimensional space <span class="math">\(\textcolor{blue}{c_t^{KV} = W^{DKV} h_t}\)</span>, and then upgrades the dimension to <span class="math">\(\textcolor{blue}{k_t^C=W^{UK}c_t^{KV}}\)</span>; and projects it to a low-dimensional <span class="math">\(\textcolor{blue}{c_t^Q=W^{DQ}h_t}\)</span>, and then upgrades the dimension to <span class="math">\(\textcolor{blue}{q_t^C = W^{UQ} c_t^Q}\)</span>. In this case, when performing inference, we only need to save low-dimensional <span class="math">\(\textcolor{blue}{c_t^{KV}}\)</span> and <span class="math">\(\textcolor{blue}{c_t^Q}\)</span>. For example, the original <span class="math">\(\text{dim}(h_t) = 7168\)</span> is reduced to 4096 dimensions through a dimensionality reduction matrix <span class="math">\(\text{dim}(W^{DKV})=4096 \times 7186\)</span>, so the matrix <span class="math">\(\textcolor{blue}{c_t^{KV}}\)</span> that needs to be saved is much smaller. Specifically,</p>
<div class="math">$$
\begin{aligned}
&amp;  \textcolor{blue}{c_t^{KV}} = W^{DKV} h_t, \\
&amp;  [k_{t,1}^{C}, k_{t,2}^{C}, ..., k_{t,n_h}^{C}] = k_t^C = W^{UK} c_t^{KV}, \\
&amp;  \textcolor{blue}{k_t^R} = \text{RoPE}(W^{KR} h_t), \\
&amp;  k_{t,i} = \left[k_{t,i}^{C}; k_t^R \right], \\
&amp;  [v_{t,1}^{C}, v_{t,2}^{C}, ..., v_{t,n_h}^{C}] = v_t^C = W^{UV} c_t^{KV},
\end{aligned}
 $$</div>
<p>Here <span class="math">\( W^{DKV} \in \mathbb{R}^{d_c \times d} $ is a dimension reduction projection matrix; $W^{UK}, W^{UV} \in \mathbb{R}^{d_h n_h \times d_c}\)</span> are the corresponding dimension increase matrices of keys and values. When doing inference, only <span class="math">\(\textcolor{blue}{c_t^{KV}}\)</span> and <span class="math">\(\textcolor{blue}{k_t^R}\)</span> are needed, which saves both computation and memory.</p>
<p>For query, the same process is used to reduce the dimension</p>
<div class="math">$$
\begin{aligned}
&amp; \textcolor{blue}{c_t^Q} = W^{DQ} h_t, \\
&amp; [q_{t,1}^{C}, q_{t,2}^{C}, ..., q_{t,n_h}^{C}] = q_t^C = W^{UQ} c_t^Q, \\
&amp; [q_{t,1}^{R}, q_{t,2}^{R}, ..., q_{t,n_h}^{R}] = q_t^R = \text{RoPE}(W^{QR} c_t^Q), \\
&amp; q_{t,i} = [q_{t,i}^{C}, q_{t,i}^{R}],
\end{aligned}
$$</div>
<h4>2.1.2. DeepSeekMoE with Auxiliary-Loss-Free Load Balancing</h4>
<h5>Basic Architecture of DeepSeekMoE</h5>
<p>In the 2017 Attention paper, the decoder first calculates Attention and then calculates FFN to predict the final token. In DS, FFN is further changed to MoE. MoE means that the model is composed of a series of Experts. Each Expert can be an MLP, a CNN or other neural network model, or each Expert can be a tansformer encoder/decoder. Each Expert is responsible for learning different data patterns. MoE selects the appropriate Expert through the Gating Network so that different data is sent to the best Expert for processing. When predicting the next token, MoE does not use a certain Expert or all Experts, but selects the Top <span class="math">\(K\)</span> Experts to make predictions. Which <span class="math">\(K\)</span> Experts are selected is determined by a Gating network. Usually <span class="math">\(K \ll N\)</span>, so MoE is a sparse network. The first advantage of using MoE is that through more refined divisions, the model has greater flexibility, and different Experts can learn different data. For example, if <span class="math">\(N=16\)</span>, <span class="math">\(K=2\)</span>, then there are <span class="math">\(C(16, 2)=160\)</span> combinations. If each Expert is split into 4 small Experts, the total number of combinations is <span class="math">\(C(64, 8)=4,426,165,368\)</span>. This greatly increases the flexibility of the model. The second benefit is saving computing resources, because only a few Experts need to be activated each time to participate in the calculation, unlike the transformer where all parameters participate in the calculation. In other words, MoE can train a larger model with more parameters, thereby remembering more knowledge, but only a small number of parameters actually participate in the calculation each time. And because each Expert is an independent model with no shared parameters, they can perform calculations in parallel.</p>
<p>MoE Architecture
<img alt="20250216_03_deepseek_v2_moe.png" src="/figures/20250216_03_deepseek_v2_moe.png"></p>
<p>In DS's FFN, DeepSeekMoE goes a step further on the basis of MOE: 1) They further refine the Expert, and then some Experts are used for routing. Specifically, for the routingd Expert, the prediction is only routed to some of these Experts. At the same time, they also have some Experts that are shared Experts, that is, they are used every time when predicting. 2) DS optimizes the selection of Experts and uses affinity scores to select experts. 3) DS's Experts are finer and sparser. This further reduces the computing cost.</p>
<p>DeepSeekMoE Architecture
<img alt="20250216_02_original_moe.png" src="/figures/20250216_02_original_moe.png"></p>
<p>Assuming that the center of each Expert <span class="math">\(i\)</span> is <span class="math">\(\mathbf{e}_{i}\)</span>, first calculate <span class="math">\(\textcolor{blue}{s_{i,t} = \sigma(\mathbf{u}_t^\top \mathbf{e}_i)}\)</span> as the similarity between token <span class="math">\(t\)</span> and Expert <span class="math">\(i\)</span>: <span class="math">\(\mathbf{e}_i\)</span> is used as the center vector of Expert <span class="math">\(i\)</span>. The dot product is performed with the input <span class="math">\(\mathbf{u}_t\)</span> of the token, and then the affinity of the token to the Expert is calculated through sigmoid, thereby determining whether the token is routed to the Expert.</p>
<p><span class="math">\(g^{\prime}_{i,t}\)</span> is the gating value of Expert <span class="math">\(i\)</span>. Based on the value of <span class="math">\(s_{i,t}\)</span> of all Experts, the highest <span class="math">\(K_r\)</span> ones are selected.</p>
<div class="math">$$
    g'_{i,t} =
    \begin{cases} 
        s_{i,t}, &amp; s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r), \\
        0, &amp; \text{otherwise},
    \end{cases}
$$</div>
<p>Then calculate the weights of these <span class="math">\(K_r\)</span> Experts <span class="math">\(\textcolor{blue}{g_{i,t} = \frac{g^{\prime}_{i,t}}{\sum_{j=1}^{N_r} g'_{j,t}}}\)</span>. Although the formula here shows that the denominator is <span class="math">\(N_r\)</span> <span class="math">\(g'_{i,t}\)</span> added together, in fact, <span class="math">\(N_r - K_r\)</span> are 0.</p>
<p>Finally, the new output is <span class="math">\(\textcolor{blue}{\mathbf{h}'_t = \mathbf{u}_t + \sum_{i=1}^{N_s} \text{FFN}^{(s)}_i(\mathbf{u}_t) + \sum_{i=1}^{N_r} g_{i,t} \text{FFN}^{(r)}_i(\mathbf{u}_t)}\)</span>. Note that <span class="math">\(K_r\)</span> are selected from <span class="math">\(N_r\)</span> Experts for calculation, <span class="math">\(K_r \ll N_r\)</span>, thus saving a lot of calculation. At the same time, compared with the general MoE, DS uses <span class="math">\(N_s\)</span> shared Experts, which participate in the calculation of all tokens. By introducing shared experts, the model can share common knowledge between different tasks, avoiding each expert from learning similar content independently, thereby reducing knowledge redundancy.</p>
<h5>Auxiliary-Loss-Free Load Balancing in DeepSeek-V3</h5>
<p>In actual operation, this situation may occur: most of the tokens of the model are predicted by a few experts, and the remaining experts are less used, resulting in unbalanced expert load. The previous MOE used auxiliary loss to balance the expert load. The purpose of balanced load is achieved by punishing imbalanced expert utilization (adding more loss). However, there are two problems with this: 1) Too strong auxiliary loss hurts model performance by interfering with learning objectives. 2) Difficult to tune: Balancing between efficiency and accuracy requires careful hyperparameter tuning.</p>
<p>DS proposed Auxiliary-Loss-Free Load Balancing to achieve balanced routing of tokens to different experts. Specifically, a bias item is added to the gating function to balance the call of the expert through the bias value.</p>
<p>Specifically, when calculating the weight <span class="math">\(g'_{i,t}\)</span>, a bias <span class="math">\(b_i\)</span> is added to the affinity score of each token and Expert, becoming <span class="math">\(s_{i,t} + b_i \in \text{Topk}(\{s_{j,t} + b_j | 1 \leq j \leq N_r\}, K_r)\)</span>. When an Expert <span class="math">\(j\)</span> is overloaded, the corresponding <span class="math">\(b_j\)</span> is reduced by <span class="math">\(\gamma\)</span>. In this way, the new <span class="math">\(s_{j,t} + b_j\)</span> will be reduced a little, so that when <span class="math">\(\text{Topk}\)</span> are selected, this Expert will be ranked a little lower and is less likely to be selected because <span class="math">\(g'_{i,t} = 0\)</span>.</p>
<h5>Complementary Sequence-Wise Auxiliary Loss</h5>
<p>DeepSeek-V3 mainly relies on the Auxiliary-Loss-Free load balancing strategy to ensure load balance among Experts. However, in order to prevent extreme imbalance within a single sequence, DeepSeek-V3 also introduces Complementary Sequence-Wise Auxiliary Loss. Usually, the load balancing of the MoE model is performed at the batch level, but for multiple sequences in a batch, some experts in a sequence may be used a lot, while other experts are used very little. The idea of DS is to make Expert utilization as balanced as possible at each sequence level, and the load will not be too concentrated or sparse. At the same time, the intensity of this loss is very low to avoid negative impact on model performance.</p>
<p>Below we mainly understand the formula in the original text and why this formula can balance the Expert load at the sequence level.</p>
<div class="math">$$
\begin{aligned}
&amp; \mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i, \\
&amp; f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right), \\
&amp; s'_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}, \\
&amp; P_i = \frac{1}{T} \sum_{t=1}^{T} s'_{i,t}
\end{aligned}
$$</div>
<p><span class="math">\(P_i\)</span> is used to see whether the use of Expert in the sequence is relatively balanced. If it is not balanced, the value of <span class="math">\(P_i\)</span> will be relatively large. The specific reason is: first look at <span class="math">\(s^{\prime}_{i,t}\)</span> and <span class="math">\(P_i\)</span>. For the normalized affinity score <span class="math">\(s^{\prime}_{i,t} = \frac{s_{i,t}}{\sum_{j=1}^{N_r} s_{j,t}}\)</span>, calculate the average score <span class="math">\(P_i = \frac{1}{T} \sum_{t=1}^{T} s^{\prime}_{i,t}\)</span> of Expert <span class="math">\(i\)</span> in the entire sequence (the length of the sequence is <span class="math">\(T\)</span>): If Expert <span class="math">\(i\)</span> is selected on many tokens <span class="math">\(t\)</span> in the sequence, then the corresponding <span class="math">\(s_{i,t}\)</span> will be larger than (<span class="math">\(s_{i,t} \gg s_{j,t}\)</span> for <span class="math">\(j \neq i\)</span>), and <span class="math">\(s^{\prime}_{i,t}\)</span> is <span class="math">\(s_{i,t}\)</span> for all The normalized value of Expert, then <span class="math">\(s^{\prime}_{i,t}\)</span> is relatively large. And <span class="math">\(P_i\)</span> is the average value of <span class="math">\(s^{\prime}_{i,t}\)</span> over the entire sequence, so it will also be relatively large. Finally, the corresponding loss function <span class="math">\(\mathcal{L}_{\text{Bal}} = \alpha \sum_{i=1}^{N_r} f_i P_i\)</span> is relatively large.</p>
<p><span class="math">\(f_i\)</span> is the normalized value of the frequency with which Expert <span class="math">\(i\)</span> is used. The formula <span class="math">\(f_i = \frac{N_r}{K_r T} \sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)</span> counts the frequency of each Expert <span class="math">\(i\)</span> being used in the current sequence <span class="math">\(T\)</span> tokens (<span class="math">\(\sum_{t=1}^{T} \mathbb{1} \left(s_{i,t} \in \text{Topk}(\{s_{j,t} | 1 \leq j \leq N_r\}, K_r) \right)\)</span>, and then normalizes it <span class="math">\(\frac{N_r}{K_r T}\)</span>. Similarly, if Expert <span class="math">\(i\)</span> is called many times in this sequence, then this value will be larger.</p>
<p>Finally, <span class="math">\(\alpha\)</span> in the loss function formula is a hyperparameter with a very small value. Its value is small in order not to overly affect the main training objective.</p>
<h3>2.2. Multi-Token Prediction (MTP)</h3>
<p>DS also uses MTP, instead of just predicting the next token like general pre-training (the 2017 Attention article only predicts the next token). This has two advantages: 1. Predicting multiple tokens at the same time can improve the efficiency of data use. 2. Allow the model to think more long-term when preparing representation. Unlike the original MTP article, DS does not predict <span class="math">\(D\)</span> tokens at the same time (outputting a <span class="math">\(D \times V\)</span> matrix, where <span class="math">\(V\)</span> is the vocabulary size); in other words, the corresponding prediction and loss function is <span class="math">\(L_n = -\sum_t \log P_{\theta} (x_{t+n:t+1} \mid x_{t:1})\)</span>. DS sequentially predicts additional token.</p>
<p>MTP from Gloeckle et al. (2024), which is  <span class="math">\(D \times V\)</span> matrix
<img alt="20250216_04_deepseek_orig_MTP.png" src="/figures/20250216_04_deepseek_orig_MTP.png"></p>
<p>The MTP module structure of DS can be simply summarized in one sentence: DS's MTP uses <span class="math">\(D\)</span> sequential modules to predict <span class="math">\(D\)</span> tokens. These <span class="math">\(D\)</span> modules share embedding <span class="math">\(\text{Emb}(\cdot)\)</span> and output head <span class="math">\(\text{OutHead}(\cdot)\)</span>. The MTP module of the <span class="math">\(k\)</span>th token has a dedicated transformer block <span class="math">\(\text{TRM}_k(\cdot)\)</span> and a projection matrix <span class="math">\(M_k \in \mathbb{R}^{d \times 2d}\)</span>.</p>
<p>DeepSeek V3 MTP: Use D MTP module and <span class="math">\(k\)</span>-th module predict <span class="math">\(k\)</span>-depth token
<img alt="20250216_05_deepseek_v3_MTP.png" src="/figures/20250216_05_deepseek_v3_MTP.png"></p>
<p><strong>prediction</strong>：For the <span class="math">\(i\)</span>th token <span class="math">\(t_i\)</span> of the input sequence, at the <span class="math">\(k\)</span>th prediction depth (that is, predict <span class="math">\(t_{i+k+1}\)</span> below), the MTP module: 1) First get the representation <span class="math">\(\mathbf{h}_i^{k-1} \in \mathbb{R}^{d}\)</span> of the <span class="math">\(i\)</span>-th token at the <span class="math">\((k-1)\)</span>-th depth, and the embediding <span class="math">\(Emb(t_{i+k}) \in \mathbb{R}^{d}\)</span> of the <span class="math">\((i+k)\)</span>-th token; 2) Then concat them (length is <span class="math">\(2d\)</span>), and then generate a new representation
<span class="math">\(h’^k_i = M_k [\text{RMSNorm}(\mathbf{h}_i^{k-1}); \text{RMSNorm}(Emb(t_{i+k}))]\)</span> through RMSNorm and the projection matrix <span class="math">\(M_k \in \mathbb{R}^{d \times 2d}\)</span>; 3) Then <span class="math">\(\mathbf{h}’^k_i\)</span> is taken as input to the <span class="math">\(k\)</span>-th transformer module <span class="math">\(\text{TRM}_k(\cdot)\)</span> to generate the output representation of the current depth <span class="math">\(\mathbf{h}^k_{1:T-k} = TRM_k(\mathbf{h}’^{k}_{1:T-k})\)</span>; finally, <span class="math">\(\text{OutHead}\)</span> is used to convert <span class="math">\(\mathbf{h}^k_i\)</span> into predicted probability <span class="math">\(p_{i+k+1} = OutHead(\mathbf{h}^k_i) \in \mathbb{R}^{V}\)</span>.</p>
<p>My understanding with the example step by step:</p>
<ol>
<li>
<p>Traditional next token prediction: <span class="math">\(t_1\)</span>-&gt; <span class="math">\(h_1\)</span> -&gt; <span class="math">\(\hat{t}_2\)</span>, <span class="math">\(t_2\)</span> -&gt; ​​<span class="math">\(h_2\)</span> -&gt; ​​<span class="math">\(\hat{t}_3\)</span>, .... This is what the main model does.</p>
</li>
<li>
<p>In the <span class="math">\(k\)</span>-th MTP module of DS, the model will get the <span class="math">\(k\)</span>-depth representation <span class="math">\(\mathbf{h}_i^k\)</span> based on the <span class="math">\((k-1)\)</span>-depth MTP representation <span class="math">\(h_i^{k-1}\)</span> and the embedding of token <span class="math">\(t_{i+k}\)</span>, and then further predict the <span class="math">\((k+1)\)</span>-th token. Below, we use <span class="math">\(k=1, 2, 3\)</span> as an example to illustrate how it works. Specifically, for input token <span class="math">\(i\)</span>:</p>
<ul>
<li>
<p>When <span class="math">\(k=1\)</span>, first concatenate the representation <span class="math">\(h_i^{k-1}=h_i^0\)</span> (from main model) of <span class="math">\(t_i\)</span> and the embedding of token <span class="math">\(t_{i+1}\)</span> (<span class="math">\(i+k=i+1\)</span>) to get <span class="math">\(h’^1_i\)</span>, then input <span class="math">\(TRM_1\)</span>, get <span class="math">\(\mathbf{h}^1_{1:T-1} = TRM_1(\mathbf{h}’^{1}_{1:T-1})\)</span>, and finally get the prediction distribution <span class="math">\(p_{i+2}\)</span> of token <span class="math">\(t_{i+2}\)</span></p>
</li>
<li>
<p>When <span class="math">\(k=2\)</span>, first concatenate the representation <span class="math">\(h_i^{k-1}=h_i^1\)</span> (from <span class="math">\(k=1\)</span> MTP module) of <span class="math">\(t_i\)</span> and the embedding of token <span class="math">\(t_{i+1}\)</span> (<span class="math">\(i+k=i+1\)</span>) to get <span class="math">\(h’^1_i\)</span>. Then input <span class="math">\(TRM_1\)</span>, get <span class="math">\(\mathbf{h}^1_{1:T-1} = TRM_1(\mathbf{h}’^{1}_{1:T-1})\)</span>, and finally get the prediction distribution <span class="math">\(p_{i+2}\)</span> of token <span class="math">\(t_{i+2}\)</span>. The embeddings of <span class="math">\(t_{i+2}\)</span> ( <span class="math">\(i+k=i+2\)</span> ) are concatenated to get <span class="math">\(h’^2_i\)</span>, and then input into <span class="math">\(TRM_2\)</span>, to get <span class="math">\(\mathbf{h}^2_{1:T-2} = TRM_2(\mathbf{h}’^{2}_{1:T-2})\)</span>, and finally get the prediction distribution <span class="math">\(p_{i+3}\)</span> of token <span class="math">\(t_{i+3}\)</span></p>
</li>
<li>
<p>When <span class="math">\(k=3\)</span>, first concatenate the representation <span class="math">\(h_i^{k-1}=h_i^2\)</span> (from <span class="math">\(k=2\)</span> MTP module) of <span class="math">\(t_{i+3}\)</span> and the embeddings of token <span class="math">\(t_{i+3}\)</span> ( <span class="math">\(i+k=i+3\)</span> ) to get <span class="math">\(h’^3_i\)</span>, and then input into <span class="math">\(\text{TRM}_3\)</span>, to get <span class="math">\(\mathbf{h}^3_{1:T-3} = TRM_3(\mathbf{h}’^{3}_{1:T-3})\)</span>, and finally get the prediction distribution <span class="math">\(p_{i+4}\)</span> of token <span class="math">\(t_{i+4}\)</span></p>
</li>
</ul>
</li>
</ol>
<p><strong>Objective function of MTP</strong> For each predicted depth k, its loss function is still cross-entropy, defined as <span class="math">\(\mathcal{L}_{\text{MTP}}^k\)</span> = <span class="math">\(\mathcal{L}_{\text{MTP}}^k\)</span> = <span class="math">\(\text{CrossEntropy}(P_{2+k:T+1}^{k}, t_{2+k:T+1})\)</span> = <span class="math">\(-\frac{1}{T} \sum_{i=2+k}^{T+1} \log p_i^k[t_i]\)</span>. Where <span class="math">\(T\)</span> is the length of the input sequence, <span class="math">\(t_i\)</span> is the <span class="math">\(i\)</span>-th true value of the input sequence, and <span class="math">\(p_i^k[t_i]\)</span> is the prediciton probability of the <span class="math">\(k\)</span>-th MTP module for <span class="math">\(t_i\)</span>.</p>
<p>The final MTP loss function is the average of all predicted depths (<span class="math">\(k\)</span> from 1 to <span class="math">\(D\)</span>) multiplied by the weight <span class="math">\(\lambda\)</span>, <span class="math">\(\mathcal{L}_{\text{MTP}}\)</span> = <span class="math">\(\frac{\lambda}{D} \sum_{k=1}^{D} \mathcal{L}_{\text{MTP}}^k\)</span></p>
<p>From DS's paper formula 25, <span class="math">\(\mathcal{L}_{\text{MTP}}\)</span> is only the loss function of MTP. The final loss function should also add <span class="math">\(\mathcal{L}_{\text{Main}}\)</span>.</p>
<p><strong>How to use MTP during inference?</strong> The main purpose of MTP is to improve the performance of the main model. During inference, MTP is not used, but only the main model is used to predict the next token.</p>
<h3>stay tuned!</h3>
<h3>Reference</h3>
<ol>
<li><a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf">DeepSeek-V3 Technical Report</a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2025-02-23T00:00:00-06:00" pubdate>Sun 23 February 2025</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/ai.html">AI</a>,    <a class="category" href="/tag/llm.html">LLM</a>,    <a class="category" href="/tag/aig.html">AIG</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/02/23/deepseek-v3-learning-notes/">DeepSeek V3 learning notes</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
      <li class="post">
          <a href="/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">GPT-1, GPT-2, GPT-3, InstructGPT / ChatGPT and GPT-4 summary</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2025/02/23/deepseek-v3-learning-notes/';
    var disqus_url = '/pages/2025/02/23/deepseek-v3-learning-notes/';
    var disqus_title = 'DeepSeek V3 learning notes';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>