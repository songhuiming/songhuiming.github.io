<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Recommendation System 01 &mdash; pydata by Huiming</title>
  <meta name="author" content="shm">


    
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>





  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata by Huiming</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Recommendation System 01</h1>
    <p class="meta">
<time datetime="2021-10-30T00:00:00-05:00" pubdate>Sat 30 October 2021</time>    </p>
</header>

  <div class="entry-content"><p>The purpose of recommendation system is to recommend the related products (item) to the users based on the user's query.</p>
<h1>1. Terminology</h1>
<h2>1.1. Items (also known as documents)</h2>
<p>The entities a system recommends. For the Google Play store, the items are apps to install. For YouTube, the items are videos.</p>
<h2>1.2. Query (also known as context)</h2>
<p>The information a system uses to make recommendations. Queries can be a combination of the following:</p>
<h3>user information</h3>
<p>the id of the user
  items that users previously interacted with</p>
<h3>additional context</h3>
<p>time of day
  the user's device</p>
<p>目的就是把item推荐给user based on user information.</p>
<h2>1.3. Embedding</h2>
<p>A mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space. Many recommendation systems rely on learning an appropriate embedding representation of the queries and items.</p>
<h1>2. Recommendation Systems Overview</h1>
<p><img alt="image" src="https://developers.google.cn/machine-learning/recommendation/images/Process.svg"></p>
<h2>2.1. Candidate Generation / Retrival</h2>
<p>In this first stage, the system starts from a potentially huge corpus and generates a much smaller subset of candidates. For example, the candidate generator in YouTube reduces billions of videos down to hundreds or thousands. The model needs to evaluate queries quickly given the enormous size of the corpus. A given model may provide multiple candidate generators, each nominating a different subset of candidates.</p>
<h2>2.2. Scoring</h2>
<p>Next, another model scores and ranks the candidates in order to select the set of items (on the order of 10) to display to the user. Since this model evaluates a relatively small subset of items, the system can use a more precise model relying on additional queries.</p>
<h2>2.3. Re-ranking</h2>
<p>Finally, the system must take into account additional constraints for the final ranking. For example, the system removes items that the user explicitly disliked or boosts the score of fresher content. Re-ranking can also help ensure diversity, freshness, and fairness.</p>
<p>We will discuss each of these stages over the course of the class and give examples from different recommendation systems, such as YouTube.</p>
<h1>3. Candidate Generation Overview</h1>
<div class="highlight"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="kr">cont</span><span class="n">ent</span><span class="o">-</span><span class="n">based</span><span class="w"> </span><span class="n">filtering</span><span class="p">:</span><span class="w"> </span><span class="n">recommendate</span><span class="w"> </span><span class="n">based</span><span class="w"> </span><span class="kr">on</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">similarity</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">items</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">collaborative</span><span class="w"> </span><span class="n">filtering</span><span class="err">：</span><span class="w"> </span><span class="kr">if</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="ow">and</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">B</span><span class="w"> </span><span class="n">are</span><span class="w"> </span><span class="n">similar</span><span class="p">,</span><span class="w"> </span><span class="kr">then</span><span class="w"> </span><span class="n">recommend</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">A</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">B</span><span class="mf">.</span>
</code></pre></div>

<h2>3.1. Embedding Space: map item and query to a <span class="math">\(d\)</span> dimension space</h2>
<p>Map each item and each query (or context) to an embedding vector in a common embedding space . Typically, the embedding space is low-dimensional <span class="math">\(E = \mathbb{R}^{d}\)</span> (that is,  is much smaller than the size of the corpus), and captures some latent structure of the item or query set.</p>
<h2>3.2. Similarity Measures：based on embedding to find item, or find the related items and querys</h2>
<p>A similarity measure is a function <span class="math">\(s: E X E -&gt; \mathbb{R}\)</span> that takes a pair of embeddings and returns a scalar measuring their similarity. The embeddings can be used for candidate generation as follows: given a query embedding <span class="math">\(q \in E\)</span>, the system looks for item embeddings <span class="math">\(x \in E\)</span> that are close to <span class="math">\(q\)</span>, that is, embeddings with high similarity <span class="math">\(s(q, x)\)</span>.</p>
<p><span class="math">\(cos(x,y)\)</span>: the anger between two vectors. Below C and item has the least anger degree.</p>
<p><span class="math">\(dot(x, y)\)</span>：dot product, the length of vector 1 times the length of vector 2, then times cos(anger). A is the longest.</p>
<p>Eculidian distance ||x - y||: which is the distance of two vectors. B and query has the shortest dist.</p>
<p><img alt="看看那个距离是对的" src="https://developers.google.cn/machine-learning/recommendation/images/Similarity.svg"></p>
<h2>3.3. Which Similarity Measure to Choose?</h2>
<p>If use dot prod, the bigger the norm of the vector, the more likely it will be selected. The item has more frequency are usually have embedding with bigger norm, which will result in the common items (or most viewed videos) are more likely to be recommended. </p>
<h1>4. Content-based Filtering</h1>
<p>Which recommend based on similar items. Ignore here sicne it is easy. </p>
<h1>5. <a href="https://developers.google.cn/machine-learning/recommendation/collaborative/basics">Collaborative Filtering</a></h1>
<h2>5.1. Basics</h2>
<p>Take an example to recommend movies to the customer. One row is one customer, and one column is one movie. The target is: for a given customer, recommend movies to him based on the customers that are clsoe to him.</p>
<p>Suppose movies has these features: name, rating, introduction</p>
<h3>5.1.1. 1-d embedding</h3>
<p>Suppose we can score the movie from -1 to 1 based on if it is for kids or for adults. -1 means for kids and 1 means only for adults. With this each movie will have a score from -1 to 1. In the same way, we can score the customers based on they are kids or adults. </p>
<p><img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1D.svg"></p>
<p>Now in the (user，item) feedback matrix we have the "like" from the user to the movie.
<img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1Dmatrix.svg"></p>
<h3>5.1.2. 2-d embedding</h3>
<p>Most of the time, 1-d cannot describe the complexity of the movie. We may need 2-d or high dimension to describe. Suppose we have another dimenison to describe the movie is Arthose or Blockbuster.</p>
<p><img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2D.svg"></p>
<p><img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2Dmatrix.svg"></p>
<h2>5.2. <a href="https://developers.google.cn/machine-learning/recommendation/collaborative/matrix">Matrix Factorization</a></h2>
<p>Assume feedback matrix <span class="math">\(A \in \mathbb{R}^{m \times n}\)</span>, m x n matrix. m means m user, and n means n items. </p>
<p>Matrix Factorization is to get a matrix U and matrix V, such that</p>
<div class="highlight"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">U</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">user</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">embedding</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">V</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="n">d</span><span class="p">,</span><span class="w"> </span><span class="n">each</span><span class="w"> </span><span class="n">row</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">an</span><span class="w"> </span><span class="n">item</span><span class="err">&#39;</span><span class="n">s</span><span class="w"> </span><span class="n">embedding</span>
</code></pre></div>

<p>Our goal is to factorize the ratings matrix <span class="math">\(A\)</span> into the product of a user embedding matrix <span class="math">\(U\)</span> and movie embedding matrix <span class="math">\(V\)</span>, such that <span class="math">\(A \approx UV^\top\)</span> with</p>
<div class="math">$$U = \begin{bmatrix} u_{1} \\ \hline \vdots \\ \hline u_{N} \end{bmatrix}$$</div>
<div class="math">$$V = \begin{bmatrix} v_{1} \\ \hline \vdots \\ \hline v_{M} \end{bmatrix}$$</div>
<p>Here</p>
<div class="highlight"><pre><span></span><code><span class="k">-</span> $N$ is the number of users,
<span class="k">-</span> $M$ is the number of movies,
<span class="k">-</span> $A_{ij}$ is the rating of the $j$th movies by the $i$th user,
<span class="k">-</span> each row $U_i$ is a $d$-dimensional vector (embedding) representing user $i$,
<span class="k">-</span> each row $V_j$ is a $d$-dimensional vector (embedding) representing movie $j$,
<span class="k">-</span> the prediction of the model for the $(i, j)$ pair is the dot product $\langle U_i, V_j \rangle$.
</code></pre></div>

<p>embedding matrix U and V has <span class="math">\(UV^T\)</span> as approx of feedback matrix A. A has dim <span class="math">\(m \times n\)</span>, the new U V has dim = <span class="math">\(（m+n）\times d\)</span>. Usually d is small, which means we will use low dim matrix to approx the original A. Singular value decomposion (SVD) is not a good method here because A is sparse most of the time.</p>
<div class="highlight"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">Stochastic</span><span class="w"> </span><span class="n">gradient</span><span class="w"> </span><span class="n">descent</span><span class="w"> </span><span class="p">(</span><span class="n">SGD</span><span class="p">)</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="n">generic</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">minimize</span><span class="w"> </span><span class="n">loss</span><span class="w"> </span><span class="n">functions</span><span class="mf">.</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">Weighted</span><span class="w"> </span><span class="n">Alternating</span><span class="w"> </span><span class="n">Least</span><span class="w"> </span><span class="n">Squares</span><span class="w"> </span><span class="p">(</span><span class="n">WALS</span><span class="p">)</span><span class="w"> </span><span class="n">is</span><span class="w"> </span><span class="n">specialized</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">this</span><span class="w"> </span><span class="n">particular</span><span class="w"> </span><span class="n">objective</span><span class="mf">.</span>

<span class="mf">1.</span><span class="w"> </span><span class="n">pros</span><span class="p">:</span><span class="w"> </span><span class="n">no</span><span class="w"> </span><span class="n">special</span><span class="w"> </span><span class="n">pre</span><span class="o">-</span><span class="n">knowledge</span><span class="p">,</span><span class="w"> </span><span class="n">Serendipity</span><span class="w"> </span><span class="p">(</span><span class="n">model</span><span class="w"> </span><span class="n">help</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="kr">get</span><span class="w"> </span><span class="n">likes</span><span class="p">),</span><span class="w"> </span><span class="n">easy</span><span class="w"> </span><span class="kr">to</span><span class="w"> </span><span class="n">start</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">cons</span><span class="p">:</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">handle</span><span class="w"> </span><span class="kr">new</span><span class="w"> </span><span class="n">item</span><span class="p">,</span><span class="w"> </span><span class="n">cannot</span><span class="w"> </span><span class="n">use</span><span class="w"> </span><span class="n">side</span><span class="w"> </span><span class="n">features</span><span class="w"> </span><span class="p">(</span><span class="n">age</span><span class="p">,</span><span class="w"> </span><span class="n">address</span><span class="p">)</span><span class="o">-</span><span class="w"> </span><span class="n">high</span><span class="w"> </span><span class="kd">dim</span><span class="w"> </span><span class="n">feedback</span><span class="w"> </span><span class="n">matrix</span><span class="w"> </span><span class="n">A</span>
</code></pre></div>

<p><a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=recommendation-systems">使用colab来练习怎么做推荐系统</a></p>
<p><a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv">kaggle data link</a></p>
<h1>6. <a href="https://developers.google.cn/machine-learning/recommendation/dnn/softmax">Recommendation with Deep Neural Network (DNN) Models</a></h1>
<p>DNN can put item feature and user feature into a network modle to learn embeddings.</p>
<h2>Softmax DNN for Recommendation</h2>
<div class="highlight"><pre><span></span><code><span class="n">input</span><span class="o">:</span><span class="w"> </span><span class="n">user</span><span class="w"> </span><span class="n">query</span><span class="w">  </span>
<span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">prob</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="n">items</span>
</code></pre></div>

<p>input X can be dense feature (watch time length, time since last watch), it can also be sparse feature (country, watch history).</p>
<p><img alt="DNN for recommendaiton system" src="https://developers.google.cn/machine-learning/recommendation/images/LossFunction.svg"></p>
<p>Last hidden layer of DNN output is <span class="math">\(\phi(x)\)</span>, which is a vector with dim = d. It will times a matrix V <span class="math">\(V \in n \times d\)</span> to get a vector with dim = n. Each value is the probability for item.</p>
<p>Loss funciton is the function of  model output <span class="math">\(\hat{p}\)</span> and the true item y. We can use cross entropy.</p>
<p>The purpose of the model is to learn embedding, s.t. for each item j, there is an vector <span class="math">\(v_j \in \mathbb{R}^d\)</span> (n items, then <span class="math">\(V \in n \times d\)</span>). With softmax model, V are the parameters of the model.</p>
<p>For query embedding, for query i, model is not to get embedding <span class="math">\(U_{i}\)</span>, but to learn the function <span class="math">\(\phi(\cdot)\)</span>, which will map input x  to a d-dim vector <span class="math">\(\phi(x)\)</span></p>
<p>Furthermore, can we build two DNN:
    1. 1st DNN is to get function <span class="math">\(\phi(x_{item}) \in \mathbb{R}^{d}\)</span>, map item to d-dim space
    2. 2nd DNN is to get funciton <span class="math">\(\psi(x_{query}) \in \mathbb{R}^{d}\)</span>, map query  to d-dim space</p>
<p>Finally we can use dot product &lt; <span class="math">\(\phi(x_{item})\)</span>, <span class="math">\(\psi(x_{query})\)</span> &gt; to get the correlation between query i and item j.</p>
<h2>Train the model</h2>
<p>The training data includes the user's query and the user's side features, and the items that the user has been interested before for each user. The forward and backward propagation is based on the design of the DNN. The minium of the loss funciton can be achieved by stochastic gradient descending algorithm.  </p>
<h2>Negative sampling</h2>
<p>When there are n items, the output prob from DNN is <span class="math">\(\hat{p} \in \mathbb{R}^{n}\)</span>. Since n is very big, which will result in lots of calcualation in backpropagation.(In NLP, when clac the prob of neighbors words given the central word, it also uses negative smapling because of word size is also very big). </p>
<p>Negative sampling will do: </p>
<div class="highlight"><pre><span></span><code><span class="mf">1.</span><span class="w"> </span><span class="n">All</span><span class="w"> </span><span class="nb">pos</span><span class="n">itive</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="p">(</span><span class="n">the</span><span class="w"> </span><span class="kr">on</span><span class="n">es</span><span class="w"> </span><span class="n">that</span><span class="w"> </span><span class="n">appear</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="n">target</span><span class="w"> </span><span class="n">label</span><span class="p">)</span>
<span class="mf">2.</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="n">sample</span><span class="w"> </span><span class="n">of</span><span class="w"> </span><span class="n">negative</span><span class="w"> </span><span class="n">items</span><span class="w"> </span><span class="p">(</span><span class="n">j</span><span class="w"> </span><span class="n">in</span><span class="w"> </span><span class="mf">1</span><span class="p">,</span><span class="w"> </span><span class="mf">2</span><span class="p">,</span><span class="w"> </span><span class="mf">3</span><span class="p">,</span><span class="w"> </span><span class="mf">...</span><span class="w"> </span><span class="n">n</span><span class="p">)</span>
</code></pre></div>

<p>An example from Youtube recommendation:
<img alt="youtube" src="/figures/20211030_recommendation_01_youtube.png"></p>
<h1>Reference</h1>
<ol>
<li><a href="https://developers.google.cn/machine-learning/recommendation">google: 推荐系统介绍</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNDA5NDYzNA==&amp;mid=2247484256&amp;idx=1&amp;sn=a92fc08b974339e1143c4f07b6591b72&amp;chksm=96c42ea5a1b3a7b39c996f91471d47478fedde1b3c7a3f6538cdcb2d0b95407199381e6b7c80&amp;cur_album_id=1555890573570523140&amp;scene=189#rd">深入理解推荐系统：召回</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/114703091">深入理解YouTube推荐系统算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/58160982">推荐系统召回四模型之：全能的FM模型</a></li>
<li><a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=recommendation-systems">使用colab来练习怎么做推荐系统</a></li>
<li><a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv">kaggle上的data链接</a></li>
<li><a href="https://mp.weixin.qq.com/s/yv27xqGDLTYG6mxguLIL0Q">情感分析技术在美团的探索与应用</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/420995638">浅谈行为序列建模</a></li>
<li><a href="https://ax.dev">ax.dev</a>  ---  <a href="https://gpytorch.ai">GPyTorch</a>  --- <a href="https://botorch.org">BoTorch</a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2021-10-30T00:00:00-05:00" pubdate>Sat 30 October 2021</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/02/18/deepseek-v3-2/">DeepSeek V3-2</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
      <li class="post">
          <a href="/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">GPT-1, GPT-2, GPT-3, InstructGPT / ChatGPT and GPT-4 summary</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2021/10/30/recommendation-system-01/';
    var disqus_url = '/pages/2021/10/30/recommendation-system-01/';
    var disqus_title = 'Recommendation System 01';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>