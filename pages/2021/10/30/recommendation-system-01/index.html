<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Recommendation System 01 &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Recommendation System 01</h1>
    <p class="meta">
<time datetime="2021-10-30T00:00:00-05:00" pubdate>Sat 30 October 2021</time>    </p>
</header>

  <div class="entry-content"><p>The purpose of recommendation system is to recommend the related products (item) to the users based on the user's query.</p>
<h1>1. Terminology</h1>
<h2>1.1. Items (also known as documents)</h2>
<p>The entities a system recommends. For the Google Play store, the items are apps to install. For YouTube, the items are videos.</p>
<h2>1.2. Query (also known as context)</h2>
<p>The information a system uses to make recommendations. Queries can be a combination of the following:</p>
<h3>user information</h3>
<p>the id of the user
  items that users previously interacted with</p>
<h3>additional context</h3>
<p>time of day
  the user's device</p>
<p>目的就是把item推荐给user based on user information.</p>
<h2>1.3. Embedding</h2>
<p>A mapping from a discrete set (in this case, the set of queries, or the set of items to recommend) to a vector space called the embedding space. Many recommendation systems rely on learning an appropriate embedding representation of the queries and items.</p>
<h1>2. Recommendation Systems Overview</h1>
<p><img alt="image" src="https://developers.google.cn/machine-learning/recommendation/images/Process.svg" /></p>
<h2>Candidate Generation / Retrival</h2>
<p>In this first stage, the system starts from a potentially huge corpus and generates a much smaller subset of candidates. For example, the candidate generator in YouTube reduces billions of videos down to hundreds or thousands. The model needs to evaluate queries quickly given the enormous size of the corpus. A given model may provide multiple candidate generators, each nominating a different subset of candidates.</p>
<h2>Scoring</h2>
<p>Next, another model scores and ranks the candidates in order to select the set of items (on the order of 10) to display to the user. Since this model evaluates a relatively small subset of items, the system can use a more precise model relying on additional queries.</p>
<h2>Re-ranking</h2>
<p>Finally, the system must take into account additional constraints for the final ranking. For example, the system removes items that the user explicitly disliked or boosts the score of fresher content. Re-ranking can also help ensure diversity, freshness, and fairness.</p>
<p>We will discuss each of these stages over the course of the class and give examples from different recommendation systems, such as YouTube.</p>
<h1>Candidate Generation Overview</h1>
<div class="highlight"><pre>1. content-based filtering: recommendate based on the similarity of items

2. collaborative filtering： if user A and user B are similar, then recommend user A&#39;s items to user B.
</pre></div>


<h2>Embedding Space: map item and query to a <span class="math">\(d\)</span> dimension space</h2>
<p>Map each item and each query (or context) to an embedding vector in a common embedding space . Typically, the embedding space is low-dimensional <span class="math">\(E = \mathbb{R}^{d}\)</span> (that is,  is much smaller than the size of the corpus), and captures some latent structure of the item or query set.</p>
<h2>Similarity Measures：based on embedding to find item, or find the related items and querys</h2>
<p>A similarity measure is a function <span class="math">\(s: E X E -&gt; \mathbb{R}\)</span> that takes a pair of embeddings and returns a scalar measuring their similarity. The embeddings can be used for candidate generation as follows: given a query embedding <span class="math">\(q \in E\)</span>, the system looks for item embeddings <span class="math">\(x \in E\)</span> that are close to <span class="math">\(q\)</span>, that is, embeddings with high similarity <span class="math">\(s(q, x)\)</span>.</p>
<p><span class="math">\(cos(x,y)\)</span>: the anger between two vectors. Below C and item has the least anger degree.</p>
<p><span class="math">\(dot(x, y)\)</span>：dot product, the length of vector 1 times the length of vector 2, then times cos(anger). A is the longest.</p>
<p>Eculidian distance ||x - y||: which is the distance of two vectors. B and query has the shortest dist.</p>
<p><img alt="看看那个距离是对的" src="https://developers.google.cn/machine-learning/recommendation/images/Similarity.svg" /></p>
<h2>Which Similarity Measure to Choose?</h2>
<p>If use dot prod, the bigger the norm of the vector, the more likely it will be selected. The item has more frequency are usually have embedding with bigger norm, which will result in the common items (or most viewed videos) are more likely to be recommended. </p>
<h1>Content-based Filtering</h1>
<h1><a href="https://developers.google.cn/machine-learning/recommendation/collaborative/basics">Collaborative Filtering</a></h1>
<h2>Basics</h2>
<p>比如给用户推荐电影。每一行是一个用户，每一列是一部电影。目的是对某个用户，根据和他类似的用户的记录来给他推荐电影。</p>
<p>假设电影由这些feature：名字，分数，简介</p>
<h3>一维embedding</h3>
<p>比如说电影适合儿童还是适合大人，打分从-1到1。越适合儿童就越靠近-1，越只适合大人就越靠近1. 这样每个电影都有一个分数。
<img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1D.svg" /></p>
<p>把用户喜欢哪些电影在（user，item）的matrix中标记出来
<img alt="一维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/1Dmatrix.svg" /></p>
<h3>二维embedding</h3>
<p>一维可能不足以表达电影的复杂度，我们需要二维，比如再加上文艺片还是动作片这个维度
<img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2D.svg" /></p>
<p><img alt="二维embedding" src="https://developers.google.cn/machine-learning/recommendation/images/2Dmatrix.svg" /></p>
<h2><a href="https://developers.google.cn/machine-learning/recommendation/collaborative/matrix">Matrix Factorization</a></h2>
<p>假设feedback matrix <span class="math">\(A \in R^(m \times n)\)</span>, m x n维的矩阵。m表示m个user，n表示n个item。 </p>
<p>Matrix Factorization是来学习得到一个矩阵 U 和矩阵 V ，满足
    1. U 是 m x d 维度的矩阵，每一行表示一个 user 的 embedding
    2. V 是 n x d 维度的矩阵，每一行表示一个 item 的 embedding</p>
<p>Our goal is to factorize the ratings matrix <span class="math">\(A\)</span> into the product of a user embedding matrix <span class="math">\(U\)</span> and movie embedding matrix <span class="math">\(V\)</span>, such that <span class="math">\(A \approx UV^\top\)</span> with
<span class="math">\(U = <div class="math">\begin{bmatrix} u_{1} \\ \hline \vdots \\ \hline u_{N} \end{bmatrix}</div>\)</span> and
<span class="math">\(V = <div class="math">\begin{bmatrix} v_{1} \\ \hline \vdots \\ \hline v_{M} \end{bmatrix}</div>\)</span>.</p>
<p>Here
- <span class="math">\(N\)</span> is the number of users,
- <span class="math">\(M\)</span> is the number of movies,
- <span class="math">\(A_{ij}\)</span> is the rating of the <span class="math">\(j\)</span>th movies by the <span class="math">\(i\)</span>th user,
- each row <span class="math">\(U_i\)</span> is a <span class="math">\(d\)</span>-dimensional vector (embedding) representing user <span class="math">\(i\)</span>,
- each row <span class="math">\(V_j\)</span> is a <span class="math">\(d\)</span>-dimensional vector (embedding) representing movie <span class="math">\(j\)</span>,
- the prediction of the model for the <span class="math">\((i, j)\)</span> pair is the dot product <span class="math">\(\langle U_i, V_j \rangle\)</span>.</p>
<p>embedding矩阵U和V使得<span class="math">\(UV^T\)</span>是feedback matrix A 的很好的近似。原来A的大小为mn，新的U V的大小为 （m+n）d，通常d比较小，这样相当于在一个低维空间上来近似原来的矩阵。奇异值分解（SVD）不是一个很好的办法，因为A通常非常sparse。</p>
<div class="highlight"><pre>1. Stochastic gradient descent (SGD) is a generic method to minimize loss functions.
2. Weighted Alternating Least Squares (WALS) is specialized to this particular objective.

1. 优点 - 不需要掌握特别的知识，Serendipity （模型来帮助用户找到喜欢的东西），容易开始
2. 缺点 - 不能处理新的item（因为大家都没有见过），不能出来side features （比如年龄，上网地址，等等）- 建立高维的feedback matrix A
</pre></div>


<p><a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=recommendation-systems">使用colab来练习怎么做推荐系统</a></p>
<p><a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv">kaggle上的data链接</a></p>
<h1><a href="https://developers.google.cn/machine-learning/recommendation/dnn/softmax">Recommendation with Deep Neural Network （DNN模型） Models</a></h1>
<p>DNN模型可以把item feature和user feature放进模型里面来学习embedding。</p>
<h2>Softmax DNN for Recommendation，使用softmax</h2>
<p>输入： user query<br />
  输出： n个items的概率</p>
<p>input X 可以是 dense feature（观看时长，距离上次观看的间隔时间），也可以是sparse feature（国家，观看历史等）。</p>
<p><img alt="DNN for recommendaiton system" src="https://developers.google.cn/machine-learning/recommendation/images/LossFunction.svg" /></p>
<p>DNN的最后一个hidden layer的输出 <span class="math">\(\phi(x)\)</span> 为 d 维的向量，然后再通过一个矩阵 V <span class="math">\(V \in n \time d\)</span> 来得到一个n维的向量，向量每一个值表示一个item的概率。</p>
<p>loss funciton是模型输出 phat 和 真实值 y 的函数。比如cross entropy。</p>
<p>模型的目的是学习得到embedding,对每一个item j，得到一个向量 <span class="math">\(v_j \in R^d\)</span> (n个item，那么 <span class="math">\(V \in n \time d\)</span>)。在softmax模型下，V就是模型的系数。</p>
<p>对query embedding，对query i，模型不是要得到一个embedding <img src="https://render.githubusercontent.com/render/math?math=U_{i}">，而是学习到了一个函数 <img src="https://render.githubusercontent.com/render/math?math=\phi(\cdot)">
, 把输入的x map到一个d维的向量 <img src="https://render.githubusercontent.com/render/math?math=\phi(x)"></p>
<p>再进一步，我们能不能build两个DNN，
  1. 第一个DNN用来找到一个函数<img src="https://render.githubusercontent.com/render/math?math=\phi(x_{item}) \in \mathbb{R}^{d}">把item map到d维空间
  2. 第二个DNN用来找到一个函数<img src="https://render.githubusercontent.com/render/math?math=\psi(x_{query}) \in \mathbb{R}^{d}">把query map到d维空间 </p>
<p>最后就可以使用内积 &lt; <img src="https://render.githubusercontent.com/render/math?math=\phi(x_{item})">, <img src="https://render.githubusercontent.com/render/math?math=\psi(x_{query})"> &gt; 来得到query i 和 item j的相关性的值。</p>
<h2>Train the model</h2>
<p>The training data includes the user's query and the user's side features, and the items that the user has been interested before for each user. The forward and backward propagation is based on the design of the DNN. The minium of the loss funciton can be achieved by stochastic gradient descending algorithm.  </p>
<h2>Negative sampling</h2>
<p>当有n个items的时候，输出概率为<img src="https://render.githubusercontent.com/render/math?math=\hat(p) \in \mathbb{R}^{n}">. 因为n非常大，从而导致在loss function的时候反向求导的计算非常多。（在NLP里面，根据中心词求旁边相邻单词的概率的时候，也用到negative sampling。同样的是因为候选单词的个数太多了）</p>
<p>Negative sampling will do</p>
<ol>
<li>All positive items (the ones that appear in the target label)</li>
<li>A sample of negative items (j in 1 2 3 。。。n)</li>
</ol>
<p>An example from Youtube recommendation:
<img alt="youtube" src="https://github.com/songhuiming/draft-blogs/blob/main/figures/20211030_recommendation_01_youtube.png" /></p>
<p><img src="https://render.githubusercontent.com/render/math?math=x_{1,2} = \frac{-b \pm \sqrt{b^2-4ac}}{2b}"></p>
<h1>Reference</h1>
<ol>
<li><a href="https://developers.google.cn/machine-learning/recommendation">google: 推荐系统介绍</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzIwNDA5NDYzNA==&amp;mid=2247484256&amp;idx=1&amp;sn=a92fc08b974339e1143c4f07b6591b72&amp;chksm=96c42ea5a1b3a7b39c996f91471d47478fedde1b3c7a3f6538cdcb2d0b95407199381e6b7c80&amp;cur_album_id=1555890573570523140&amp;scene=189#rd">深入理解推荐系统：召回</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/114703091">深入理解YouTube推荐系统算法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/58160982">推荐系统召回四模型之：全能的FM模型</a></li>
<li><a href="https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb?utm_source=ss-recommendation-systems&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=recommendation-systems">使用colab来练习怎么做推荐系统</a></li>
<li><a href="https://www.kaggle.com/grouplens/movielens-20m-dataset?select=tag.csv">kaggle上的data链接</a></li>
<li><a href="https://mp.weixin.qq.com/s/yv27xqGDLTYG6mxguLIL0Q">情感分析技术在美团的探索与应用</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/420995638">浅谈行为序列建模</a></li>
<li><a href=""></a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2021-10-30T00:00:00-05:00" pubdate>Sat 30 October 2021</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2021/10/30/recommendation-system-01/">Recommendation System 01</a>
      </li>
      <li class="post">
          <a href="/pages/2021/10/22/zhi-chang-hua-ti-bei-mei-hua-ren-gao-guan-zong-jie-de-zhi-chang-shang-sheng-fang-fa/">职场话题——北美华人高管总结的职场上升方法</a>
      </li>
      <li class="post">
          <a href="/pages/2019/07/27/2019-07-27-week-30-gong-ju-he-ji/">2019-07-27 Week 30 工具合集</a>
      </li>
      <li class="post">
          <a href="/pages/2019/07/20/2019-07-20-week-29-nvidia-smi-error/">2019-07-20 Week 29, nvidia-smi error</a>
      </li>
      <li class="post">
          <a href="/pages/2019/06/22/2019-06-22-week-25-gong-ju-he-ji/">2019-06-22 Week 25 工具合集</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2021  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2021/10/30/recommendation-system-01/';
    var disqus_url = '/pages/2021/10/30/recommendation-system-01/';
    var disqus_title = 'Recommendation System 01';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>