<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Image Generation 2: Latent Diffusion model / Stable Diffusion &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Image Generation 2: Latent Diffusion model / Stable Diffusion</h1>
    <p class="meta">
<time datetime="2023-10-01T00:00:00-05:00" pubdate>Sun 01 October 2023</time>    </p>
</header>

  <div class="entry-content"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script>

<p>In the <a href="https://songhuiming.github.io/pages/2023/07/04/image-generation-1-diffusion-model/">previous blog</a> we introduced diffusion model (DDPM) which is to learn the step (time <span class="math">\(t\)</span>) and the noise function (NN model) by adding Gaussian noise to an image step by step and reversing the process by denosiing from Gaussian noise to an image. Diffusion model is the most important part of Gen AI, however, there are still a few steps from Diffusion model to Gen AI: 
1. How to comsume the prompts (instructions to Gen AI to generate a response) in the model? 
2. Diffusion model needs huge computaiton as it is running on image pixel directly. 
To sovle this, latent diffusion model (stable diffusion) was introduced to project the pixel level image to the smaller latent space with perceptual conpression. </p>
<h2>What problem Latent Diffusion Models (LDM) solve?</h2>
<p>Latent diffusion model (LDM) applies diffusion model on the latent space from encoder rather than the pixel level data. Diffusion model (DDPM) directly adds noise to the image and uses UNet to learn the noise in the denoise step. One challenge here is that it requires a lot of computation with high dimension RGB images in the training step. Also because the noise and denoise needs to be run for many different steps, it makes the computation more expensive and time consumable to run. To solve this problem, the LDM paper (<a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models
</a>) proposed an approach to use an encoder (<span class="math">\(\mathcal{E}\)</span>) to convert the image to a latent space <span class="math">\(z\)</span> (which has less dimension than the original image) and and a decoder (<span class="math">\(\mathcal{D}\)</span>) to convert the latent space to image. Diffusion model is then trained on the latent space with better scaling properties because of lower spatial dimension. The reduced complexity also makes it more efficient in image generation from the latent space.</p>
<p><img alt="stable diffusion 02" src="/figures/20231001_stable_diffusion_model_02.png" /></p>
<p>The reason of DP can be done in the latent space is that most bits of a digital image correspond to imperceptible details. After the perceptual compression through the encoder projected to the latent space, most of the perceptual information will still be kept and it only eliminates the imperceptible details. The LDM as a generative model learns the semantic and conceptual composition of the data.</p>
<p>In order to generate images from text prompt <span class="math">\(y\)</span>, that is, to predict the probability of latent space <span class="math">\(z\)</span> given prompt <span class="math">\(y\)</span>, the authors also design the archtecture of conditional denoising autoencoder <span class="math">\(\epsilon_{\theta}(z_t, t, y)\)</span> which is UNet similar to DM to control the synthesis process with input <span class="math">\(y\)</span> by concating it with the latent space. </p>
<h2>How does Latent Diffusion Model work?</h2>
<h3>1. Perceptual Image Compression</h3>
<p>Given an image <span class="math">\(x \in \mathbb{R}^{H × W ×3}\)</span> in RGB space, the encoder <span class="math">\(\mathcal{E}\)</span> encodes <span class="math">\(x\)</span> into a latent representation <span class="math">\(z = \mathcal{E}(x) \in \mathbb{R}^{h×w×c}\)</span>, and the decoder <span class="math">\(\mathcal{D}\)</span> reconstructs the image from the latent space with <span class="math">\( \tilde{x} = \mathcal{D}(z) = \mathcal{D}(\mathcal{E}(x))\)</span>. The encoder downsamples the image by a factor <span class="math">\(f = H/h = W/w\)</span> where downsampling factors <span class="math">\(f = 2^m\)</span>, with <span class="math">\(m \in \mathcal{N}\)</span>. For example, if <span class="math">\(m=3\)</span>, the original image <span class="math">\(x\)</span> size will be reduced about <span class="math">\(64\)</span> times less.</p>
<h3>2. Latent Diffusion Models</h3>
<p>Similar to <a href="https://songhuiming.github.io/pages/2023/07/04/image-generation-1-diffusion-model/">DP model</a> which is to find the <span class="math">\(\epsilon_{\theta}(x_t, t)\)</span> to approximate the error <span class="math">\(\epsilon\)</span> given input image <span class="math">\(x_t\)</span> at step <span class="math">\(t\)</span>, latent diffusion model is to find <span class="math">\(\epsilon_{\theta}(z_t, t)\)</span> given the latent space <span class="math">\(z_t\)</span> at step <span class="math">\(t\)</span>, where <span class="math">\(\epsilon_{\theta}\)</span> is UNet from 2D convolutional layers. So, starting from an image <span class="math">\(x_t\)</span>, LDM includes these steps:
1. Encoder <span class="math">\(\mathcal{E}\)</span> to convert an image <span class="math">\(x_t\)</span> to latent space <span class="math">\(z_t\)</span>
2. Build UNet to learn <span class="math">\(\epsilon_{\theta}(z_t, t)\)</span> 
3. Sample from <span class="math">\(P(z)\)</span> and apply the decoder <span class="math">\(\mathcal{D}\)</span> to convert <span class="math">\(z_t\)</span> back to <span class="math">\(x_t\)</span></p>
<h3>3. Conditioning mechanism to generate image from prompt/condition</h3>
<p>To generate contents from input like prompt <span class="math">\(y\)</span>, the model will not only depends on the time step <span class="math">\(t\)</span>, but also the input <span class="math">\(y\)</span>, that is the conditional distribution of <span class="math">\(p(z_t | y)\)</span>. Accordingly, <span class="math">\(\epsilon_{\theta}(z_t, t)\)</span> is not only the funcion of <span class="math">\(z_t\)</span> and <span class="math">\(t\)</span>, but also with input <span class="math">\(y\)</span>. That is, <span class="math">\(\epsilon_{\theta}(z_t, t, y)\)</span>.</p>
<p><img alt="stable diffusion 01" src="/figures/20231001_stable_diffusion_model_01.png" /></p>
<p><strong>Notations for the figure above</strong>:</p>
<ol>
<li><span class="math">\(x\)</span>: input of the image. If RGB, its dimension is <span class="math">\(H \times W \times 3\)</span></li>
<li><span class="math">\(\mathcal{E}\)</span> and <span class="math">\(\mathcal{D}\)</span> are the encoder and decoder, where <span class="math">\(\mathcal{E}\)</span> is to project the image to a low dimension space <span class="math">\(z = \mathcal{E}(x)\)</span> which will save the calculation in the later steps, and  <span class="math">\(\mathcal{D}\)</span> is to project from the latent space to the image.</li>
<li><span class="math">\(\tau_{\theta}\)</span> is the encoder to project the conditional input (e.g., prompt text) to the intermediate space</li>
<li><span class="math">\(\epsilon_{\theta}(z_t, t, \tau_{\theta}(y)\)</span> is the conditional denosing autoencoder to denoise from <span class="math">\(z_t\)</span> to <span class="math">\(z_{t-1}\)</span></li>
</ol>
<p>The authors develop the attention mechanism to model the cross-attention relationships among the inputs by augmenting the UNet backbone with cross-attention mechanism. The attention is between the condition encoder <span class="math">\(\tau_{\theta}(y)\)</span> and the each intermediate representation layer <span class="math">\(i\)</span> of UNet. To do this, they first build an encoder <span class="math">\(\tau_{\theta}\)</span> to project <span class="math">\(y\)</span> to intermedidate representation <span class="math">\(\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}\)</span>. The cross-attention is between the <span class="math">\(\tau_{\theta}(y)\)</span> and the projection of the latent space <span class="math">\(\varphi_i(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}\)</span> in the UNet with attention defined as 
</p>
<div class="math">$$ \text{Attention}(Q, K, V) = \text{softmax}\left( \frac{QK^{T}}{\sqrt{d}} \right) \cdot V$$</div>
<p> and</p>
<div class="math">$$\color{blue}{Q = \varphi_{i}(z_t) \cdot W_{Q}^{(i)} , \ \ K = \tau_{\theta}(y) \cdot  W_{K}^{(i)} , \ \ V = \tau_{\theta}(y) \cdot W_{V}^{(i)} }$$</div>
<p> 
Here <span class="math">\( \varphi_{i}(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}\)</span> denotes a fattened intermediate representation layer <span class="math">\(i\)</span> of the UNet implementing <span class="math">\(\epsilon_{\theta}\)</span>. And <span class="math">\(\tau_{\theta}(y) \in \mathbb{R}^{M \times d_{\tau}}\)</span> is the embedding of the conditinal input.  The projection coefficient <span class="math">\(\color{blue}{W_{Q}^{(i)} \in \mathbb{R} ^{d_{\epsilon}^i \times d}}\)</span>,  <span class="math">\(\color{blue}{W_{K}^{(i)} \in \mathbb{R} ^{d_{\tau}  \times d }}\)</span>, <span class="math">\(\color{blue}{W_{V}^{(i)} \in \mathbb{R} ^{d_{\tau}  \times d }}\)</span>. </p>
<p><strong>Note</strong>: it seems the notations in the <a href="https://arxiv.org/pdf/2112.10752.pdf">original paper</a> is not correct. In the original paper, the projection is written as
</p>
<div class="math">$$\color{red}{Q = W_{Q}^{(i)} \cdot \varphi_{i}(z_t), \ \ K = W_{K}^{(i)} \cdot \tau_{\theta}(y), \ \ V = W_{V}^{(i)} \cdot \tau_{\theta}(y)}$$</div>
<p> and the size of the weights matrix  <span class="math">\(\color{red} {W_{V}^{(i)} \in \mathbb{R} ^{d \times d_{\epsilon}^i} }\)</span>,  <span class="math">\(\color{red} {W_{Q}^{(i)} \in \mathbb{R} ^{d \times d_{\tau}} }\)</span>, <span class="math">\(\color{red} {W_{K}^{(i)} \in \mathbb{R} ^{d \times d_{\tau}} }\)</span> which is not right. Because <span class="math">\(\color{red} { \varphi_{i}(z_t) \in \mathbb{R}^{N \times d_{\epsilon}^i}}\)</span>,  it is easy to find that the matrix multipicaiton <span class="math">\(\color{red}{Q = W_{Q}^{(i)} \cdot \varphi_{i}(z_t)}\)</span> cannot be conducted if the size is as described in the paper.</p>
<p>Based on this, the conditional LDM is to learn <span class="math">\(\epsilon_{\theta}\)</span> and <span class="math">\(\tau_{\theta}\)</span> from 
</p>
<div class="math">$$ L_{LDM} := \mathbb{E}_{\mathcal{E}(x), y, \epsilon \sim \mathcal{N}(0, 1), t}  \left[ ||\epsilon - \epsilon_{\theta}(z_t, t, \tau_{\theta}(y))||_2^2 \right] $$</div>
<h3>Code for LDM</h3>
<h4>1. Encoder-Decoder - project image to latent space and project from latent space to image</h4>
<p>In order to avoid arbitrarily high-variance latent spaces, the authors experiment with two different kinds of regularizations. The first variant is KL-reg which imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE. The second is VQ-reg which uses a vector quantization layer within the decoder.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">AutoencoderKL</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ddconfig</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">......</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">moments</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_conv</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">posterior</span> <span class="o">=</span> <span class="n">DiagonalGaussianDistribution</span><span class="p">(</span><span class="n">moments</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">posterior</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_quant_conv</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec</span>
    <span class="o">...</span> <span class="o">...</span>


<span class="k">class</span> <span class="nc">VQModel</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ddconfig</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>

    <span class="o">...</span> <span class="o">...</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_conv</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="n">quant</span><span class="p">,</span> <span class="n">emb_loss</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">quant</span><span class="p">,</span> <span class="n">emb_loss</span><span class="p">,</span> <span class="n">info</span>

    <span class="k">def</span> <span class="nf">encode_to_prequant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant_conv</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">h</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quant</span><span class="p">):</span>
        <span class="n">quant</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_quant_conv</span><span class="p">(</span><span class="n">quant</span><span class="p">)</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">quant</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec</span>

    <span class="k">def</span> <span class="nf">decode_code</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">code_b</span><span class="p">):</span>
        <span class="n">quant_b</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantize</span><span class="o">.</span><span class="n">embed_code</span><span class="p">(</span><span class="n">code_b</span><span class="p">)</span>
        <span class="n">dec</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">quant_b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dec</span>
    <span class="o">......</span>
</pre></div>


<h4>2. Embedding funciton for conditional input from <span class="math">\(y\)</span> to <span class="math">\(\tau_{\theta}(y)\)</span></h4>
<p>Deifferent embedding functions can be applied to convert the conditonal input to the numeric embeding. Like BERT embedding for text, or CLIP embedding for text or image depending on the input data.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">TransformerEmbedder</span><span class="p">(</span><span class="n">AbstractEncoder</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Some transformer encoder layers&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embed</span><span class="p">,</span> <span class="n">n_layer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">77</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="o">...</span>

<span class="k">class</span> <span class="nc">BERTEmbedder</span><span class="p">(</span><span class="n">AbstractEncoder</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Uses the BERT tokenizr model and add some transformer encoder layers&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embed</span><span class="p">,</span> <span class="n">n_layer</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">30522</span><span class="p">,</span> <span class="n">max_seq_len</span><span class="o">=</span><span class="mi">77</span><span class="p">,</span>
                 <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span><span class="n">use_tokenizer</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">embedding_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="o">...</span>

<span class="k">class</span> <span class="nc">FrozenCLIPTextEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the CLIP transformer encoder for text.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s1">&#39;ViT-L/14&#39;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">77</span><span class="p">,</span> <span class="n">n_repeat</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="o">...</span>


<span class="k">class</span> <span class="nc">FrozenClipImageEmbedder</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Uses the CLIP image encoder.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">vargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="o">...</span>
</pre></div>


<h4>3. Attention - attention between K and Q for new V</h4>
<p>Query is from the image in DDPM or the latent space is LDM. Key and Value are from the context or the conditions in LDM.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">CrossAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">,</span> <span class="n">context_dim</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">dim_head</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="n">inner_dim</span> <span class="o">=</span> <span class="n">dim_head</span> <span class="o">*</span> <span class="n">heads</span>
        <span class="n">context_dim</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">query_dim</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">to_q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">query_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">context_dim</span><span class="p">,</span> <span class="n">inner_dim</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">heads</span>
        <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_q</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">context</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_k</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_v</span><span class="p">(</span><span class="n">context</span><span class="p">)</span>

        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="n">rearrange</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s1">&#39;b n (h d) -&gt; (b h) n d&#39;</span><span class="p">,</span> <span class="n">h</span><span class="o">=</span><span class="n">h</span><span class="p">),</span> <span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
        <span class="n">sim</span> <span class="o">=</span> <span class="n">einsum</span><span class="p">(</span><span class="s1">&#39;b i d, b j d -&gt; b i j&#39;</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>

        <span class="c1"># attention, what we cannot get enough of</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="o">...</span> <span class="o">...</span>
</pre></div>


<h4>4. UNet - learn the noise for step <span class="math">\(t\)</span></h4>
<p>From the code, it shows that for each level of UNet, there is attention block between condition input <span class="math">\(\tau_{\theta}(y)\)</span> and the UNet intermediate representation <span class="math">\(\varphi_i(z_t)\)</span>. The attention mechanism is applied in both down sample and up sample levels in UNet.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">UNetModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">......</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="o">......</span>

        <span class="c1"># param channel_mult: channel multiplier for each level of the UNet.</span>
        <span class="k">for</span> <span class="n">level</span><span class="p">,</span> <span class="n">mult</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">channel_mult</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_res_blocks</span><span class="p">):</span>
                <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">ResBlock</span><span class="p">(</span><span class="o">...</span><span class="p">)]</span>

                <span class="n">ch</span> <span class="o">=</span> <span class="n">mult</span> <span class="o">*</span> <span class="n">model_channels</span>
                <span class="k">if</span> <span class="n">ds</span> <span class="ow">in</span> <span class="n">attention_resolutions</span><span class="p">:</span>
                    <span class="o">......</span>
                    <span class="n">layers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                        <span class="n">AttentionBlock</span><span class="p">(</span>
                            <span class="n">ch</span><span class="p">,</span>
                            <span class="n">use_checkpoint</span><span class="o">=</span><span class="n">use_checkpoint</span><span class="p">,</span>
                            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                            <span class="n">num_head_channels</span><span class="o">=</span><span class="n">dim_head</span><span class="p">,</span>
                            <span class="n">use_new_attention_order</span><span class="o">=</span><span class="n">use_new_attention_order</span><span class="p">,</span>
                        <span class="p">)</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">use_spatial_transformer</span> <span class="k">else</span> <span class="n">SpatialTransformer</span><span class="p">(</span>
                            <span class="n">ch</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">dim_head</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">transformer_depth</span><span class="p">,</span> <span class="n">context_dim</span><span class="o">=</span><span class="n">context_dim</span>
                        <span class="p">)</span>
                    <span class="p">)</span>

        <span class="c1"># Similar attention is applied in the upsampling steps</span>
</pre></div>


<h4>5. Loss funcion - loss between the model output and the target, target to optimize</h4>
<p>Steps here are:
1. Sample data from the funcion <span class="math">\(q(x)\)</span>
2. Input the sampled data and step <span class="math">\(t\)</span> into the model
3. Calc the MSE loss between the model output and the target</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">p_losses</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">default</span><span class="p">(</span><span class="n">noise</span><span class="p">,</span> <span class="k">lambda</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x_start</span><span class="p">))</span>
    <span class="n">x_noisy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q_sample</span><span class="p">(</span><span class="n">x_start</span><span class="o">=</span><span class="n">x_start</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>
    <span class="n">model_out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x_noisy</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

    <span class="n">loss_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameterization</span> <span class="o">==</span> <span class="s2">&quot;eps&quot;</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">noise</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">parameterization</span> <span class="o">==</span> <span class="s2">&quot;x0&quot;</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">x_start</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="n">f</span><span class="s2">&quot;Paramterization {self.parameterization} not yet supported&quot;</span><span class="p">)</span>

    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">model_out</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
</pre></div>


<h3>Reference</h3>
<ol>
<li><a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></li>
<li><a href="https://github.com/CompVis/latent-diffusion">Latent Diffusion Models</a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2023-10-01T00:00:00-05:00" pubdate>Sun 01 October 2023</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/ai.html">AI</a>,    <a class="category" href="/tag/llm.html">LLM</a>,    <a class="category" href="/tag/aig.html">AIG</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
      <li class="post">
          <a href="/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">GPT-1, GPT-2, GPT-3, InstructGPT / ChatGPT and GPT-4 summary</a>
      </li>
      <li class="post">
          <a href="/pages/2021/12/31/recommendation-system-05-bayesian-optimization/">Recommendation System 05 - Bayesian Optimization</a>
      </li>
      <li class="post">
          <a href="/pages/2021/12/26/recommendation-system-04-gaussian-process-regression/">Recommendation System 04 - Gaussian process regression</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2023  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/';
    var disqus_url = '/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/';
    var disqus_title = 'Image Generation 2: Latent Diffusion model / Stable Diffusion';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>