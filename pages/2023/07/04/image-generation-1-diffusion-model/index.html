<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Image Generation 1: Diffusion model &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Image Generation 1: Diffusion model</h1>
    <p class="meta">
<time datetime="2023-07-04T00:00:00-05:00" pubdate>Tue 04 July 2023</time>    </p>
</header>

  <div class="entry-content"><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script>

<p>The <a href="https://songhuiming.github.io/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">previous notes introduced the text generation models (GPT family)</a>. This reading note is about image generator papers.</p>
<p>Similar to text generator which generate the next token, OpenAI has <a href="https://openai.com/research/image-gpt">image-GPT</a> which is a large transformer trained on next pixel prediction in which the pixels are concated into a vector to build the autoregressive model. This model is time consuming as one image has lots of pixels compared to the tokens in the sentence. </p>
<p>Other models usually generate image from an embedding vector rather than pixel by pixel. These generative models usually have the input from the probability distribution (e.g., normal distribution). The sample vector is put into the image generator model to generate the image. These models include: 1) VAE; 2) Flow-based models; 3) GAN and 4) Diffusion models.</p>
<p><img alt="00_summary different image generator models" src="/figures/20230628_01_diffusion_summary_00.png" /></p>
<p><strong>VAE</strong>: VAE had an encoder to convert the input image to a vector and the decoder model to convert the vector (normal distribution) to image. </p>
<p><strong>Flow-based model</strong>: it trains an encoder model to convert the input image to a vector (normally distributed) and the encoder is invertible that can be used to convert the vector to image. To make the encoder invertible, the input and the output of the encoder mush have the same size. </p>
<p><strong>Diffusion model</strong>: diffusion model has the forward diffusion process that gradually adds Gaussian noise to an image until the image converge to Gaussian noise, and the reverse diffusion process where a neural network is trained to denoise from the Gaussian noise to an image. </p>
<p><strong>GAN</strong>: GAN consists of two simultaneously trained model, one is Generator trained to generate fake data and the other is Discriminator trained to discriminate the fake data from the real data. </p>
<table>
<thead>
<tr>
<th>Generative Models</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td><img alt="02 VAE" src="/figures/20230628_02_VAE.png" /></td>
<td><img alt="03 Flow-based model" src="/figures/20230628_03_Flow_based_model.png" /></td>
</tr>
<tr>
<td><img alt="04 GAN model" src="/figures/20230628_04_GAN.png" /></td>
<td><img alt="05 Diffusion model" src="/figures/20230628_05_Diffusion.png" /></td>
</tr>
</tbody>
</table>
<p>As most of the text to image generative models like DALLE2, ImageGen, CLIP are all based on the diffusion model, we will start from the <a href="https://arxiv.org/abs/2006.11239">diffusion model</a>.</p>
<h2>What are Diffusion Models?</h2>
<p>Diffusion Models include two steps: the first step is to add noice to the image step by step to convert the image to white noise, which is called Forward Diffusion Process; the second is to denoise from the white noise back to the image, which is called Reverse Diffusion Process. </p>
<h3>Forward diffusion process</h3>
<p>Given a data <span class="math">\(\mathbf{x}_0\)</span> sampled from the data distribution <span class="math">\(\mathbf{x}_0 \sim q(\mathbf{x})\)</span>, forward diffusion process is to add a small amount of white noise to it in <span class="math">\(1, 2, \cdots, T\)</span> steps to produce the sequence <span class="math">\(\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_T\)</span>. From step <span class="math">\(t-1\)</span> to step <span class="math">\(t\)</span>, the input is degraded by <span class="math">\(\sqrt{1-\beta_t}\)</span> and the new added noise is <span class="math">\(\sqrt{\beta_t} \boldsymbol{\epsilon}_{t-1}\)</span> where <span class="math">\(\beta_t \in (0, 1)\)</span> and <span class="math">\(\boldsymbol{\epsilon}_{t-1} \sim \mathcal{N}(0, \mathbf{I})\)</span>, that is <span class="math">\(\mathbf{x}_t = \sqrt{1-\beta_t} \mathbf{x}_{t-1} + \sqrt{\beta_t} \boldsymbol{\epsilon}_{t-1}\)</span>. In the experiment, the varience in each step is set to linearly increasing from <span class="math">\(\beta_1 = 10^{-5}\)</span> to <span class="math">\(\beta_T = 0.02\)</span>. Based on the relation from <span class="math">\(\mathbf{x}_t\)</span> to <span class="math">\(\mathbf{x}_{t-1}\)</span>, we can deduce the relation of <span class="math">\(\mathbf{x}_{t-1}\)</span> to <span class="math">\(\mathbf{x}_{t-2}\)</span> and replace <span class="math">\(\mathbf{x}_{t-1}\)</span> with <span class="math">\(\mathbf{x}_{t-2}\)</span>, and so on. After the iteration until <span class="math">\(\mathbf{x}_{0}\)</span>, we have the relation of <span class="math">\(\mathbf{x}_{t}\)</span> represented as <span class="math">\(\mathbf{x}_{0}\)</span>:</p>
<div class="math">$$\mathbf{x}_{t} = \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}$$</div>
<p> 
where <span class="math">\(\alpha_t = 1 - \beta_t\)</span> and <span class="math">\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\)</span>.</p>
<p>In python:</p>
<div class="highlight"><pre>    <span class="n">alpha</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">beta</span>
    <span class="n">alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cumprod</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">sqrt_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_hat</span><span class="p">[</span><span class="n">t</span><span class="p">])[:,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">]</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">sqrt_alpha_hat</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">sqrt_one_minus_alpha_hat</span> <span class="o">*</span> <span class="n">z</span>
</pre></div>


<h3>Reverse diffusion process</h3>
<p>This step is also called denoise as it is from the noise <span class="math">\(\mathbf{x}_{T}\)</span> back to the image <span class="math">\(\mathbf{x}_{0}\)</span>, like from <span class="math">\(\mathbf{x}_{T} \rightarrow \mathbf{x}_{t-1} \rightarrow \cdots \rightarrow \mathbf{x}_{1} \rightarrow \mathbf{x}_{0}\)</span>. That is, given <span class="math">\(\mathbf{x}_{t}\)</span>, this step is to find a noise term <span class="math">\(\boldsymbol{\epsilon}_t\)</span> so that <span class="math">\(\mathbf{x}_{t-1} = \mathbf{x}_{t} - \boldsymbol{\epsilon}_t\)</span>, Theoretially this is not doable as you need to start from the white noise <span class="math">\(\mathbf{x}_{T}\)</span> to get an image <span class="math">\(\mathbf{x}_{0}\)</span> by denoising. But is there any way to get <span class="math">\(q(\mathbf{x}_{t-1} | \mathbf{x}_{t})\)</span>? We can rewrite it based on Bayesian formula</p>
<div class="math">$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t) 
&amp;= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}) \frac{ q(\mathbf{x}_{t-1}  ) }{ q(\mathbf{x}_t  ) }
\end{aligned}
$$</div>
<p>In this way we convert <span class="math">\(q(\mathbf{x}_{t-1} \vert \mathbf{x}_t) $ to  $q(\mathbf{x}_t \vert \mathbf{x}_{t-1}),  q(\mathbf{x}_{t-1}), \text{ and } q(\mathbf{x}_t)\)</span>. The thing is, we still don't know <span class="math">\(q(\mathbf{x}_{t-1})  \text{ and } q(\mathbf{x}_t)\)</span>. But we know <span class="math">\(q(\mathbf{x}_{t-1} \vert  \mathbf{x}_0)\)</span> from fowward process. So now we will rewrite the formula to conditional on <span class="math">\(\mathbf{x}_0\)</span>, and denote it as $q(\mathbf{x}<em t-1="t-1">{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) $. That is, we want to calculate $\color{blue} {q(\mathbf{x}</em> \vert \mathbf{x}_t, \mathbf{x}_0)} $.</p>
<p>From the forward process, we know that:</p>
<div class="math">$$
\begin{aligned}
&amp; \color{blue}{q(\mathbf{x}_{t} \vert \mathbf{x}_{t-1})} \mbox{ with mean } \sqrt{{\alpha}_{t}}\mathbf{x}_{t-1} + \sqrt{1 - {\alpha}_{t}}\boldsymbol{\epsilon}   \text{    },  \sim \mathcal{N}\left(\sqrt{{\alpha}_{t}}\mathbf{x}_{t-1}, (1-{\alpha}_{t})\mathbf{I}\right) \\
&amp; \color{blue}{q(\mathbf{x}_{t-1} \vert \mathbf{x}_0)} \mbox{ with mean } \sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_{t-1}}\boldsymbol{\epsilon}  \text{    }, \sim \mathcal{N}\left(\sqrt{\bar{\alpha}_{t-1}}\mathbf{x}_0 , (1-\bar{\alpha}_{t-1})\mathbf{I}\right) \\
&amp; \color{blue}{q(\mathbf{x}_{t} \vert \mathbf{x}_0)} \mbox{ with mean } \sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}  \text{    },  \sim \mathcal{N}\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I}\right) 
\end{aligned}
$$</div>
<p>From the Bayesian formula, we have:</p>
<div class="math">$$
\begin{aligned}
q(\mathbf{x}_{t-1} \vert \mathbf{x}_t, \mathbf{x}_0) 
&amp;= q(\mathbf{x}_t \vert \mathbf{x}_{t-1}, \mathbf{x}_0) \frac{ q(\mathbf{x}_{t-1} \vert \mathbf{x}_0) }{ q(\mathbf{x}_t \vert \mathbf{x}_0) } \\
&amp;\propto \exp \Big(-\frac{1}{2} \big(\frac{\left(\mathbf{x}_t - \sqrt{\alpha_t} \mathbf{x}_{t-1}\right)^2}{\beta_t} + \frac{(\mathbf{x}_{t-1} - \sqrt{\bar{\alpha}_{t-1}} \mathbf{x}_0)^2}{1-\bar{\alpha}_{t-1}} - \frac{(\mathbf{x}_t - \sqrt{\bar{\alpha}_t} \mathbf{x}_0)^2}{1-\bar{\alpha}_t} \big) \Big) \\
&amp;= \exp\Big( -\frac{1}{2} \big({\left(\frac{\alpha_t}{\beta_t} + \frac{1}{1 - \bar{\alpha}_{t-1}}\right)} \mathbf{x}_{t-1}^2 -  {\left(\frac{2\sqrt{\alpha_t}}{\beta_t} \mathbf{x}_t + \frac{2\sqrt{\bar{\alpha}_{t-1}}}{1 - \bar{\alpha}_{t-1}} \mathbf{x}_0\right)} \mathbf{x}_{t-1}  { + C(\mathbf{x}_t, \mathbf{x}_0) \big) \Big)}
\end{aligned}
$$</div>
<p>This is the kernel funciton for normal distribution with the mean equals to:</p>
<div class="math">$$
\begin{aligned}
\tilde{\boldsymbol{\mu}}_t
&amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \mathbf{x}_0,  \text{    and   } \mathbf{x}_0 = \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&amp;= \frac{\sqrt{\alpha_t}(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t} \mathbf{x}_t + \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t} \frac{1}{\sqrt{\bar{\alpha}_t}}(\mathbf{x}_t - \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}_t) \\
&amp;= {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}  \boldsymbol{\epsilon}_t \Big)}
\end{aligned}
$$</div>
<p>Here <span class="math">\(\boldsymbol{\epsilon}_t\)</span> is not known, it will be learned from the forward process. In forward process, from <span class="math">\(\mathbf{x}_t\)</span> to  <span class="math">\(\mathbf{x}_{t+1}\)</span> by adding a noise <span class="math">\(\boldsymbol{\epsilon}_t\)</span>. The true value is from the difference of the input, and A NN model (in the paper, it is UNet) will be built to learn this added noice. </p>
<p>In python:</p>
<div class="highlight"><pre>    <span class="n">predicted_noise</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha_hat</span><span class="p">)))</span> <span class="o">*</span> <span class="n">predicted_noise</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">beta</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>
</pre></div>


<p><img alt="06 forward reverse process" src="/figures/20230628_06_forward_reverse_process.png" /></p>
<h3>Algorithm</h3>
<p><img alt="Diffusion Model DDPM" src="/figures/20230628_07_Diffusion_DDPM_algorithm.png" /></p>
<p><strong>Training</strong>: Combine the forward and reverse process together, in the training process, step <span class="math">\(t\)</span> in the forward process will generate the true value of the noise for that given step, and the reverse process will try to build a neural network model to predict the denoised term, which should be close to the noise term in the froward process. Because for any step <span class="math">\(t\)</span>, it needs to learn the noise so the step <span class="math">\(t\)</span> will also be an input to the model.</p>
<p>In all, there are two inputs to the initial step: a given image <span class="math">\(\mathbf{x}_{0}\)</span> and the step <span class="math">\(t\)</span>. It will learn a neural network (U-net in the original paper) <span class="math">\(\boldsymbol{\epsilon}_{\theta}\)</span> to approxiate the added Gaussian noise where <span class="math">\(\boldsymbol{\epsilon}_{\theta} = \boldsymbol{\epsilon}_{\theta}\left(\sqrt{\bar{\alpha}_t}\mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t}\boldsymbol{\epsilon}, t  \right)\)</span>.</p>
<p><strong>Sampling</strong>: starting from the Gaussian noise <span class="math">\(\mathbf{x}_{T} \sim \mathcal{N}(0, \mathbf{I})\)</span>, for each step <span class="math">\(t\)</span>, calculate the denoised term from the trained neural network <span class="math">\(\boldsymbol{\epsilon}_{\theta}(\mathbf{x}_{t}, t)\)</span> and then denoise from step <span class="math">\(t\)</span> to step <span class="math">\(t-1\)</span> through <span class="math">\(\mathbf{x}_{t-1} = {\frac{1}{\sqrt{\alpha_t}} \Big( \mathbf{x}_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}}  \boldsymbol{\epsilon}_{\theta}(\mathbf{x}_t, t) \Big)} + \sigma_t \mathbf{z}\)</span> where <span class="math">\(\mathbf{z}\)</span> is Gaussian noise for <span class="math">\(t&gt;1\)</span> else 0.</p>
<h2>Stable diffusion</h2>
<p>paper link: <a href="https://arxiv.org/abs/2112.10752">High-Resolution Image Synthesis with Latent Diffusion Models</a></p>
<h2>Reference</h2>
<ol>
<li><a href="https://vsehwag.github.io/blog/2023/2/all_papers_on_diffusion.html">All papers on diffusion</a></li>
</ol>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }
    
    var mathjaxscript = document.createElement('script');
    var location_protocol = (false) ? 'https' : document.location.protocol;
    if (location_protocol !== 'http' && location_protocol !== 'https') location_protocol = 'https:';
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript'; 
    mathjaxscript.src = location_protocol + '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js','color.js','mhchem.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2023-07-04T00:00:00-05:00" pubdate>Tue 04 July 2023</time>  <span class="categories">
    <a class='category' href='/category/python.html'>python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/ai.html">AI</a>,    <a class="category" href="/tag/llm.html">LLM</a>,    <a class="category" href="/tag/aig.html">AIG</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2023/07/04/image-generation-1-diffusion-model/">Image Generation 1: Diffusion model</a>
      </li>
      <li class="post">
          <a href="/pages/2023/05/28/gpt-1-gpt-2-gpt-3-instructgpt-chatgpt-and-gpt-4-summary/">GPT-1, GPT-2, GPT-3, InstructGPT / ChatGPT and GPT-4 summary</a>
      </li>
      <li class="post">
          <a href="/pages/2021/12/31/recommendation-system-05-bayesian-optimization/">Recommendation System 05 - Bayesian Optimization</a>
      </li>
      <li class="post">
          <a href="/pages/2021/12/26/recommendation-system-04-gaussian-process-regression/">Recommendation System 04 - Gaussian process regression</a>
      </li>
      <li class="post">
          <a href="/pages/2021/12/22/zhi-chang-hua-ti-guan-yu-zhi-chang-communications-skillde-yi-dian-gan-xiang-zhuan-zai/">职场话题——关于职场communications skill的一点感想 (转载)</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2023  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2023/07/04/image-generation-1-diffusion-model/';
    var disqus_url = '/pages/2023/07/04/image-generation-1-diffusion-model/';
    var disqus_title = 'Image Generation 1: Diffusion model';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>