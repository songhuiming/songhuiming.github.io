<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Credit Card Fraud Detection / Imbalanced data modeling - Part II: Random Forest &mdash; pydata: Huiming's learning notes</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><header>
  <h1><a href="/">pydata: Huiming's learning notes</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</header>

<script>
window.MathJax = {
  tex: {
    inlineMath: [['$', '$'], ['\\(', '\\)']],  // 允许 $...$ 和 \( ... \) 作为行内公式
    displayMath: [['$$', '$$'], ['\\[', '\\]']],  // 允许 $$...$$ 和 \[ ... \] 作为块级公式
    processEscapes: true,  // 允许在公式中使用转义符，如 \$ 表示美元符号
    processEnvironments: true  // 允许解析 \begin{equation} ... \end{equation} 等数学环境
  },
  options: {
    skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre'],  // 跳过某些 HTML 标签，防止错误解析
    renderActions: {
      addMenu: []  // 移除右键菜单
    }
  }
};

window.addEventListener('load', () => {
  document.querySelectorAll("mjx-container").forEach(x => {
    x.parentElement.classList.add('has-jax');  // 使用 classList.add() 避免字符串拼接错误
  });
});
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script></header>
  <nav role="navigation">

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Credit Card Fraud Detection / Imbalanced data modeling - Part II: Random Forest</h1>
    <p class="meta">
<time datetime="2018-05-12T18:08:00-05:00" pubdate>Sat 12 May 2018</time>    </p>
</header>

  <div class="entry-content"><h1>Table of Contents<span class="tocSkip"></span></h1>
<div class="toc"><ul class="toc-item"><li><span><a href="#0.-Introduction" data-toc-modified-id="0.-Introduction-1">0. Introduction</a></span><ul class="toc-item"><li><span><a href="#Conclusion:" data-toc-modified-id="Conclusion:-1.1">Conclusion:</a></span></li></ul></li><li><span><a href="#Modeling-Part-2:-RandomForestClassifier" data-toc-modified-id="Modeling-Part-2:-RandomForestClassifier-2">Modeling Part 2: RandomForestClassifier</a></span></li><li><span><a href="#1.-Use-the-Imbalanced-Data-Directly-in-RandomForestClassifier" data-toc-modified-id="1.-Use-the-Imbalanced-Data-Directly-in-RandomForestClassifier-3">1. Use the Imbalanced Data Directly in RandomForestClassifier</a></span></li><li><span><a href="#2.-Create-Over-sampling-data-and-Fit-the-model" data-toc-modified-id="2.-Create-Over-sampling-data-and-Fit-the-model-4">2. Create Over-sampling data and Fit the model</a></span></li><li><span><a href="#3.-RandomForestClassifier-with-class_weight" data-toc-modified-id="3.-RandomForestClassifier-with-class_weight-5">3. RandomForestClassifier with class_weight</a></span></li><li><span><a href="#4.-Self-defined-Score-and-GridSearchCV-of-hyperparameter" data-toc-modified-id="4.-Self-defined-Score-and-GridSearchCV-of-hyperparameter-6">4. Self-defined Score and GridSearchCV of hyperparameter</a></span></li></ul></div>

<h1>0. Introduction</h1>
<p>It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.</p>
<p>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p>
<p><a href="http://songhuiming.github.io/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/">In the first part, Loigstic regression model was built do different kind of analysis</a>. In this part, we will try Random Forest models.Since this is imbalanced data, we will try different methods and compare their results:</p>
<pre><code>1. Model on imbalanced data directly
2. Model on over-sampling data
3. Assign more weights on rare class
4. Use customed loss function
</code></pre>
<h2>Conclusion:</h2>
<ol>
<li>As expected, use the imbalanced data is not a good way. The performance is the worst compaed to using over-sampling or class weights</li>
<li>Use imbalanced data, RandomForestClassifier result is better than LogisticRegression. </li>
<li>If we can custom a good loss function, the model performance will be better: here the customed loss function performance is better than roc_auc scoring function.</li>
</ol>
<pre><code class="language-python">import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler, Normalizer, scale
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier
from sklearn.svm import SVC, LinearSVC
from sklearn.metrics import confusion_matrix, log_loss, auc, roc_curve, roc_auc_score, recall_score, precision_recall_curve
from sklearn.metrics import make_scorer, precision_score, fbeta_score, f1_score, classification_report
from sklearn.model_selection import cross_val_score, train_test_split, KFold, StratifiedShuffleSplit, GridSearchCV
from sklearn.linear_model import LogisticRegression

%matplotlib inline
plt.rcParams['figure.figsize'] = (16, 9)

seed = 999

creditcard = pd.read_csv('creditcard.csv')
creditcard.columns = [x.lower() for x in creditcard.columns]
creditcard.rename(columns = {'class': 'fraud'}, inplace = True)
</code></pre>
<pre><code class="language-python"># 1. Split Test Data Out
creditcard.drop(columns = 'time', inplace = True)

# Normalize the 'amount' column
scaler = StandardScaler()
creditcard['amount'] = scaler.fit_transform(creditcard['amount'].values.reshape(-1, 1))
# creditcard.drop(columns = 'amount', inplace = True)

X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)
</code></pre>
<p>The reason why I check this:</p>
<p>For non-fraud transactions, the average amount is 88. For fraud transactions, the average amount is 122.
So, in average there will be 122 loss for a fraud. Suppose for each transaction, the company can get 2% transaction fee. 
That is, the average is 88 * 2% = 1.76. </p>
<p>That means: if we predict a non-fraud as fraud, we might loss 1.76. However, if we miss to detect a fraud transaction, we will
loss about 122.</p>
<p>Later I will use this to build a customed loss function.</p>
<h1>Modeling Part 2: RandomForestClassifier</h1>
<p>Usually for imbalanced data, we can try:</p>
<pre><code>1. Collect more data (which not work here since the data is given)
2. Down-Sampling or Over-Sampling to get balanced samples
3. Change the Thresholds to adjust the prediction
4. Assign class weights for the low rate class
</code></pre>
<p>Here we will try 4 different ways and compare their results:</p>
<pre><code>2.1. Do nothing, use original data to model
2.2. Do Over-Sampling, use the over-sampled data to model
2.3. Assigning sample weights in RandomForestClassifier
2.4. Use customed loss function
</code></pre>
<p>Since this is Fraud detection question, if we miss predicting a fraud, the credit company will lose a lot. If we miss predicting a normal transaction as Fraud, we can still let the exprt to review the transactions or we can ask the user to verify the transaction. So in this specific case, False Positive will cause more loss than False Negative.  </p>
<h1>1. Use the Imbalanced Data Directly in RandomForestClassifier</h1>
<pre><code class="language-python">X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&quot;max_depth&quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain, ytrain)

# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.65
print(precision_score(ytest, pred_test))  # 0.85
print(roc_auc_score(ytest, pred_test))    # 0.83
print(&quot;confustion matrix on validation data: \n&quot; + str(confusion_matrix(ytest, pred_test)))
</code></pre>
<p>confustion matrix on validation data: </p>
<pre><code>[[93807    18]
 [   57   105]]
</code></pre>
<p>If we use the imbalanced data directly in the RandomForestClassifier, we will find the result is not very good: recall score is 0.65 and the auc = 0.83. Although this result is better than the result from Logistic Regression using imbalanced data directly. To improve the model performance, we will try two methods: over-sampling and assigning more weights to rare class.</p>
<h1>2. Create Over-sampling data and Fit the model</h1>
<pre><code class="language-python">oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&quot;max_depth&quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain_oversample, ytrain_oversample)

# print cv_grid.best_params_
# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.83
print(precision_score(ytest, pred_test))  # 0.83
print(roc_auc_score(ytest, pred_test))    # 0.92
print(&quot;\n confustion matrix on validation data: \n&quot; + str(confusion_matrix(ytest, pred_test)))
</code></pre>
<pre><code>[[93798    27]
 [   27   135]]
</code></pre>
<p>By using over-sampling, we can find the model performance is improved a lot. recall score = 0.83 now and the auc = 0.92
From the confusion matrix, 135 frauds from 162 True frauds are detected. There are 27 non-frauds are mistakenly predicted
as frauds. We can do fine-tuning by changeing the thresholds to get less false negatives. The price will be getting more
false positives.</p>
<p>Next we will test using class_weights rather than over-sampling. We know if Logistic Regression these two are equivalent.
We will have a try to see what will happen for RandomForest?</p>
<h1>3. RandomForestClassifier with class_weight</h1>
<p>Rather than over-sampling, we can assign more weights to the lower rate class.</p>
<pre><code class="language-python">
X = creditcard.iloc[:, :-1]
y = creditcard.iloc[:, -1]
Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size = .33, stratify = y, random_state = seed)

positive_weight = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1

estimator = RandomForestClassifier(random_state=0, class_weight = {0 : 1, 1 : positive_weight}, warm_start = True)

rf_tuned_parameters = {&quot;max_depth&quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = 'roc_auc', verbose = 5, n_jobs = 70) # 'recall', my_score
cv_grid.fit(Xtrain, ytrain)

# print cv_grid.cv_results_

best_parameters = cv_grid.best_estimator_.get_params()

# for param_name in sorted(rf_tuned_parameters.keys()):
#     print(&quot;\t%s: %r&quot; % (param_name, best_parameters[param_name]))

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     #  0.85
print(precision_score(ytest, pred_test))  #  0.81
print(roc_auc_score(ytest, pred_test))    #  0.92
print(&quot;\n confustion matrix on validation data: \n&quot; + str(confusion_matrix(ytest, pred_test)))

</code></pre>
<pre><code>[[93793    32]
 [   25   137]]
</code></pre>
<p>Compared with over-sampling, the recall score increased from 0.83 to 0.85, while the precision decreases from 0.83 to 0.81.
The auc is 0.92, which is close the the result of over-sampling.</p>
<p>Overall, I think this model works well: 85% of the frauds can be detected by this model, which will prevent a lot of loss.
At the same time, only 0.03% of the non-frauds will be mistakenly predicted as frauds. This will result in very little potential
loss for the company. The company can also do manual review of these false fraud detections.</p>
<h1>4. Self-defined Score and GridSearchCV of hyperparameter</h1>
<p>Since the loss from frauds and false predicted frauds are different for us. We will define a function to re-weight the 
effects by average loss from missing predicted frauds and falsely predicted frauds.</p>
<pre><code class="language-python">def scoring(ground_truth, predictions):
    '''
    based on results above about the average loss from false positive and false negative predictions.
    '''
    cmatrix = confusion_matrix(ground_truth, predictions)
    fp = cmatrix[0, 1]
    fn = cmatrix[1, 0]
    return  fn * 122 + fp * 1.76

wt_loss_score = make_scorer(scoring, greater_is_better = False)

oversample_ratio = sum(ytrain == 0) / sum(ytrain == 1)  # size to repeat y == 1
# repeat the positive data for X and y
ytrain_pos_oversample = pd.concat([ytrain[ytrain==1]] * oversample_ratio, axis = 0)
Xtrain_pos_oversample = pd.concat([Xtrain.loc[ytrain==1, :]] * oversample_ratio, axis = 0)
# concat the repeated data with the original data together
ytrain_oversample = pd.concat([ytrain, ytrain_pos_oversample], axis = 0).reset_index(drop = True)
Xtrain_oversample = pd.concat([Xtrain, Xtrain_pos_oversample], axis = 0).reset_index(drop = True)

ytrain_oversample.value_counts(dropna = False, normalize = True)   # 50:50

estimator = RandomForestClassifier(random_state=0, warm_start = True)

rf_tuned_parameters = {&quot;max_depth&quot;: [10, 20, 50, 100], 'n_estimators': [50, 100, 200, 500], 
                       'min_samples_leaf': [10, 20, 50]}

cv_grid = GridSearchCV(estimator, param_grid = rf_tuned_parameters, scoring = wt_loss_score, verbose = 5, n_jobs = 70)
cv_grid.fit(Xtrain_oversample, ytrain_oversample)

# print cv_grid.best_params_

pred_test = cv_grid.predict(Xtest)
print(recall_score(ytest, pred_test))     # 0.84
print(precision_score(ytest, pred_test))  # 0.84
print(roc_auc_score(ytest, pred_test))    # 0.92
print(&quot;\n confustion matrix on validation data: \n&quot; + str(confusion_matrix(ytest, pred_test)))

</code></pre>
<p>With the self defined loss function, the confusion matrix is:</p>
<pre><code>[[93800    25]
 [   26   136]]
</code></pre>
<p>Compared to the same setup but using 'roc_auc' as the scoring function results:</p>
<pre><code>[[93798    27]
 [   27   135]]
</code></pre></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2018-05-12T18:08:00-05:00" pubdate>Sat 12 May 2018</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/data-mining.html">data mining</a>,    <a class="category" href="/tag/sklearn.html">sklearn</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2025/04/20/deepseek-expert-parallelism-load-balancer-eplb-code-reading/">DeepSeek Expert Parallelism Load Balancer (EPLB) Code Reading</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/23/deepseek-v3-learning-notes/">DeepSeek V3 learning notes</a>
      </li>
      <li class="post">
          <a href="/pages/2025/02/16/deepseek-v3/">DeepSeek V3</a>
      </li>
      <li class="post">
          <a href="/pages/2024/04/21/prediction-in-decoder-and-kv-cache/">Prediction in decoder and KV-Cache</a>
      </li>
      <li class="post">
          <a href="/pages/2023/10/01/image-generation-2-latent-diffusion-model-stable-diffusion/">Image Generation 2: Latent Diffusion model / Stable Diffusion</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/python.html">python</a>,    <a href="/tag/ai.html">AI</a>,    <a href="/tag/llm.html">LLM</a>,    <a href="/tag/agi.html">AGI</a>,    <a href="/tag/aig.html">AIG</a>,    <a href="/tag/gpt.html">GPT</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/sklearn.html">sklearn</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/re.html">re</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2025  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/';
    var disqus_url = '/pages/2018/05/12/credit-card-fraud-detection-imbalanced-data-modeling-part-ii-random-forest/';
    var disqus_title = 'Credit Card Fraud Detection / Imbalanced data modeling - Part II: Random Forest';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>