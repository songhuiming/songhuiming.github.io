<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression &mdash; pydata</title>
  <meta name="author" content="shm">






  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">


    <link href="/favicon.png" rel="icon">

  <link href="/theme/css/main.css" media="screen, projection"
        rel="stylesheet" type="text/css">

  <link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
  <link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic"
        rel="stylesheet" type="text/css">
</head>

<body>
  <header role="banner"><hgroup>
  <h1><a href="/">pydata</a></h1>
    <h2>Keep Looking, Don't Settle</h2>
</hgroup>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
        MathJax.Hub.Config({
            config: ["MMLorHTML.js"],
            extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
            jax: ["input/TeX"],
            tex2jax: {
                inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                processEscapes: false
            },
            TeX: {
                TagSide: "right",
                TagIndent: ".8em",
                MultLineWidth: "85%",
                equationNumbers: {
                   autoNumber: "AMS",
                },
                unicode: {
                   fonts: "STIXGeneral,'Arial Unicode MS'"
                }
            },
            showProcessingMessages: false
        });
</script></header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
</ul>

<form class="search" action="/search.html">
    <input type="text" class="search-query" placeholder="Search" name="q" id="s">
</form>

<form action="https://www.google.com/search" method="get">
    <fieldset role="search">
       <input type="hidden" name="q" value="site:songhuiming.github.io" />
       <input class="search" type="text" name="q" results="0" placeholder="Search"/>
    </fieldset>
</form>


<ul class="main-navigation">
    <li><a href="/functions/archives.html">Archives</a></li>
      <li >
        <a href="/category/career-growth.html">Career growth</a>
      </li>
      <li >
        <a href="/category/linux.html">Linux</a>
      </li>
      <li class="active">
        <a href="/category/python.html">Python</a>
      </li>
      <li >
        <a href="/category/rthers.html">Rthers</a>
      </li>
</ul></nav>
  <div id="main">
    <div id="content">
<div>
  <article class="hentry" role="article">
<header>
      <h1 class="entry-title">Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression</h1>
    <p class="meta">
<time datetime="2018-05-05T18:08:00-05:00" pubdate>Sat 05 May 2018</time>    </p>
</header>

  <div class="entry-content"><h1>Table of Contents<span class="tocSkip"></span></h1>

<div class="toc"><ul class="toc-item"><li><span><a href="#0.-Introduction" data-toc-modified-id="0.-Introduction-1">0. Introduction</a></span></li><li><span><a href="#1.-Read-in-data-and-the-libraries" data-toc-modified-id="1.-Read-in-data-and-the-libraries-2">1. Read in data and the libraries</a></span></li><li><span><a href="#Modeling-Part-I:-Logistic-Regression-method" data-toc-modified-id="Modeling-Part-I:-Logistic-Regression-method-3">Modeling Part I: Logistic Regression method</a></span><ul class="toc-item"><li><span><a href="#Preprocess-data" data-toc-modified-id="Preprocess-data-3.1">Preprocess data</a></span></li></ul></li><li><span><a href="#1.-Model-imbalanced-data-directly" data-toc-modified-id="1.-Model-imbalanced-data-directly-4">1. Model imbalanced data directly</a></span><ul class="toc-item"><li><span><a href="#1.1.-Change-the-Thresholds" data-toc-modified-id="1.1.-Change-the-Thresholds-4.1">1.1. Change the Thresholds</a></span></li></ul></li><li><span><a href="#2.-Create-Over-sampling-data-and-Fit-the-model" data-toc-modified-id="2.-Create-Over-sampling-data-and-Fit-the-model-5">2. Create Over-sampling data and Fit the model</a></span><ul class="toc-item"><li><span><a href="#2.1-Change-the-Thresholds" data-toc-modified-id="2.1-Change-the-Thresholds-5.1">2.1 Change the Thresholds</a></span></li></ul></li><li><span><a href="#3.-Logistic-Regression-with-class_weight" data-toc-modified-id="3.-Logistic-Regression-with-class_weight-6">3. Logistic Regression with class_weight</a></span><ul class="toc-item"><li><span><a href="#Reference" data-toc-modified-id="Reference-6.1">Reference</a></span></li></ul></li></ul></div>

<div id="toc"></div>

<h1>0. Introduction</h1>
<p>It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.</p>
<p>The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.</p>
<p>It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.</p>
<h1>1. Read in data and the libraries</h1>
<p>The data is in good shape, that is, there is no missing. </p>
<ol>
<li>Data Clean (missing impute / outliers / normalize)</li>
<li>Feature Engineering (Categorical variables, transformation, correlation analysis)</li>
</ol>
<p>The provided data is imbalanced, with positive rate around 0.17%.</p>
<p>If we use this data directly to feed the model, the model will prefer to predict all as 0 for a high accuracy of 0 prediction. </p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">Normalizer</span><span class="p">,</span> <span class="n">scale</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span><span class="p">,</span> <span class="n">AdaBoostClassifier</span><span class="p">,</span> <span class="n">GradientBoostingClassifier</span><span class="p">,</span> <span class="n">ExtraTreesClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span><span class="p">,</span> <span class="n">LinearSVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">log_loss</span><span class="p">,</span> <span class="n">auc</span><span class="p">,</span> <span class="n">roc_curve</span><span class="p">,</span> <span class="n">roc_auc_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">precision_recall_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">fbeta_score</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">StratifiedShuffleSplit</span><span class="p">,</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">9</span><span class="p">)</span>

<span class="n">seed</span> <span class="o">=</span> <span class="mi">999</span>

<span class="n">creditcard</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;creditcard.csv&#39;</span><span class="p">)</span>
<span class="n">creditcard</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">creditcard</span><span class="o">.</span><span class="n">columns</span><span class="p">]</span>
<span class="n">creditcard</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;class&#39;</span><span class="p">:</span> <span class="s1">&#39;fraud&#39;</span><span class="p">},</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
</pre></div>


<p>Imbalanced data with very low proportion of positive signals</p>
<div class="highlight"><pre><span class="n">creditcard</span><span class="o">.</span><span class="n">fraud</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="bp">False</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>0    284315
1       492
</pre></div>


<p>There are 284315 rows (99.8%) with y = 0, and only 492 rows (0.172%) with y = 1. So it is very imbalanced data. </p>
<p>Usually we have these methods to deal with imbalanced data:
1. Collect more data
2. Over-Sampling or Down-Sampling
3. Change the prediction thresholds
4. Assign weights</p>
<p>Here we will do two things: </p>
<ol>
<li>
<p>Use LogisticRegression directly to model the data; </p>
</li>
<li>
<p>Over-sampling the data to get a balanced proportion of positive/negative values</p>
</li>
</ol>
<p>Before oversampling, we will first take a random sample as Test data.</p>
<div class="highlight"><pre><span class="n">creditcard</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s1">&#39;fraud&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">amount</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>


<div class="highlight"><pre>fraud
0     88.291022
1    122.211321
</pre></div>


<p>The reason why I check this:</p>
<p>For non-fraud transactions, the average amount is 88. For fraud transactions, the average amount is 122.
So, in average there will be 122 loss for a fraud. Suppose for each transaction, the company can get 2% transaction fee. 
That is, the average is 88*2% = 1.76. </p>
<p>That means: if we predict a non-fraud as fraud, we might loss 1.76. However, if we miss to detect a fraud transaction, we will
loss about 122.</p>
<p>Later I will use this to build a self-defined loss function.</p>
<h1>Modeling Part I: Logistic Regression method</h1>
<p>Usually for imbalanced data, we can try:</p>
<div class="highlight"><pre>1. Collect more data (which not work here since the data is given)
2. Down-Sampling or Over-Sampling to get balanced samples
3. Change the Thresholds to adjust the prediction
4. Assign class weights for the low rate class
</pre></div>


<p>Here we will try 5 different ways and compare their results:</p>
<div class="highlight"><pre>2.1. Do nothing, use original data to model
2.2. Do Over-Sampling, use the over-sampled data to model
2.3. Change the threshold to selected value, rather than using default 0.5
2.4. Assigning sample weights in Logistic Regression
2.5. Change the performance metric, like using ROC, f1-score rather than using accuracy
</pre></div>


<p>Since this is Fraud detection question, if we miss predicting a fraud, the credit company will lose a lot. If we miss predicting a normal transaction as Fraud, we can still let the exprt to review the transactions or we can ask the user to verify the transaction. So in this specific case, False Positive will cause more loss than False Negative.  </p>
<h2>Preprocess data</h2>
<div class="highlight"><pre><span class="c1"># 1. Split Test Data Out</span>
<span class="n">creditcard</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="s1">&#39;time&#39;</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># Normalize the &#39;amount&#39; column</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">creditcard</span><span class="p">[</span><span class="s1">&#39;amount&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">creditcard</span><span class="p">[</span><span class="s1">&#39;amount&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="c1"># creditcard.drop(columns = &#39;amount&#39;, inplace = True)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">creditcard</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">creditcard</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="o">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
</pre></div>


<h1>1. Model imbalanced data directly</h1>
<p>We will use the imbalanced data directly in logistic regression. That is, the positive rate is about 0.172%. Accuracy is not good since if all predicted as 0, the accuracy for 0 is very high. So, here <code>recall</code>, <code>precision</code>, <code>roc</code> and <code>confusion_matrix</code> are listed to compare model performance.</p>
<div class="highlight"><pre><span class="c1"># 2. If we don&#39;t do Over-sampling, what will happen?</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">creditcard</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">creditcard</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="o">.</span><span class="mi">33</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>

<span class="n">logitreg_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))}</span>
<span class="n">logitreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">warm_start</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">logitreg_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logitreg</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">logitreg_parameters</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">logitreg_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>   <span class="c1"># logitreg_grid.best_params_ ; logitreg_grid.best_estimator_</span>

<span class="c1"># on OVER-Sampled TRAINing data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Thre recall score on Training data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytrain</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">))</span>    <span class="c1"># 0.58</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Thre precision score on Training data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytrain</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">))</span>   <span class="c1"># 0.89</span>

<span class="c1"># on the separated TEST data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Thre recall score on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1"># 0.58</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Thre precision score on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1"># 0.86</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Thre Confusion Matrix on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span class="k">[LibLinear]</span>
 <span class="err">Thre</span> <span class="err">recall</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Training</span> <span class="err">data</span> <span class="err">is:</span>
<span class="err">0.584848484848</span>

 <span class="err">Thre</span> <span class="err">precision</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Training</span> <span class="err">data</span> <span class="err">is:</span>
<span class="err">0.893518518519</span>

 <span class="err">Thre</span> <span class="err">recall</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is:</span>
<span class="err">0.586419753086</span>

 <span class="err">Thre</span> <span class="err">precision</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is:</span>
<span class="err">0.863636363636</span>

 <span class="err">Thre</span> <span class="err">Confusion</span> <span class="err">Matrix</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is:</span>
<span class="k">[[93810    15]</span>
 <span class="k">[   67    95]]</span>
</pre></div>


<p>Conclusions:</p>
<p>From the output above, we know on the training data, the recall score is 0.58 which means 58 over 100 of the 
True positive conditions are predicted correctly. And 89 over 100 of the predicted positives are True Positive.</p>
<p>On the Test data, the model performance metric evalued by recall or precision are close to the Training data.
There is a precision score of 0.86 on the Test data, which means 86 out of 100 predicted positives are True positives.</p>
<p>From Confusion Matrix, 95 of 162 True Positives are predicted as positives. And of all 110 predicted as positive, 95 of 
them are True positives.</p>
<h2>1.1. Change the Thresholds</h2>
<div class="highlight"><pre><span class="n">ytrain_pred_probas</span> <span class="o">=</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>   <span class="c1"># prob of predict as 1</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">ytrain</span><span class="p">,</span> <span class="n">ytrain_pred_probas</span><span class="p">)</span>   <span class="c1"># precision_recall_curve</span>
<span class="n">roc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;FPR&#39;</span><span class="p">:</span><span class="n">fpr</span><span class="p">,</span><span class="s1">&#39;TPR&#39;</span><span class="p">:</span><span class="n">tpr</span><span class="p">,</span><span class="s1">&#39;Thresholds&#39;</span><span class="p">:</span><span class="n">thresholds</span><span class="p">})</span>
</pre></div>


<div class="highlight"><pre><span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">roc</span><span class="o">.</span><span class="n">FPR</span><span class="p">,</span> <span class="n">roc</span><span class="o">.</span><span class="n">TPR</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;#00C851&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;FPR&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;TPR&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="/figures/20180505_creditCardFraudDetection_01.png" /></p>
<p>By default, the threshold is 0.5. Since the recall score is low, we shall lower the threshold to get more predicted as Positive. 
At the same time, more True Negative data will be falsely predicted as Positive. So the Precision score will be lower.</p>
<div class="highlight"><pre><span class="n">ytest_pred_probas</span> <span class="o">=</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">new_threshold</span> <span class="o">=</span> <span class="mf">0.1</span>   <span class="c1"># 0.5 is the default value</span>
<span class="n">ytest_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">ytest_pred_probas</span> <span class="o">&gt;=</span> <span class="n">new_threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;After change threshold to 0.1, the recall socre on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>     <span class="c1"># 0.827</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After change threshold to 0.1, the precision socre on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>  <span class="c1"># 0.812</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">After change threshold to 0.1, the Confusion Matrix on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>After change threshold to 0.1, the recall socre on Test data is:
0.827160493827

 After change threshold to 0.1, the precision socre on Test data is:
0.812121212121

After change threshold to 0.1, the Confusion Matrix on Test data is:
[[93794    31]
 [   28   134]]
</pre></div>


<p>If we lower the threshold to 0.1, we will get recall rate of 0.827. That is, 134 of 162 True Frauds will be detected while
only 31 on 165 predicted Frauds are not True Frauds.</p>
<p>If we lower the threshold to 0.01, then the recall score will be 0.91 while the precision score is 0.1.</p>
<h1>2. Create Over-sampling data and Fit the model</h1>
<p>Since there are much more samples </p>
<div class="highlight"><pre><span class="n">oversample_ratio</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ytrain</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ytrain</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># size to repeat y == 1</span>
<span class="c1"># repeat the positive data for X and y</span>
<span class="n">ytrain_pos_oversample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ytrain</span><span class="p">[</span><span class="n">ytrain</span><span class="o">==</span><span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="n">oversample_ratio</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">Xtrain_pos_oversample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Xtrain</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">ytrain</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]]</span> <span class="o">*</span> <span class="n">oversample_ratio</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="c1"># concat the repeated data with the original data together</span>
<span class="n">ytrain_oversample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">ytrain</span><span class="p">,</span> <span class="n">ytrain_pos_oversample</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">Xtrain_oversample</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtrain_pos_oversample</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">ytrain_oversample</span><span class="o">.</span><span class="n">value_counts</span><span class="p">(</span><span class="n">dropna</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>   <span class="c1"># 50:50</span>

<span class="n">logitreg_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))}</span>
<span class="n">logitreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">warm_start</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">logitreg_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logitreg</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">logitreg_parameters</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">logitreg_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain_oversample</span><span class="p">,</span> <span class="n">ytrain_oversample</span><span class="p">)</span>   <span class="c1"># logitreg_grid.best_params_ ; logitreg_grid.best_estimator_</span>

<span class="c1"># on OVER-Sampled TRAINing data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Over-Sampling, the recall score on Training data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytrain_oversample</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain_oversample</span><span class="p">))</span>    <span class="c1"># 0.918</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Over-Sampling, the precision score on Training data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytrain_oversample</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain_oversample</span><span class="p">))</span>   <span class="c1"># 0.971</span>

<span class="c1"># on the separated TEST data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Over-Sampling, the recall score on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1">#  0.932</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Over-Sampling, the precision score on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1"># 0.056</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After Over-Sampling, the Confusion Matrix on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre><span class="k">[LibLinear]</span>
 <span class="err">After</span> <span class="err">Over-Sampling,</span> <span class="err">the</span> <span class="err">recall</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Training</span> <span class="err">data</span> <span class="err">is</span>
<span class="err">0.918181818182</span>

 <span class="err">After</span> <span class="err">Over-Sampling,</span> <span class="err">the</span> <span class="err">precision</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Training</span> <span class="err">data</span> <span class="err">is</span>
<span class="err">0.971089227494</span>

 <span class="err">After</span> <span class="err">Over-Sampling,</span> <span class="err">the</span> <span class="err">recall</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is</span>
<span class="err">0.932098765432</span>

 <span class="err">After</span> <span class="err">Over-Sampling,</span> <span class="err">the</span> <span class="err">precision</span> <span class="err">score</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is</span>
<span class="err">0.0562383612663</span>

 <span class="err">After</span> <span class="err">Over-Sampling,</span> <span class="err">the</span> <span class="err">Confusion</span> <span class="err">Matrix</span> <span class="err">on</span> <span class="err">Test</span> <span class="err">data</span> <span class="err">is</span>
<span class="k">[[91291  2534]</span>
 <span class="k">[   11   151]]</span>
</pre></div>


<p>From the output above, we know on the training data, the recall score is 0.918 which means 91.8 over 100 of the  True conditions are predicted correctly. And 97 over 100 of the predicted positives are really positive.</p>
<p>However, there is only a precision score of 0.056 on the Test data, which means only 5.6 out of 100 predicted positives are real positives.</p>
<p>From Confusion Matrix, 151 of 162 True Positives are predicted as positives. However, the model predicted 2533 Negative data as Positive.</p>
<p>That is, out model has pretty strong over-fitting.</p>
<h2>2.1 Change the Thresholds</h2>
<div class="highlight"><pre><span class="n">ytrain_pred_probas</span> <span class="o">=</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">fpr</span><span class="p">,</span> <span class="n">tpr</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">ytrain</span><span class="p">,</span> <span class="n">ytrain_pred_probas</span><span class="p">)</span>   <span class="c1"># precision_recall_curve</span>
<span class="n">roc</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;FPR&#39;</span><span class="p">:</span><span class="n">fpr</span><span class="p">,</span><span class="s1">&#39;TPR&#39;</span><span class="p">:</span><span class="n">tpr</span><span class="p">,</span><span class="s1">&#39;Thresholds&#39;</span><span class="p">:</span><span class="n">thresholds</span><span class="p">})</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">roc</span><span class="o">.</span><span class="n">FPR</span><span class="p">,</span> <span class="n">roc</span><span class="o">.</span><span class="n">TPR</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;#00C851&#39;</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;FPR&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;TPR&quot;</span><span class="p">)</span>
</pre></div>


<p><img alt="png" src="/figures/20180505_creditCardFraudDetection_02.png" /></p>
<div class="highlight"><pre><span class="n">ytest_pred_probas</span> <span class="o">=</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtest</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">new_threshold</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">ytest_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">ytest_pred_probas</span> <span class="o">&gt;=</span> <span class="n">new_threshold</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s2">&quot;After change threshold to 0.2, the recall socre on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After change threshold to 0.2, the precision socre on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After change threshold to 0.2, the Confusion Matrix on Test data is:&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">ytest_pred</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre>After change threshold to 0.2, the recall socre on Test data is:
0.956790123457

 After change threshold to 0.2, the precision socre on Test data is:
0.016130710792

 After change threshold to 0.2, the Confusion Matrix on Test data is:
[[84371  9454]
 [    7   155]]
</pre></div>


<p>Conclusion: After over-sampling, the model will have higher recall rate. That is, the model will work better on detect the Frauds from True Frauds. The price we paid is the lower precision rate.</p>
<h1>3. Logistic Regression with class_weight</h1>
<p>Rather than over-sampling, we can assign more weights to the lower rate class. In fact, if you write out the Likelihood function for Logistic Regression, the Over-Sampling and the assigning more Weights will be equivalent.</p>
<div class="highlight"><pre><span class="n">positive_weight</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ytrain</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">ytrain</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span>  <span class="c1"># size to repeat y == 1</span>

<span class="n">logitreg_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))}</span>
<span class="n">logitreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">class_weight</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span> <span class="p">:</span> <span class="n">positive_weight</span><span class="p">},</span> <span class="n">verbose</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">warm_start</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
<span class="n">logitreg_grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">logitreg</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">logitreg_parameters</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">,</span> <span class="n">n_jobs</span> <span class="o">=</span> <span class="mi">70</span><span class="p">)</span>

<span class="n">logitreg_grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span>
</pre></div>


<div class="highlight"><pre><span class="k">[LibLinear]</span>


<span class="na">GridSearchCV(cv</span><span class="o">=</span><span class="s">None, error_score=&#39;raise&#39;,</span>
<span class="s">       estimator=LogisticRegression(C=1.0, class_weight={0: 1, 1: 577}, dual=False,</span>
<span class="s">          fit_intercept=True, intercept_scaling=1, max_iter=100,</span>
<span class="s">          multi_class=&#39;ovr&#39;, n_jobs=1, penalty=&#39;l2&#39;, random_state=None,</span>
<span class="s">          solver=&#39;liblinear&#39;, tol=0.0001, verbose=3, warm_start=True),</span>
<span class="s">       fit_params=None, iid=True, n_jobs=70,</span>
<span class="s">       param_grid={&#39;C&#39;: array([  1.00000e-03,   1.00000e-02,   1.00000e-01,   1.00000e+00,</span>
<span class="s">         1.00000e+01,   1.00000e+02])},</span>
<span class="s">       pre_dispatch=&#39;2*n_jobs&#39;, refit=True, return_train_score=&#39;warn&#39;,</span>
<span class="s">       scoring=&#39;roc_auc&#39;, verbose=0)</span>
</pre></div>


<div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the recall score on Training data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytrain_oversample</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain_oversample</span><span class="p">))</span>    <span class="c1"># 0.912</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the precision score on Training data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytrain_oversample</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtrain_oversample</span><span class="p">))</span>   <span class="c1"># 0.972</span>

<span class="c1"># on the separated TEST data</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the recall score on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1">#  0.932</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the precision score on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>   <span class="c1"># 0.058</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the Confusion Matrix on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> After assign class_weight, the ROC AUC Score on Test data is&quot;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">roc_auc_score</span><span class="p">(</span><span class="n">ytest</span><span class="p">,</span> <span class="n">logitreg_grid</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtest</span><span class="p">))</span>
</pre></div>


<div class="highlight"><pre> After assign class_weight, the recall score on Training data is
0.912121212121

 After assign class_weight, the precision score on Training data is
0.972226568612

 After assign class_weight, the recall score on Test data is
0.932098765432

 After assign class_weight, the precision score on Test data is
0.0581888246628

 After assign class_weight, the Confusion Matrix on Test data is
[[91381  2444]
 [   11   151]]

 After assign class_weight, the ROC AUC Score on Test data is
0.953025135447
</pre></div>


<p>If we set up the class weight for the positive as the ratio of non-Fraud / Fraud, we will get the result close to the over-sampling.</p>
<p>So, in summary:
This specific data is about fraud detection. So the model should focus on to find the frauds
to avoid potential loss for the bank. That is, we should focus on RECALL rate. </p>
<ol>
<li>If we use the imbalanced data directly, we will get low performance model since the model prefer to predict to the class with dominated frequency class. The recall rate is 0.58. That is, only 58% of the frauds can be detected by this model.</li>
<li>To fix that, one way is to do over-sampling or down-sampling. If we use over-sampling, the model performance will be  improved a lot. For this specific case, the recall rate on the independent test set will be improved from 0.58 to 0.932</li>
<li>Another way to improve the model performance is to assign more weights to the low frequency class. Generally speaking, for Logistic Regression, assigning weights is similar to over-sampling, from the likelihood function perspective. The final output results are close too as demonstrated above.</li>
</ol>
<h2>Reference</h2>
<ol>
<li><a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">Credit Card Fraud Detection</a></li>
<li><a href="https://bitbucket.org/songhuiming/test/src/master/test1/20180510_CreditCardFraudDetection.py">CreditCardFraudDetection.py</a></li>
</ol></div>
    <footer>
<p class="meta">
  <span class="byline author vcard">
    Posted by <span class="fn">
        <a href="/author/huiming-song.html">Huiming Song</a>
    </span>
  </span>
<time datetime="2018-05-05T18:08:00-05:00" pubdate>Sat 05 May 2018</time>  <span class="categories">
    <a class='category' href='/category/python.html'>Python</a>
  </span>
  <span class="categories">
    <a class="category" href="/tag/python.html">python</a>,    <a class="category" href="/tag/data-mining.html">data mining</a>,    <a class="category" href="/tag/sklearn.html">sklearn</a>  </span>
</p><div class="sharing">
</div>    </footer>
  </article>

</div>
<aside class="sidebar">
  <section>
    <h1>Recent Posts</h1>
    <ul id="recent_posts">
      <li class="post">
          <a href="/pages/2021/11/06/recommendation-system-02/">Recommendation System 02</a>
      </li>
      <li class="post">
          <a href="/pages/2021/11/06/recommendation-system-03/">Recommendation System 03</a>
      </li>
      <li class="post">
          <a href="/pages/2021/10/30/recommendation-system-01/">Recommendation System 01</a>
      </li>
      <li class="post">
          <a href="/pages/2021/10/22/zhi-chang-hua-ti-bei-mei-hua-ren-gao-guan-zong-jie-de-zhi-chang-shang-sheng-fang-fa/">职场话题——北美华人高管总结的职场上升方法</a>
      </li>
      <li class="post">
          <a href="/pages/2019/07/27/2019-07-27-week-30-gong-ju-he-ji/">2019-07-27 Week 30 工具合集</a>
      </li>
    </ul>
  </section>
  <section>
      
    <h1>Categories</h1>
    <ul id="recent_posts">
        <li><a href="/category/career-growth.html">career growth</a></li>
        <li><a href="/category/linux.html">Linux</a></li>
        <li><a href="/category/python.html">python</a></li>
        <li><a href="/category/rthers.html">Rthers</a></li>
    </ul>
  </section>
 

  <section>
  <h1>Tags</h1>
    <a href="/tag/pelican.html">pelican</a>,    <a href="/tag/apply_async.html">apply_async</a>,    <a href="/tag/mysql.html">mysql</a>,    <a href="/tag/deep-learning.html">deep learning</a>,    <a href="/tag/data-visualization.html">data visualization</a>,    <a href="/tag/linux.html">linux</a>,    <a href="/tag/apply.html">apply</a>,    <a href="/tag/git.html">git</a>,    <a href="/tag/re.html">re</a>,    <a href="/tag/flask.html">flask</a>,    <a href="/tag/pyqt.html">PyQt</a>,    <a href="/tag/career-growth.html">career growth</a>,    <a href="/tag/dynamic-programming.html">dynamic programming</a>,    <a href="/tag/bokeh.html">bokeh</a>,    <a href="/tag/quant.html">quant</a>,    <a href="/tag/remote-access.html">remote access</a>,    <a href="/tag/tensorflow.html">tensorflow</a>,    <a href="/tag/webcrawl.html">webCrawl</a>,    <a href="/tag/numpy.html">numpy</a>,    <a href="/tag/pandas.html">pandas</a>,    <a href="/tag/tweepy.html">tweepy</a>,    <a href="/tag/map.html">map</a>,    <a href="/tag/shiny.html">shiny</a>,    <a href="/tag/random-walk.html">random walk</a>,    <a href="/tag/python.html">python</a>,    <a href="/tag/leetcode.html">leetcode</a>,    <a href="/tag/matplotlib.html">matplotlib</a>,    <a href="/tag/pytorch.html">pytorch</a>,    <a href="/tag/base.html">base</a>,    <a href="/tag/sentiment-analysis.html">sentiment analysis</a>,    <a href="/tag/sql.html">sql</a>,    <a href="/tag/data-minging.html">data minging</a>,    <a href="/tag/tkinter.html">tkinter</a>,    <a href="/tag/data-mining.html">data mining</a>,    <a href="/tag/spyre.html">spyre</a>,    <a href="/tag/highcharts.html">highcharts</a>,    <a href="/tag/r.html">R</a>,    <a href="/tag/statsmodels.html">statsmodels</a>,    <a href="/tag/docker.html">docker</a>,    <a href="/tag/cx_freeze.html">cx_freeze</a>,    <a href="/tag/multiprocessing.html">multiprocessing</a>,    <a href="/tag/sklearn.html">sklearn</a>  </section>


    <section>
        <h1>Social</h1>
        <ul>
            <li><a href="https://www.linkedin.com/pub/huiming-song/24/735/349" target="_blank">Linkedin</a></li>
        </ul>
    </section>
    <section>
        <h1>Blogroll</h1>
        <ul>
            <li><a href="http://easysas.blogspot.com/" target="_blank">my old SAS blog</a></li>
        </ul>
    </section>

</aside>    </div>
  </div>
  <footer role="contentinfo"><p>
    Copyright &copy;  2015&ndash;2021  shm &mdash;
  <span class="credit">Powered by <a href="http://getpelican.com">Pelican</a></span>
</p></footer>
  <script src="/theme/js/modernizr-2.0.js"></script>
  <script src="/theme/js/ender.js"></script>
  <script src="/theme/js/octopress.js" type="text/javascript"></script>
    <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-65938411-1']);
    _gaq.push(['_trackPageview']);
    (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();

    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-65938411-1');
    ga('send', 'pageview');
</script>
  <script type="text/javascript">
    var disqus_shortname = 'songhuiming';
    var disqus_identifier = '/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/';
    var disqus_url = '/pages/2018/05/05/credit-card-fraud-detection-imbalanced-data-modeling-part-i-logistic-regression/';
    var disqus_title = 'Credit Card Fraud Detection / Imbalanced data modeling - Part I: Logistic Regression';
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = "//" + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
     })();
  </script>
</body>
</html>